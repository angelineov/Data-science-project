conference ,title,description,company,industry
NVIDIA GTC 2020,Accelerating Unsupervised SAR Polarimetric Image Segmentation by Parallel Wishart Classifier [P21999],"Our poster presents the unsupervised segmentation of SAR polarimetric images. These images are multi-band areal images of the Earth captured by satellites with special sensors. The process required to segment these images is massive, and the task of segmentation is crucial, as through it we needed to extract important physical information of the area under observation, such as the geometric orientation, structure, shape, and configuration. We propose a much faster parallel implementation of Wishart Classification Based technique that outperforms the vanilla methods by 240%.","Shivam Patel, Visiting Research Student, University of Cambridge",Aerospace / National labs
NVIDIA GTC 2020,AI @ The Network Edge (Presented by HPE) [S22480],"It's at the extreme edge of your network, where most data generation occurs. In this session, we'll cover the unique system, software, and data capabilities required to extend AI to the edge in extreme environments. We will also discuss how to extend the data fabric that AI depends on from the edge to other processing elements within the Core and Cloud.","Jeffrey Winterich, DoD Account Chief Technologist, Hewlett Packard Enterprise",Aerospace / National labs
NVIDIA GTC 2020,Anomaly Detection on Aircraft Sensor Data Using Deep Learning [s21454],"A vast amount of aircraft sensor data still remains unexploited due to its volume and shear complexity. In 2019, Airbus released, as part of a four-month long challenge, a 65+ gigabyte dataset recorded from real aircraft systems. The goal was to detect a set of anomalies from an unlabeled multidimension time series dataset. We'll describe our solution, which is based on a two-stage approach using auto encoders and long short-term memory neural networks, and how we reached third place out of 160 teams competing. You'll learn the benefits of using autoencoder to achieve dimension reduction in a clustering problem and how LSTM-based neural networks can be applied to detect anomalies in an unsupervised way. We'll dive into the technical details of the solution and discuss the results obtained, as well as potential next steps. Basic knowledge of deep learning-techniques such as Autoencoder or LSTM will help, but isn't required.","Stephane Rion, Deep Learning Scientist, Teradata",Aerospace / National labs
NVIDIA GTC 2020,Complex 60 GPU CFD Simulations for Aerospace Gearboxes [s21788],"We'll present a unique new computational fluid dynamics approach to solving the extremely complex multiphase case of an aerospace gearbox model using a purely GPU-based CFD solver, Altair nanoFluidX. The challenge is to better understand the complex flow inside such machinery, as good oil supply to all the key areas inside the gearbox is critical for efficient heat management, lubrication, and fuel consumption. We'll introduce the numerical method used, main challenges of the current state-of-the-art methods, CFD results (flow fields), and performance numbers from the cluster, going all the way up to 60 GPUs.","Milos Stanic, Product Manager, Altair Engineering",Aerospace / National labs
NVIDIA GTC 2020,Deep Learning-Based Anti-Drone System [P21900],This poster is about research on detecting and tracking drones by applying deep learning technology to capture drones that are illegally operated.,"Hanseob Lee, Ph.D. Candidate, KAIST",Aerospace / National labs
NVIDIA GTC 2020,Deep-Learning Model for Finding New Superconductors [P21328],"We report the first deep-learning model for finding new superconductors. We represented the periodic table in such a way that deep learning can learn it. Although we used only the chemical composition of materials as information, we obtained an R2 value of 0.92 for predicting Tc for materials in a database of superconductors. We obtained three remarkable results. The deep-learning method can predict superconductivity for a material with a precision of 62\%, which shows the usefulness of the model; it found the recently discovered superconductor CaBi2, which is not in the superconductor database; and it found Fe-based high-temperature superconductors (discovered in 2008) from the training data before 2008. These results open the way for the discovery of new high-temperature superconductor families.","Tomohiko Konno, Researcher, National Institute of Information and Communications Technology",Aerospace / National labs
NVIDIA GTC 2020,Landing on Mars: Petascale Unstructured-Grid Computational Fluid Dynamics Simulations on Summit [S21584],"We'll present a campaign to investigate the use of supersonic retropropulsion as a means to land payloads large enough to enable human exploration on Mars. The world’s largest supercomputer, Summit, located at Oak Ridge National Laboratory, performs simulations. We'll review the engineering and computational challenges associated with retropropulsion aerodynamics and the need for large-scale resources like Summit. A GPU implementation of NASA Langley Research Center's FUN3D flow solver is used for these simulations. We'll compare the development history, performance, and scalability with those of contemporary HPC architectures. Using an optimized GPU-accelerated computational fluid dynamics solver on Summit has enabled simulations well beyond conventional computing paradigms.","Eric Nielsen, Senior Research Scientist, Nasa Langley Research Center",Aerospace / National labs
NVIDIA GTC 2020,Large-Scale Landslides Detection from Satellite Images with Incomplete Labels [P21920],"Earthquakes and tropical cyclones cause suffering for millions of people around the world every year. The resulting landslides exacerbate the effects of these disasters. Landslide detection is, therefore, critical for protecting human life and livelihood in mountainous areas. To tackle this problem, we propose a combination of satellite technology and deep neural networks (DNNs). We evaluate the performance of multiple DNN-based methods for landslide detection on actual satellite images of landslide damage using the capabilities of high-performance GPUs. Our analysis demonstrates the potential for a meaningful social impact on disasters and rescue.","Masanari Kimura, Engineer, Ridge-i inc.",Aerospace / National labs
NVIDIA GTC 2020,LUCID: High-Resolution Ground-Based Observations of LEO Satellites with Multi-Frame Blind Deconvolution [s21270],"High-resolution imaging of objects in space from a ground-based observatory is achievable with a sufficiently large aperture, but atmospheric turbulence causes significant degradation. Computationally expensive algorithms can mitigate the blurring effects of turbulence, and these algorithms have only recently begun to leave the domain of CPU-bound computation. We'll describe space domain awareness, the imaging-through-turbulence problem, and algorithms that attempt to solve it. We'll also describe Likelihood-based Uncertainty Constrained Iterative Deconvolution (LUCID), a new multi-frame blind deconvolution implementation that uses CUDA to extract high-resolution images of low-Earth-orbit (LEO) satellites from a series of short-exposure observations.","Michael Werth, Physicist, The Boeing Company",Aerospace / National labs
NVIDIA GTC 2020,Super-Resolution of Digital Terrain Data [P21822],"Given a digital elevation model (DEM) at some fixed-grid post spacing, we'd like to better interpolate between the posts to produce a DEM of higher resolution. Intuitively, single-image super-resolution techniques can do that. A neural network is trained on low-resolution elevation data (for example, 90m post spacing) until it can produce a reasonable approximation of higher-resolution elevation data, such as 30m post spacing. Dictionary-based interpolation is achieved through a statistical mapping from low-resolution (LR) features to high-resolution (HR) features, as learned by the hidden layers in the network. This approach shows promising results in producing pseudo-HR elevation models from LR models, and if applied to HR data, can produce pseudo-VHR models suitable for meshing with lidar scans.","Theodore Hromadka, Senior Principal Software Engineer, Centauri",Aerospace / National labs
NVIDIA GTC 2020,Using Robust Networks to Inform Lightweight Models in Semi-Supervised Learning for Object Detection [P21789],"Learn from industry practitioners as we present a practical, streamlined data curation workflow designed to mitigate burdens associated with hand-labeled data for supervised learning in object detection. We utilize the capacity of robust object detection networks to efficiently train low-latency networks with access to large pools of unlabeled data but only limited hand-labeled data. By fine-tuning a robust model on the limited examples, we create new, larger training datasets via robust inference on unlabeled data. The combined hand-labeled subset and the inferenced dataset are then used to train a lightweight model. This approach results in more accurate lightweight models with minimal cost from hand-labeled data while also providing an efficient way of curating ground-truth datasets.","Jonathan Worobey, Software Engineer, SURVICE Engineering",Aerospace / National labs
NVIDIA GTC 2020,VR Blast Forensics [S21629],We'll present a unique approach of coupling high-fidelity computational physics GPU-based solvers with the software Unity to produce a post-blast forensics virtual reality environment. We'll also cover the benefits of utilizing GPUs to perform both computational physics and VR rendering.,"Phillip Mulligan, Assistant Research Professor, Missouri University of Science and Technology",Aerospace / National labs
NVIDIA GTC 2020,WASP: A WeArable SuPercomputing Platform for Lost Person Search-and-Rescue [P22071],"WASP, A WeArable SuPercomputing Platform for Lost Person Search-and-Rescue (SAR), redefines computation at the edge. More than 100,000 people were reported lost in the wilderness and urban settings in the United States in 2017. Use of UAVs for SAR applications shines due to their aerial point of view, enhanced mobility, and wireless connectivity by virtue of unhindered airspace in otherwise-dense foliage. Despite these benefits, the adoption of UAVs in such applications is rendered somewhat infeasible due to their short flight times and limited computation and load-bearing capabilities. WASP, which houses four Jetson Xavier AGXs, aims at offloading computation from the drone to a wearable backpack for the human-in-the-loop searcher, with a high-capacity battery and wireless router. The backpack not only manages massive real-time computational workloads including deep-learning inference, dynamic path planning, and GUI, but also hands-off enhanced control and contextual information to the human searchers.","Chinmaya Patnayak, Student, Virginia Tech",Aerospace / National labs
NVIDIA GTC 2020,Accelerating DNN Inference with GraphBLAS and the GPU [S21796],"Our work addresses the 2019 Sparse Deep Neural Network Graph Challenge with an implementation using the GraphBLAS programming model. We'll demonstrate our solution to this challenge with GraphBLAST, a GraphBLAS implementation on the GPU, and compare it to SuiteSparse, a GraphBLAS implementation on the CPU. The GraphBLAST implementation is 1.94x faster than SuiteSparse; the primary opportunity to increase performance on the GPU is a higher-performance sparse-matrix-times-sparse-matrix (SpGEMM) kernel.","Xiaoyun Wang, Ph.D. Candidate, University of California, Davis",Aerospace / National labs
NVIDIA GTC 2020,Accelerating Graph Algorithms on Exascale Systems [S21642],"We'll discuss our current efforts for accelerating graph algorithms on the latest US Department of Energy leadership systems. Graph methods are key kernels for large-scale data analytics, as well as for several exascale application domains, including smart grids, computational biology, computational chemistry, and climate science. We'll present our latest results on distributed implementations employing GPUs and accelerators of graph kernels such as community detection and influence maximization, showing how we can tackle large-scale problems with heterogeneous supercomputers.","Mahantesh Halappanavar, Senior Research Scientist and Team Lead, Pacific Northwest National Laboratory",Aerospace / National labs
NVIDIA GTC 2020,Accelerating Large-Scale GW Calculations in Material Science [s21353],"Learn the balancing act of porting a large-scale HPC code to modern GPUs, where a plethora of architectural characteristics can both accelerate and limit performance. We'll showcase various techniques used to accelerate the material science code BerkeleyGW on NVIDIA GPUs targeting large-scale simulations with thousands of atoms, matrices of up to 1 million by 1 million, and reductions of thousands of billions of numbers. These techniques include the use of cuBLAS and cuFFT, pinned memory, streams, batched operations, shared memory, and the overlapping of message-passing interface communication and GPU computation. Excellent strong scaling and weak scaling are observed on thousands of Volta GPUs, and a 16x improvement is obtained on FLOPs/Watt efficiency compared to the CPU-only implementation.","Charlene Yang, Application Performance Specialist, NERSC, Lawrence Berkeley National Laboratory",Aerospace / National labs
NVIDIA GTC 2020,Accelerating the Singular Value Decomposition: Measuring SVD Performance for Small Matrices [P21776],"Advanced algorithms for spatio-temporal signal processing of data from a phased sensor array often require singular value decompositions (SVD) of a very large number of complex-valued matrices of a relatively small size. Our poster describes a GPU-implemented SVD that we custom-designed for such signal processing applications in order to enable real-time performance. We compare our SVD with tools available in the cuSolver, MKL, and MAGMA libraries, and we document a substantial performance improvement on batches of small square matrices.","Charlotte Kotas, Research Scientist, Oak Ridge National Laboratory",Aerospace / National labs
NVIDIA GTC 2020,A Framework for Measuring Hardware Gather-Scatter Support [P21887],"This poster describes a new benchmark tool, Spatter, for assessing memory system architectures in the context of indexed accesses. This type of memory operation is often used to express sparse and irregular data patterns. Gathers and scatters have widespread utility in modern HPC applications, including traditional scientific simulations, data mining and analysis computations, and graph processing. Spatter specifically measures gather / scatter (G/S) variations for multiple platforms, with several tunable backends, including CUDA and OpenMP, and provides comparison metrics for different sparse access patterns. We show how Spatter can be used to evaluate the recent improvements to G/S hardware on NVIDIA GPUs and CPUs, and show performance numbers for STREAM-like, uniform-stride, and application-derived memory access patterns.","Patrick Lavin, Ph.D. Student, Georgia Tech",Aerospace / National labs
NVIDIA GTC 2020,A Partitioned Global Address Space Library for Large GPU Clusters [S22093],"We'll discuss NVSHMEM, a PGAS library that implements the OpenSHMEM specification for communication across NVIDIA GPUs connected by different types of interconnects that include PCI-E, NVLink, and Infiniband. NVSHMEM makes it possible to initiate communication from within a CUDA kernel. As a result, CUDA kernel boundaries are not forced on an application due to its communication requirements. Less synchronization on the CPU helps strong scaling efficiency. Ability to initiate fine-grained communication from inside the CUDA kernel helps achieve better overlap of communication with computation. QUDA is a popular GPU-Enabled QCD library used by several popular packages like Chroma and MILC. NVSHMEM enables better strong scaling in QUDA. NVSHMEM not only benefits latency-bound applications like QUDA, but can also help improve performance and reduce complexity of codes like FFT that are bandwidth-bound, and codes like Breadth First Search that have a dynamic communication pattern.","Akhil Langer, Senior Software Engineer, NVIDIA",Aerospace / National labs
NVIDIA GTC 2020,A Performance-Portable Diffusion Solver Based on Axom [P21892],"The sheer size and complexity of large-scale multiphysics codes motivate maintenance of a single-source codebase that is parallel and readily portable across different architectures. This is especially attractive due to recent trends toward heterogeneous architectures. We present Mint, an API that provides a mesh-aware, fine-grain, parallel execution model that underpins the development of computational tools and discretization methods. We build a 2D finite-volume diffusion solver that models separation of chemical species that arise in inertial confinement fusion. We demonstrate portability of the solver across two different architectures, one on a CPU-based architecture and one with CPU+GPU architecture. When compiled with the CUDA backend relative to the OpenMP backend, we find up to 3.5x speedup, demonstrating the viability of the API.","Tyler Masthay, Research Assistant, University of Texas at Austin",Aerospace / National labs
NVIDIA GTC 2020,Applications of Convolutional Neural Network to the Important Earth Science Problems [P22283],"We applied the convolutional neural network to several Earth science problems that involve (1) discrimination of seismic signals from earthquakes and tectonic tremors; (2) determination of an earthquake hypocenter; and (3) solar radiation estimation from fixed-point camera images. Monitoring tremor activity provides insights into deformation processes of megathrust earthquakes. We could achieve 99.5% accuracy for identifications of signals from tremors, regular earthquakes, and noise. We establish neural networks for the determination of hypocentral parameters. We calculate theoretical seismograms for a realistic 3D Earth model and use these seismograms as learning dataset. We can determine an earthquake hypocenter with this neural network. We develop a new observation method that used deep learning to estimate the amount of solar radiation from images. This new technique can be used to make multifaceted observations.","Daisuke Sugiyama, Engineer, Japan Agency for Marine-Earth Science and Technology",Aerospace / National labs
NVIDIA GTC 2020,Big Lasers for Small Accelerators: Exascale Simulations for Better Cancer Therapy [S21850],"Learn how we leverage PIConGPU, an open source, multi-platform particle-in-cell code scaling to the fastest supercomputers in the TOP500 list (Titan, Piz Daint, Summit), to model advanced plasma physics applications. Advances in compact plasma-based accelerators driven by petawatt-class lasers spark great interest in their applications. Ion beams accelerated by intense laser pulses break new ground for treating cancer. Laser-generated electron beams can drive new compact X-ray sources to create snapshots of ultrafast processes in materials and show a path to reaching the energy frontier for studying fundamental physics. We'll present our strategies to harness the power of future exascale supercomputers, handling extreme data flows from thousands of GPUs for analysis with in-situ data analytics and an open data ecosystem. We'll provide detailed performance analysis and show the benefits of PIConGPU for real-world physics cases.","Michael Bussmann, Head of Center for Advanced Systems Understanding, Helmholtz-Zentrum Dresden-Rossendorf",Aerospace / National labs
NVIDIA GTC 2020,CUDA C++ in Jupyter: Adding CUDA Runtime Support to Cling [S21588],"Jupyter Notebooks are omnipresent in the modern scientist's and engineer's toolbox just as CUDA C++ is in accelerated computing. We present the first implementation of a CUDA C++ enabled read-eval-print-loop (REPL) that allows to interactively ""script"" the popular CUDA C++ runtime syntax in Notebooks. With our novel implementation, based on CERN's C++ interpreter Cling, the modern CUDA C++ developer can work as interactively and productively as (I)Python developers while keeping all the benefits of the vast C++ computing and library ecosystem coupled with first-class performance.","Axel Huebl, Researcher, Lawrence Berkeley National Laboratory",Aerospace / National labs
NVIDIA GTC 2020,Day of the Living Cell: Supercomputers Reveal Molecular Design Principles of Photosynthesis [S21500],"We'll discuss developing the highly parallel molecular dynamics code NAMD for current and upcoming machines, followed by an accessible presentation of results from a decade of work that paves the way to first-principles modeling of whole living cells. One of the many ambitious research projects begun by the late Klaus Schulten was to model, simulate, and ultimately understand the photosynthetic apparatus of bacteria, from the scale of individual atoms to that of the entire cell. The GPU-accelerated supercomputers of Oak Ridge National Laboratory running NAMD have been critical to enabling progress in this continuing work.","James Phillips, Senior Research Programmer, University of Illinois",Aerospace / National labs
NVIDIA GTC 2020,Deep Learning for Efficient Modeling of High-Dimensional Spatiotemporal Physics [S22094],"Several research problems in physical sciences are exceptionally complex and high-dimensional, exhibiting spatio-temporal dynamics, non-linearity, and chaos. In an era when vast quantities of such scientific data are generated, building practical, physics-driven reduced-order models (ROM) of such phenomena is crucial. While deep neural networks for spatio-temporal data have shown considerable promise, they face severe computational bottlenecks in learning extremely high-dimensional datasets, often with greater than 10^9 degrees of freedom. These application-agnostic networks may also lack physical constraints and interpretability that is desired in scientific ROMs. We'll present our efforts in leveraging the strong mathematical and physical foundations underlying wavelet theory with the learning capacity of deep neural nets. We'll demonstrate computationally efficient, partially interpretable learning with some embedded physics constraints for modeling large scientific datasets.","Arvind Mohan, Postdoctoral Researcher, Los Alamos National Laboratory",Aerospace / National labs
NVIDIA GTC 2020,Dramatic Acceleration of Quantum Transport Simulations: Solving Non-Equilibrium Green’s Function with GPU Devices [P21787],"The poster presents in-depth discussion on technical details and strategies for GPU-driven performance enhancement in solving the Non-Equilibrium Green's Function (NEGF), which is critically used to simulate quantum transport behaviors of electronic devices in a nanoscale regime. Although here we focus on NEGF as a main target of performance enhancement, the contents (particularly the strategies of performance improvement with GPU computing) presented in this poster can be still useful for ANY numerical problems that involve the computation of an inverse of block-tridiagonal (dense/sparse, real/complex) matrices.","Yosang Jeong, Researcher, Korea Institute of Science and Technology Information",Aerospace / National labs
NVIDIA GTC 2020,Effective Use of Mixed Precision for HPC [S21725],"We'll discuss how using mixed-precision techniques effectively can significantly speed up current-generation supercomputers, as well as the practical and numerical challenges in adopting such techniques. We'll focus specifically on lattice quantum chromodynamics, which is a regular top-cycle consumer of public supercomputers. Such techniques are an important and sometimes necessary optimization direction for HPC applications.","Kate Clark, Principal Developer Technology Engineer, NVIDIA",Aerospace / National labs
NVIDIA GTC 2020,Enabling 800 Projects for GPU-Accelerated Science on Perlmutter at NERSC [S21582],"The National Energy Research Scientific Computing Center (NERSC) is the mission HPC center for the U.S. Department of Energy Office of Science and supports the needs of 800+ projects and 7,000+ scientists with advanced HPC and data capabilities. NERSC’s newest system, Perlmutter, is an upcoming Cray system with heterogeneous nodes including AMD CPUs and NVIDIA Volta-Next GPUs. It will be the first NERSC flagship system with GPUs. Preparing our diverse user base for the new system is a critical part of making the system successful in enabling science at scale. The NERSC Exascale Science Application Program is responsible for preparing the simulation, data, and machine learning workloads to take advantage of the new architecture. We'll outline our strategy to enable our users to take advantage of the new architecture in a performance-portable way and discuss early outcomes. We'll highlight our use of tools and performance models to evaluate application readiness for Perlmutter and how we effectively frame the conversation about GPU optimization with our wide user base. In addition, we'll highlight a number of activities we are undertaking in order to make Perlmutter a more productive system when it arrives through compiler, library, and tool development. We'll also cover outcomes from a series of case studies that demonstrate our strategy to enable users to take advantage of the new architecture. We'll discuss the programming model used to port codes to GPUs, the strategy used to optimize code bottlenecks, and the GPU vs. CPU speedup achieved so far. The codes will include Tomopy (tomographic reconstruction), Exabiome (genomics de novo assembly), and AMReX (Adaptive Mesh Refinement software framework).","Jack Deslippe, Application Performance Group Lead, NERSC",Aerospace / National labs
NVIDIA GTC 2020,Enhancing Intra-Node Multi-GPU Stencil Calculations on DGX-2 Using Concurrent-Addressing with Unified Memory [P21784],"In the “CityLBM” project at the Japan Atomic Energy Agency, a real-time AMR (adaptive mesh refinement)-based urban wind prediction code was developed. The next generation of CityLBM code needs ensemble simulations to improve the reliability of the prediction. To achieve that, memory usage should shrink to a single node, or 4-16 GPUs per simulation. To reduce memory usage and accelerate data communication in the AMR code, we tried an intra-node multi-GPU implementation using Unified Memory in CUDA. This approach enables easy parallel-GPU implementation, because the access to Unified Memory is automatically managed from via HBM2 (self GPU) or NVLink (neighbor GPU). We implemented multi-GPU calculations for a 3D diffusion equation and a lattice Boltzmann equation on uniform mesh, and tested weak/strong scalability and NVLink utilization.","Yuta Hasegawa, Scientist, Japan Atomic Energy Agency",Aerospace / National labs
NVIDIA GTC 2020,Exploiting Novel GPU Parallelism in the SNAP Interatomic Potential [S21976],"Cutting-edge investigations of material behavior via classical molecular dynamics simulation methods require application-specific, quantum-accurate interatomic potentials (IAPs). The SNAP machine-learning IAP, formulated in terms of general four-body geometric invariants, is trained against quantum electronic structure calculations. This enables the verifiably high-fidelity investigation of diverse material systems at length- and timescales unattainable by purely quantum calculations. Despite the high arithmetic complexity, achieving good SNAP performance with the increasing parallelism provided by modern GPU architectures is challenging. To address this, we have developed a novel parallelization over the geometric structure of the SNAP IAP, prompting memory layout optimizations which facilitate data reuse and reduce memory bandwidth requirements. The new SNAP algorithm will be deployed in the GPU-optimized LAMMPS implementation using the Kokkos templated C++ library.","Evan Weinberg, DevTech Compute, NVIDIA",Aerospace / National labs
NVIDIA GTC 2020,Explosive DGX Performance for Weapon Component Modeling [P21870],"At Sandia National Laboratories, we model explosive weapon components using sophisticated simulations. This poster shows how porting those simulations using CUDA and running them on a DGX Workstation provided great speedups, enabling weapon designers a whole new workflow for simulation.","Dan Ibanez, Staff Member, Sandia National Laboratories",Aerospace / National labs
NVIDIA GTC 2020,Extensions of TensorFlow-Based Computational Fluid Dynamics [S21816],"The National Energy Technology Laboratory (NETL) recently developed a completely TensorFlow-based computational fluid dynamics code for single-phase flow as a potential next-generation solver for Multiphase Flow with Interphase eXchanges (MFIX). NETL demonstrated over a 3x speedup using NVIDIA V100 GPUs on a DGX-1 before any domain decomposition was implemented. NETL is currently working on implementing parallel linear solvers in TensorFlow. In addition, NETL has strategically chosen to invest in a TensorFlow-based Multiphase Particle-In-Cell (MP-PIC) methodology. We'll cover the most recent developments to further the TensorFlow-based solver, and discuss plans for AI/ML acceleration.","Dirk Van Essendelft, General Engineer, The National Energy Technology Laboratory",Aerospace / National labs
NVIDIA GTC 2020,Fromage Optimizer for Deep Neural Networks [P22300],"We propose a new geometric characterization of neural network loss surfaces, called deep relative trust. This characterization suggests a new optimization algorithm that we call Fromage, short for ""Frobenius matched gradient descent."" Experiments with the optimizer on standard deep learning benchmarks—including Transformer and GAN—are promising.","Jeremy Bernstein, Ph.D. Student, California Institute of Technology",Aerospace / National labs
NVIDIA GTC 2020,"GPU-Accelerated Deep Learning for Weather, Climate, and Space [s21255]","We'll demonstrate how to use deep learning to tackle important challenges in weather forecasting, climate modeling, and the processing of satellite observations. We'll present recent results from ongoing research collaborations with the National Oceanic and Atmospheric Administration, NASA, and various universities, and explain how accurate results were achieved. We'll show how to automate feature detection to identify threats from severe weather, solar storms, and near-earth objects, and we'll discuss how to accelerate weather/climate models and data assimilation techniques to produce more accurate predictions. Finally, we'll illustrate how to better use satellite observations by enhancing, transforming, interpolating, fusing, and repairing multispectral data.","David Hall, Senior Solution Architect, NVIDIA",Aerospace / National labs
NVIDIA GTC 2020,GraphDefense: Toward Robust Large-Scale Graph Convolutional Network [P21806],"We study the robustness of graph convolutional networks (GCNs). Despite the good performance of GCNs on graph semi-supervised learning tasks, previous works have shown that the original GCNs are very unstable to adversarial perturbations. Inspired by the previous works on adversarial defense for deep neural networks, and especially adversarial training algorithm, we propose a method called GraphDefense to defend against the adversarial perturbations. In addition, for our defense method, we could still maintain semi-supervised learning settings without a large label rate. We also show that adversarial training in features is equivalent to adversarial training for edges with a small perturbation. Our experiments show that the proposed defense methods successfully increase the robustness of Graph Convolutional Networks. Furthermore, we show that with careful design, our proposed algorithm can scale to large graphs, such as Reddit dataset.","Xiaoyun Wang, Ph.D. Candidate, University of California, Davis",Aerospace / National labs
NVIDIA GTC 2020,Hedgehog: A Performance-Oriented General Purpose Library that Exploits Multi-GPU Systems [s21227],"We'll present Hedgehog, a general-purpose library for taking advantage of powerful compute nodes, multicore CPUs, and multiple GPUs. The novel aspects of Hedgehog are: (1) its explicit representation of a program as a dataflow graph, (2) its pure dataflow-driven scheduling, (3) its maintenance of a computation’s localized state via state managers, and (4) its fine control of memory via memory managers. This dataflow approach results in extremely low overhead for task executions (< 1 microsecond) and no-cost profiling at the task level. This allows us to prototype operations that compare favorably with leading libraries such as cuBLAS-XT.","Tim Blattner, Computer Scientist, NIST",Aerospace / National labs
NVIDIA GTC 2020,"High-Throughput 3D Image Reconstruction, Visualization, and Segmentation of Large-Scale Data at the Sirius Synchrotron Light Source [s21278]","We'll present highly efficient tools for large-scale 3D image reconstruction, visualization, and segmentation being developed for the Sirius synchrotron light source. Sirius will be the second fourth-generation synchrotron in the world, and will acquire 3D/4D images with resolution up to <50 nm using hard X-rays. With NVIDIA, we're creating integrated pipelines to reconstruct 3D images from modalities such as coherent-diffraction imaging and transmission tomography, visualize the data in streaming mode, and segment the images to provide almost real-time feedback. We rely on multi-GPU/node CUDA programming and machine/deep learning-optimized inference to address the issues, given that each 3D image may be larger than 100 GB and may be acquired down to 1s, resulting in around 50 TB of data of a wide variety of samples (for example, biological and geological) expected to be produced every day.","Thiago Vallin Spina, Researcher, Brazilian Synchrotron Light Laboratory / CNPEM",Aerospace / National labs
NVIDIA GTC 2020,High-Throughput Real-time Data Processing with GPUs at CERN [s21341],"We'll present the design and performance considerations and system optimization of a GPU-based, real-time physics selection system at a Large Hadron Collider experiment. Millions of particles collide every second at the LHCb experiment at CERN in Switzerland. To select interesting particle collisions, data must pass through an acquisition system and be filtered with real-time selection software. The throughput processed in the first stage of this streaming data processing application amounts to 40 terabytes per second, and the efficiency of the selection is crucial toward improving our fundamental understanding of the universe. In order to process this massive data throughput in real-time, we developed GPU physics reconstruction software called Allen. The Allen framework hands the raw data to GPU streams, which perform the decoding, reconstruction, and selection of particle collisions.","Daniel Hugo Cámpora Pérez, Research Postdoc, NIKHEF and University of Maastricht",Aerospace / National labs
NVIDIA GTC 2020,HPL-AI: Benchmarking Half-Precision Hardware with Modern Numerical Linear Algebra [S21470],"We'll present the mixed-precision iterative and direct methods used by the HPL-AI benchmark. These new approaches are instrumental in kernel-based performance evaluation of modern hardware accelerators that offer fast implementation of limited-precision floating-point units. The scope of the benchmark spans a number of important computational patterns that scientific codes, both in the past and on modern GPUs, often rely upon. We had to resolve a number of numerical issues to achieve a robust implementation, and we'll present those results for relevant background on the design process involved in benchmarking and how it can be leveraged for the benefit of the GTC audience.","Piotr Luszczek, Research Assistant Professor, University of Tennessee",Aerospace / National labs
NVIDIA GTC 2020,Improving Geophysical Turbulence Models with Machine Learning [S21574],"In large-scale geophysical flows, the relatively small-scale processes of turbulence and mixing can have a leading-order impact on the prediction of (for instance) ocean circulation and global energy budgets. Such predictions are critical components of weather and climate simulation — geophysical problems where small-scale models help offset the otherwise prohibitively expensive computational cost of simulation. These turbulence closure models attempt to capture dynamics that have complex functional dependence on a potentially broad range of large-scale flow parameters. However, models and frameworks are often phenomenological and heuristic in nature, such that robust model calibration to simulation, observation, and experiment data is a challenge. We'll explain how a nonintrusive supervised GPU-driven machine learning framework, such as Neural ODE, can help improve the state of turbulence models in canonical geophysical flows. We'll also discuss the interpretability of these machine-learning models and provide a roadmap to create more general frameworks for modeling such physics.","Peetak Mitra, Computational Physics Intern, Los Alamos National Laboratory",Aerospace / National labs
NVIDIA GTC 2020,Improving Ocean Model Framework NEMO with GPU Port and Asynchronous Execution [P21955],"Our work describes the improvement of an operational ocean model framework (NEMO) with routine porting and asynchronous execution in large-scale clusters using CUDA. We show routine GPU optimizations speed-up and implementation of asynchronous execution over large scale workloads. Considering that NEMO is a standard production framework for research activities and forecasting service, we aim to expose the ongoing challenges of accelerating Fortran/MPI long-term applications aligned with the needs of a broad community of developers.","Maicon Faria, HPC Support Specialist, Barcelona Supercomputing Center",Aerospace / National labs
NVIDIA GTC 2020,"Integrating NVIDIA Tesla V100 GPUs into a Cray System for a Diverse Simulation, Machine Learning, and Data Workload [S21569]","The HPC system ""Perlmutter"" will the first GPU-accelerated production system at the U.S. Department of Energy’s National Energy Research Scientific Computing Center (NERSC) when it is deployed in 2021. We'll explain how, to enable its users to prepare their applications for Perlmutter, NERSC recently integrated 18 GPU-accelerated compute nodes into its current production system, the Knights Landing-based Cray system ""Cori."" These nodes’ primary purpose is application development and profiling for GPU acceleration, as part of the NERSC Exascale Science Application Program (NESAP). Despite significant differences in hardware from the rest of the Cori system, the GPU nodes have been configured such that, from both the user’s and the administrator’s perspective, they are seamlessly integrated into Cori. This integration has streamlined access for hundreds of NESAP users to these nodes, and facilitates a diverse workload of NESAP applications spanning simulation, data-intensive workloads, and machine learning.","Brian Friesen, Application Performance Specialist, Lawrence Berkeley National Laboratory",Aerospace / National labs
NVIDIA GTC 2020,Interpretable Deep Learning for Hurricane Intensity Prediction [S21548],"Hurricanes can experience rapid increases in intensity, in which they can strengthen from a tropical storm to a major hurricane in only a couple of days. These rapid intensification periods are currently difficult to predict, but deep learning may be able to detect spatial patterns in the storms that are precursors to rapid intensification. We'll show how a convolutional neural network trained on output from the Hurricane Weather Research and Forecasting model can produce probabilistic estimates of rapid intensification. We'll also show how deep-learning interpretation techniques can reveal what storm structures are associated with rapid intensification versus rapid weakening.","Richard Loft, Director of Technology Development, Computational and Information Systems Laboratory, National Center for Atmospheric Research",Aerospace / National labs
NVIDIA GTC 2020,Learning the Hard Parts: Scaling Reacting Flow Simulations with Machine Learning [S21740],"Explore a new technique for overcoming bottlenecks in computational fluid dynamics with machine learning. We'll describe using a neural chemistry solver for an order-of-magnitude speedup, opening the doors to problems that are currently impossible. Simulating chemically reacting flow — fluid flow with a chemical source term (such as a flame) — is computationally prohibitive: an at-scale combustor simulation with a practical hydrocarbon fuel can take years! Solving the chemical source term dominates this process, taking up to 90% of the total compute time. This share increases for more complex reactions, resulting in poor scalability. We'll discuss designing a neural network to model hydrogen combustion; integrating this neural chemistry solver into computational fluid dynamics code; analyzing the performance of this new approach; and discussing how these lessons could be applied to other CFD bottlenecks.","Adam Moses, Computer Scientist, Naval Research Lab",Aerospace / National labs
NVIDIA GTC 2020,Matrix-Free Real-time Online Reconstruction of Compressive Focal Plane Array Camera [P21865],"This study proposes a solution for online and real-time reconstruction of compressive focal plane array cameras. These cameras enable high-resolution imaging using low-resolution sensors through compressive sensing reconstruction. Here, we use a spatial light modulator for encoding high-resolution data onto low-resolution sensors. Then, we solve an optimization problem for reconstruction. Previous methods involve reconstruction through algorithms that require many batched matrix multiplications. In this study, we propose using an algorithm that does not involve any matrix multiplication and show its performance against the previous method. Due to the high computational load of the image reconstruction algorithms, we use GPUs for fast computation. Moreover, we show that the camera can work in real time by sampling each data online and reconstruction frames in real time in a pipelined fashion. Finally, we show the reconstructed images through OpenGL interoperability library.","Alper Gungor, Senior Research Engineer, Aselsan Research Center",Aerospace / National labs
NVIDIA GTC 2020,Optimizing Stencil Operations in OpenACC [P21723],"Stencil operations are used widely in HPC applications and pose an optimization challenge on both CPUs and GPUs. On GPUs, fine-tuned optimizations can be formulated using low-level APIs such as CUDA, but many large established codes prefer a portable, higher-level API such as OpenACC. Although OpenACC lacks the fine-tuning of CUDA, it does allow for some tuning through a variety of parallelization constructs and loop directives. Here, we optimize a stencil operation within our production solar physics research code Magnetohydrodynamics Around a Sphere. We explore numerous OpenACC directive options (including tile, cache, collapse, etc.) and compare their performance over several problem types and sizes. The optimal result is used to run a full-scale simulation and analyzed with Nsight Systems. An emerging cautionary result is that although many directive options yield a speedup of the operator, using the ""wrong"" directives can result in drastically poor performance.","Ronald Caplan, Computational Scientist, Predictive Science Inc.",Aerospace / National labs
NVIDIA GTC 2020,"Performance Comparison of Search for Neighbor-Particle in MPS on Xeon, Xeon Phi and GPU [P21969]","The MPS method is a particle-based simulation used for computation fluid dynamics. It was originally developed for simulating fluid dynamics such as fragmentation of incompressible fluid. Target fluid or objects are divided into particles and each particle interacts with neighbor-particles. Searching for neighbor-particles is the main bottleneck of MPS. We’re researching and developing the in-house program called P-Flow_lite. We're proposing two optimizations for a search for neighbor-particles and implementing them on Xeon, Xeon Phi, and GPU by using directives. V100 is the fastest among them.","Takaaki Miyajima, Postdocoral Researcher, RIKEN Center for Computational Science",Aerospace / National labs
NVIDIA GTC 2020,Roofline Performance Model for HPC and Deep-Learning Applications [s21565],"Learn how to use the Roofline model to analyze the performance of GPU-accelerated applications. We'll cover the basics of the model, explain how to use tools such as nvprof and Nsight Systems/Compute to automate the data collection, and demonstrate how to track progress using Roofline for both HPC and deep-learning applications. We'll use examples such as GPP from material science, high-performance geometric multigrid from adaptive mesh refinement, and two kernels from TensorFlow to show how characteristics such as arithmetic intensity, memory access pattern, and thread divergence/prediction can all be captured by Roofline, offering useful insights to performance optimization.","Charlene Yang, Application Performance Specialist, NERSC, Lawrence Berkeley National Laboratory",Aerospace / National labs
NVIDIA GTC 2020,Scalable Workflow System for Whole Slide Microscopy Analyses Using Neural Networks [P21983],"We present a streaming, asynchronous, high-throughput workflow for joining traditional computer vision and artificial intelligence inference across multiple models using the Hybrid Task Graph Scheduler and TensorRT for scalable, single-node, multi-GPU object detection, classification, and regression. Our approach targets very large (100k x 50k pixel) automated whole-slide microscopy imaging, which operates under tight time constraints. The end-to-end workflow starts with a microscope scan and finishes with a populated database containing quality-assurance metrics, object detections, and other salient features. This generalizable workflow applies three independent TensorRT models, with dependency management, concurrently across all available GPUs within a single computer.","Tim Blattner, Computer Scientist, NIST",Aerospace / National labs
NVIDIA GTC 2020,Scaling Data by 109x and Compute for Deep-Learning Applications [S21390],"We'll explore the scalable applications of artificial intelligence on massive data sets. First, we'll cover how we optimized and developed highly parallelized implementations of DL algorithms and tested them on HPC GPU clusters. Then we'll demonstrate how to develop models that can run over large high-resolution datasets, identifying the spatial and temporal relationships between physical parameters in global-scale high-resolution numerical weather prediction models.","John Taylor, Program Leader, DST/CSIRO",Aerospace / National labs
NVIDIA GTC 2020,The Rocky Road to Tasking: Task Queues Reloaded [S21189],"We'll show you how to parallelize your irregular algorithm on GPUs with tasking, starting with an overview of our CUDA C++ tasking framework for fine-grained task parallelism. After touching on persistent threads, synchronization mechanisms, and load balancing, we'll present diverse optimization strategies. First, we'll describe the implementation of task queues based on static memory allocation. Second, we'll show how to implement work sharing on a GPU through hierarchical task queues. Third, we'll present a thread coordination scheme to reduce contention on the task queues, thus keeping all threads busy. We'll analyze each optimization step's performance gains for a prototypic implementation of a task-based fast multipole method for molecular dynamics.","Ivo Kabadshow, Scientist, Jülich Supercomputing Centre",Aerospace / National labs
NVIDIA GTC 2020,"Tiny, Tiny Tasks: Fine-Grained FMM Tasking on GPUs [P22059]","Learn how fine-grained task parallelism occurring in many HPC applications can be exploited with GPUs. In contrast to available tasking approaches (e.g. CUDA graphs) that rely in some part on the involvement of the host CPU by scheduling new tasks, we present a scheme to fully control the execution of tasks by the GPU without additional synchronization or data movement from the CPU. A fine-grained formulation of tasks can lead to more available parallelism at any given time, and may thereby help to reduce the runtime even further. The poster will show the current concept of our GPU-tasking for the Fast Multipole Method. However, our ideas can be reused for other, especially hierarchical, algorithms. We'll discuss common bottlenecks and pitfalls when applying fine-grained tasking on a GPU.","Laura Morgenstern, Ph.D. Student, Computer Science, Jülich Supercomputing Centre",Aerospace / National labs
NVIDIA GTC 2020,Toward an Exascale Earth System Model with Machine Learning Components: An Update [S21834],"Many have speculated that combining exascale GPU computational power with machine-learning algorithms could radically improve weather and climate modeling. We'll discuss the status of an ambitious project at the U.S. National Center for Atmospheric Research that's moving in that direction. Having achieved performance portability for a standalone version of the Model for Prediction Across Scales-Atmosphere (MPAS-A) on heterogeneous CPU/GPU architectures across thousands of GPUs using OpenACC, our project has begun looking at two new directions. First, we've launched an effort to port the MOM-6 Ocean Model. Second, machine-learning scientists at NCAR and elsewhere have begun evaluating replacing atmospheric parameterizations with machine-learned emulators, including the atmospheric surface layer, cloud microphysics, and aerosol parameterizations. We'll also discuss related efforts to apply machine-learning emulation to model physics.","Richard Loft, Director of Technology Development, Computational and Information Systems Laboratory, National Center for Atmospheric Research",Aerospace / National labs
NVIDIA GTC 2020,Turbulence Forecasting via Neural ODEs [P22054],"Fluid turbulence is characterized by strong coupling across a broad range of scales. Furthermore, besides the usual local cascades, such coupling may extend to interactions that are non-local in scale-space. As such, the computational demands associated with explicitly resolving the full set of scales and their interactions, as in the direct numerical simulation of the Navier-Stokes equations, are so high in most problems of practical interest that modeling of scales and interactions must be reduced before further progress can be made. While popular reduced models are typically based on phenomenological modeling of relevant turbulent processes, recent advances in machine-learning techniques have energized efforts to further improve the accuracy of such reduced models. In contrast to such efforts that seek to improve an existing turbulence model, we propose a machine learning methodology that captures, de novo, underlying turbulence phenomenology without a pre-specified model form.","Peetak Mitra, Computational Physics Intern, Los Alamos National Laboratory",Aerospace / National labs
NVIDIA GTC 2020,Workload Management for Complex Workflows on a GPU-Enabled Heterogeneous System [s21608],"The National Energy Research Scientific Computing Center, the U.S. Department of Energy mission supercomputing center, is integrating experimental science workflows into its existing HPC-centric workload. This introduces new scheduling requirements, including real-time computing demands and complex workflows that depend on external services, data, and devices. These new workflows also need to co-schedule heterogeneous job components, with detailed subtask scheduling on both GPUs and CPUs. It is challenging to maintain scheduler responsiveness to this dynamically changing workload on an extreme-scale HPC system. We'll explain innovations that NERSC introduced to the Slurm scheduler to help meet these challenges, enhancing the scheduling algorithm to enable full-scale jobs and single-core jobs to schedule fairly, and adapting reservation and preemption capabilities to allow real-time computing. We'll also show enhancements to support Linux containers both for user software and system resources, as well as heterogeneous job execution.","Douglas Jacobsen, Group Lead, Computational Systems Group, Lawrence Berkeley National Laboratory",Aerospace / National labs
NVIDIA GTC 2020,"Accelerate ML Lifecycle with Containers, Kubernetes and NVIDIA GPUs (Presented by Red Hat) [S22516]","Data scientists desire a self-service, cloud-like experience to easily access ML modeling tools, data, and GPUs to rapidly build, scale, reproduce, and share ML modeling results with peers and developers. Containers and kubernetes platforms, integrated with NVIDIA GPUs, provide these capabilities to accelerate the training, testing, and deploying the ML models in production quickly. We'll provide an overview of how these technologies are helping solve real-world customer challenges. We'll walk through the various customer use cases and solutions associated with the combination of these technologies. We'll review the key capabilities required in a containers and kubernetes platform to help data scientists easily use technologies (such as Jupyter Notebooks, ML frameworks, etc.) to innovate faster. Finally, we'll share the platform options (for example, Red Hat OpenShift, Kubeflow), and examples of how data scientists are accelerating ML initiatives with containers and Kubernetes.","Tushar Katarki, Product Manager, Red Hat",Software / Cloud Services
NVIDIA GTC 2020,AWDF: An Adaptive Weighted Deep Fusion Architecture for Multi-Modality Learning [P21985],"Deep model fusion is getting lots of attention in dealing with multi-modality learning problems. We propose an Adaptive Weighted Deep Fusion scheme (AWDF) to capture potential relationships among various input sources. It integrates the feature-level and decision-level fusion in one framework. To address the limitations of existing fusing models with fixed weights, we also propose a new scheme named Cross Decision Weights Method (CDWM). It can dynamically learn the weight for each input branch during the fusion process instead of utilizing predefined weights. To evaluate the performance of AWDF, we conducted experiments on three different real-world datasets: Wild Business Terms, Iceberg Detection, and CareerCon. Our experiments demonstrate that AWDF outperforms other fusion systems by a large margin. (This work has been accepted by IEEE Big Data 2019 Special Session on Intelligent Data Mining).","Qinghan Xue, Applied Data Scientist, IBM",Software / Cloud Services
NVIDIA GTC 2020,Container-Based Artificial Intelligence Applications Deployment Platform for GPU High Performance Computing Clusters [P21902],"A container-based AI applications deployment platform was proposed and implemented in our center to solve root privileges and network issues in HPC clusters. A virtual machine is used to simulate the targeted HPC cluster, which has the same or similar runtime environment including compilers, shared libraries, and GPU drivers based on a GPU card attached by PCI pass-through way. Users can have root privileges of virtual machines to set up containers. Further, a user can create containers and deploy AI applications by network tools, because each user has an exclusive virtual machine running on an independent server that connects to internet. The deploy platform and the HPC clusters are connected by a 10Gb/s, or even 100Gb/s network so that users can transfer the container images quickly and easily.","Rongqiang Cao, Associate Professor, Computer Network Information Center, Chinese Academy of Sciences",Software / Cloud Services
NVIDIA GTC 2020,CTR Inference Optimization on GPU [s21416],"The CTR (click-through-rate) model is one of the most important models in internet businesses such as search, recommendation, and advertising. The performance of CTR online service has become a critical impact on user experience/enterprise revenue. With the development of deep learning technology, the CTR model began to adopt deeper and more complex DNN structure, and the computation scale and complexity continued to rise, which demanded more computing power. With the evolution of the CTR model in recent years and the promotion of NVIDIA GPU computing platform, more and more companies have begun to use NVIDIA GPU to accelerate the CTR online inference model, and achieved significant acceleration and got commercial benefits. We'll introduce how to profile and locate the issues when doing optimization CTR inference model on NVIDIA GPU, and provide the general methods on how to solve the issues and get satisfying speedup.","David Wu, Senior Software Engineer, NVIDIA",Software / Cloud Services
NVIDIA GTC 2020,Deploying your Models to GPU with ONNX Runtime for Inferencing in Cloud and Edge Endpoints [s21379],"Models are mostly trained targeting high-powered data centers for deployment — not low-power, low-bandwidth, compute-constrained edge devices. There is a need to accelerate the execution of the ML algorithm with GPU to speed up performance. GPUs are used in the cloud, and now increasingly on the edge. And the number of edge devices that need ML model execution is exploding, with more than 5 billion total endpoints expected by 2022. ONNX Runtime is the inference engine for accelerating your ONNX models on GPU across cloud and edge. We'll discuss how to build your AI application using AML Notebooks and Visual Studio, use prebuild/custom containers, and, with ONNX Runtime, run the same application code across cloud GPU and edge devices like the Azure Stack Edge with T4 and low-powered device like Jetson Nano. We'll also demonstrate distribution strategies for those models, using hosted services like Azure IoT Edge. You'll take away an understanding of the various tradeoffs for moving ML to the edge, and how to optimize for a variety of specific scenarios.","Manash Goswami, Principal Program Manager, Microsoft",Software / Cloud Services
NVIDIA GTC 2020,Edge Computing for Building Machine Learning Pipelines Using Azure Stack (Presented by Microsoft) [S22473],"Machine learning models need to be built closer to the source due to latency and data sovereignty requirements. Microsoft offers industry-leading hybrid cloud solutions, such as Azure Stack Hub and Azure Stack Edge in partnership with NVIDIA GPUs, to drive innovation and deliver a consistent cloud experience for these edge applications. Learn how to unlock customer-use cases by building production-scale models using data-science pipelines on the edge.","Kirtana Venkatraman, Program Manager 2, Microsoft",Software / Cloud Services
NVIDIA GTC 2020,Faster Transformer [s21417],"Recently, models such as BERT and XLNet, which adopt a stack of transformer layers as key components, show breakthrough performance in various deep learning tasks. Consequently, the inference performance of the transformer layer greatly limits the possibility that such models can be adopted in online services. First, we'll show how Faster Transformer optimizes the inference computation of both the transformer encoder and decoder layers. In addition to optimizations on the standard transformer, we'll get into how to customize Faster Transformer to accelerate a pruned transformer encoder layer together with the CUTLASS library.","Bo Yang Hsueh, DevTech , NVIDIA",Software / Cloud Services
NVIDIA GTC 2020,From Hours to Minutes: The Journey of Optimizing Mask-RCNN and BERT Using MXNet [S22483],"Training large deep learning models like Mask R-CNN and BERT takes lots of time and compute resources. Using MXNet, the Amazon Web Services deep learning framework team has been working with NVIDIA to optimize many different areas to cut the training time from hours to minutes.","Haibin Lin, Applied Scientist, AWS AI, Amazon",Software / Cloud Services
NVIDIA GTC 2020,Google Cloud AutoML Video and Edge Deployment [S22022],"Google Cloud will be presenting AutoML Edge Video, a solution using NVIDIA GPUs. AutoML Edge Video allows users to train customized models without knowing how to tune parameters, using Google’s AutoML. In this talk we will focus on our end to end solution for Video Classification and Video Object tracking. To train a model, we use multiple NVIDIA GPUs in parallel for hyper-parameter tuning and transfer learning, allowing us to return high-performance models quickly. We will show how to export models to the edge using a frozen Tensorflow graph, and how to optimize for NVIDIA GPUs including Jetson and Tesla T4.","Yongzhe Wang, Software Engineer, Google",Software / Cloud Services
NVIDIA GTC 2020,GradZip: Gradient Compression Using Alternating Matrix Factorization for Large-scale Deep Learning [P22289],"Our poster first explains the need for a dense and homomorphic algorithm for gradient compression in deep learning, in contrast to the conventional sparse-encoding-based approaches. Then, it details the thinking that came up with GradZip step by step, mainly modifying the existing matrix-factorization in the context of deep learning that reuses the latest all-reduce result as a part of alternating matrix-factorization. The final section captures the key performance result against the popular ResNet50 to demonstrate the effectiveness and efficiency of the proposed compression algorithm.","Minsik Cho, Program Director, IBM",Software / Cloud Services
NVIDIA GTC 2020,Improve ML Training Performance with Amazon SageMaker Debugger (Presented by Amazon Web Services) [S22493],"During ML model training, it’s challenging to ensure that models are progressively learning the correct values for different parameters and to analyze and debug model characteristics without building additional tools, making the process time-consuming and cumbersome. With Amazon SageMaker Debugger, developers can get complete insights into the training process by automating data capture and analysis from training runs without code changes. We'll take a closer look at how you can define rules to monitor and analyze tensors and watch for issues in your model. By monitoring training flow, developers can improve GPU utilization, reduce troubleshooting time during training, and build high-quality models.","Shashank Prasanna, Sr. Developer Advocate, AI/ML, Amazon Web Services",Software / Cloud Services
NVIDIA GTC 2020,Is the Label Trustworthy? Training Better Deep-Learning Models via Uncertainty Mining Net [P22055],"In this work, we consider a new problem of training deep neural networks on partially labeled data with label noise. To our best knowledge, there have been very few efforts to tackle such problems. We present a novel end-to-end deep generative pipeline for improving classifier performance when dealing with such data problems. We call it Uncertainty Mining Net (UMN). During the training stage, we utilize all the available data (labeled and unlabeled) to train the classifier via a semi-supervised generative framework. During training, UMN estimates the uncertainly of the labels to focus on clean data for learning. More precisely, UMN applies the sample-wise label uncertainty estimation scheme. Extensive experiments and comparisons against state-of-the-art methods on several popular benchmark datasets demonstrate that UMN can reduce the effects of label noise and significantly improve classifier performance.","Yang Sun, Applied Research Scientist, IBM",Software / Cloud Services
NVIDIA GTC 2020,Manifold Regularization For Time Series Embedding [P22063],"The wide use of multivariate time-series data has also brought up challenges for humans to parse the long sequences, in its raw form, to identify patterns, create labeled data for supervised modeling approaches, summarize, and explain cause and effect. Variational AutoEncoder (VAE) is a class of deep-generative models that have been used to learn a concise representation of the input data while minimizing information loss. However, when dealing with time-series data these models do not take into account the continuity in the input time-series data. In this work, we use the VAE model along with manifold regularization on the embeddings to learn a smooth representation for the time-series data that can be used to effectively visualize different patterns.","Abhishek Kolagunda, Applied Deep Learning Scientist, IBM",Software / Cloud Services
NVIDIA GTC 2020,Performance Optimization on Quantized Deep-Learning Models [S22484],"Quantization (8-bit) has been broadly adapted in computer vision-related deep-learning models for better inference performance. We present a set of techniques to speed up inference performance on a quantized model (8-bit). At graph level, we proposed a quantization-aware global layout transformation and graph optimization to minimize the data-layout transformation between 32-bit float and 8-bit integer. At the kernel level, we proposed an algorithm to fused the IM2COL with GEMM to save both the GPU memory usage and CUDA launch time caused by the process to generate IM2COL matrix. In addition, we proposed a double-buffering technique to improve the concurrency and reduce the data dependency. By comparing with cuDNN 7.1, our proposed method got up to 5x performance improvement.","Zhenyu Gu, Staff Engineer, Alibaba Group",Software / Cloud Services
NVIDIA GTC 2020,PyTorch from Research to Production [S21928],"Learn how to get your neural network from the PyTorch framework into production. Explore ways to handle complex neural network architectures during deployment. We'll show how to transform a neural network developed in PyTorch into a model ready for a production environment and exemplify the workflow on a conversational AI system. For full understanding, you should be familiar with PyTorch framework and have some interest in model deployment for inference. We’ll demonstrate the neural network system on TensorRT Inference Server (TRTIS).","Grzegorz Karch, Engineer, ASIC, NVIDIA",Software / Cloud Services
NVIDIA GTC 2020,qCUDA-ARM: Virtualization for Embedded GPU Architectures [P21851],"The emergence of the internet of things (IoT) is changing the ways to compute resource acquisition, from centralized cloud data centers to distributed pervasive edge nodes. We investigated two research trends to cope with the small amount of diversity problem for IoT devices and applications for the system design of edge nodes: heterogeneity and virtualization. Our poster presents the integration of those two important trends, and a virtualization system for embedded GPU architectures, called qCUDA-ARM. We evaluated the performance of qCUDA-ARM with three CUDA benchmarks and two-real world applications. For computationally intensive jobs, qCUDA-ARM can perform similar to the native system; for memory-bound programs, qCUDA-ARM can reach up to 90% of native performance.","Che-Rung Lee, Professor, National Tsing Hua University",Software / Cloud Services
NVIDIA GTC 2020,Recommendation Systems Using Distributed Graph Convolutional Networks (GCN) on GPUs (Presented by Amazon Web Services) [S22491],"Deep learning so far has been mostly applied to simple and structured data, such as images, or sequences, such as time-series and language. Most of the information in the world is, however, non-Euclidean and complex and can be presented as graph. Molecules, social network relatedness, and relationships between products and consumers are among the more popular examples. We'll represent an intuitive theory of Graph Convolutional Networks and provide a walkthrough on an implementation for recommender systems using Deep Graph Library and Apache MXNet.","Cyrus Vahid, Principal Solutions Engineer, AI Platforms, Amazon Web Services",Software / Cloud Services
NVIDIA GTC 2020,SPTAG and GPU Acceleration for Approximate Nearest Neighborhood Search [s21561],"The Approximate Nearest Neighborhood (ANN) search algorithm is essential to many machine-learning applications. For instance, vector similarity search, multimedia search, and duplicate entry search all employ ANN for handling very large datasets efficiently. There are two main approaches to implementing ANN: space partitioning trees and locality sensitive hashing. Although hashing methods are accelerated on the GPUs (RAPIDS and FAISS), to our knowledge there is no GPU-accelerated space partitioning ANN algorithm in the literature. We'll present the GPU acceleration of the SPTAG library, where both space partition tree and neighborhood graph construction are accelerated on GPUs. We'll discuss the data structures and algorithms developed for efficient GPU implementation. Finally, we'll discuss the performance characteristics and compare the execution times against a single-socket GPU.","Mingqin Li, Principal Software Engineer Manager",Software / Cloud Services
NVIDIA GTC 2020,Visual Anomaly Detection using NVIDIA DeepStream with Azure IoT [S22675],"In this workshop, you'll discover how to build a solution that can process up to 8 real-time video streams with an AI model on a $100 device, how to remotely operate your device, and demonstrate how you can deploy custom AI models to it. With this solution, you can transform pixels from a camera into insights to know when there is an available parking spot, a missing product on a retail store shelf, an anomaly on a solar panel, a worker approaching a hazardous zone., etc. We'll build this solution using NVIDIA DeepStream on a NVIDIA Jetson Nano device connected to Azure via Azure IoT Edge. DeepStream is a highly-optimized video processing pipeline, capable of running deep neural networks. It is a must-have tool whenever you have complex video analytics requirements like real-time object detection or when employing cascading AI models. IoT Edge gives you the possibility to run this pipeline next to your cameras, where the video data is being generated, thus lowering your bandwidth costs and enabling scenarios with poor internet connectivity or privacy concerns. We'll operate this solution with an aesthetic UI provided by IoT Central and customize the objects detected in video streams using Custom Vision, a service that automatically generates computer vision AI models from pictures.","Paul DeCarlo, Senior Cloud Advocate, Microsoft",Software / Cloud Services
NVIDIA GTC 2020,Wide and Deep Recommender Inference on GPU [S21559],"We'll discuss using GPUs to accelerate so-called ""wide and deep"" models in the recommendation inference setting. Machine learning-powered recommender systems permeate modern online platforms. Wide and deep models have become a popular choice for recommendation problems due to their high expressiveness compared to more traditional machine learning models, and the ease with which they can be trained and deployed using Tensorflow. We'll demonstrate simple APIs to convert trained canned Tensorflow estimators to TensorRT executable engines and deploy them for inference using NVIDIA’s TensorRT Inference Server. The generated TensorRT engines can also be configured to enable reduced-precision computation that leverages tensor cores in NVIDIA GPUs. Finally, we'll show how to integrate these served models into an optimized inference pipeline, exploiting shared request-level features across batches of items to minimize network traffic and fully leverage GPU acceleration.","Alec Gunny, Solutions Architect, NVIDIA",Software / Cloud Services
NVIDIA GTC 2020,Accelerated Analytics Fit for Purpose: Scaling Out and Up (Presented by OmniSci) [S22556],"OmniSci has demonstrated the massive scaling possible using GPUs for computation and visualization. However, not every analytics problem or user persona requires massive scale; rather, our customers say they want proper-sized tools for the various problems they encounter across the enterprise. Our talk will outline the vision for scaling the OmniSci platform from trillions of records in a giant data store to hundreds of millions of records on a laptop, and every form factor in between. Whether you have a massive cluster of servers, a data science workstation, a GPU-enabled laptop, or even a CPU-only laptop, OmniSci can provide the same accelerated analytics experience appropriate for the problem at hand.","Venkat Krishnamurthy , VP Product, OmniSci",Software / Cloud Services
NVIDIA GTC 2020,Accelerating AI Workflows with NGC [S22421],"AI has moved beyond research into mission-critical production. AI is now solving real-world problems for organizations around the globe, who are looking to move faster and do more with their data. NVIDIA provides a range of SDKs that simplify training, inference, and deployment of AI for industries including health care, smart cities, robotics, and telecommunications. We'll cover how NGC, through containers, pre-trained models, helm charts, and SDKs, allows data scientists and developers to build AI solutions faster, DevOps to streamline the development-to-production process, and IT teams to quickly provide compute platforms that the users need. We'll demo how you can take advantage of an SDK to easily build and deploy your AI solution on-premises, at the edge, or in the cloud.","Adel El Hallak, Director of Product Management for NGC, NVIDIA",Software / Cloud Services
NVIDIA GTC 2020,Accelerating Applications for the NERSC Perlmutter Supercomputer Using OpenMP [s21387],"Learn about the NERSC/NVIDIA effort to support OpenMP target offload on the forthcoming NERSC-9 Perlmutter supercomputer with next-generation NVIDIA GPUs. NVIDIA's HPC compilers for C, C++, and Fortran will support a subset of OpenMP 4.5/5.0 for GPU offload on Perlmutter. We'll review the primary features included in this subset, walk through the reasons some features were included and others were left out, and highlight the characteristics that make some OpenMP applications better candidates for GPU acceleration than others. Selected teams from the NERSC Exascale Science Applications Program (NESAP) are porting their applications to NVIDIA GPUs using this subset of OpenMP for target offload. We'll share their early experiences and those of other NERSC developers working with early releases of NVIDIA's OpenMP-offload-enabled compilers.","Christopher Daley, HPC Performance Engineer, Lawrence Berkeley National Laboratory",Software / Cloud Services
NVIDIA GTC 2020,Accelerating Chemistry Modules in Atmospheric Models Using GPUs [S22005],"We’ll explore a novel solution to speed up chemistry modules in atmospheric models. The Multiscale Online Nonhydrostatic AtmospheRe CHemistry model (MONARCH), a chemical weather prediction system developed by the Barcelona Supercomputing Center, is used as our test bed. The model implements a new flexible treatment for gas- and aerosol-phase chemical processes, Chemistry Across Multiple Phases (CAMP), that allows multiple chemical processes (such as gas- and aerosol-phase chemical reactions, emissions, deposition, photolysis, and mass-transfer) to be solved simultaneously as a single system. We’ll discuss innovative ways to speed up the CAMP module, including reducing memory accesses, multiple-cell chemistry solving, adaptation of the most time-consuming functions to GPUs, and heterogeneous computation approaches for CPU/GPU execution. We'll compare the optimized model to state-of-the art chemistry solvers commonly used in the earth sciences community (for example, EBI and KPP).","Christian Guzman-Ruiz, Junior Engineer, Barcelona Supercomputing Center",Software / Cloud Services
NVIDIA GTC 2020,Accelerating Hyperparameter Tuning with Container-Level GPU Virtualization [S21463],"Many people think that hyperparameter tuning requires a large number of GPUs to get optimal results quickly. That's generally true, but to what extent? We'll present what we've learned about finding a sweet spot to balance both costs and accuracy by exploiting partitioned GPUs with Backend.AI's container-level GPU virtualization. Our benchmark includes distributed mnist, cifar10 transfer learning and TGS salt identification cases using AutoML with network morphism and ENAS tuner with NNI running on NGC-optimized containers on Backend.AI. You'll get a tangible guide to plan and deploy your GPU infrastructure capacity in a more cost-effective way.","Jeongkyu Shin, CEO/Researcher, Lablup Inc.",Software / Cloud Services
NVIDIA GTC 2020,Acceleration of Test Data Quality Assurance Technology Using Neuron Coverage [P21809],"“Neuron coverage test” is one way to test quality in deep-learning systems. We've implemented “DeepXplore,” which is a quality test method for DL systems that applies our unique high-speed inference library. DeepXplore is a framework that can systematically test DL systems and automatically generate misrecognition images from the coverage rate of the trained model and the output difference of the comparison model. However, a large amount of inference would be necessary to automatically generate these misrecognition images, and that could require a lot of processing time. We sped up processing by using our unique high-speed inference library to implement DeepXplore.","Yuki Shindo, Staff, Computermind Corp.",Software / Cloud Services
NVIDIA GTC 2020,AceCAST GPU-Enabled Weather Research and Forecasting Model Development and Applications [P22064],"The Weather Research and Forecasting (WRF) model is an open-source, mesoscale numerical weather prediction system designed to serve both operational forecasting and atmospheric research needs. It is the most widely used regional weather forecasting model and a top 5 HPC application worldwide. TQI has implemented an OpenACC/CUDA-based version of WRF to take advantage of NVIDIA GPUs. By utilizing GPUs, the measured performance benefits enable better forecasting through higher resolution, temporal/geographical extents, and so on. Our poster discusses the GPU implementation of the model as well as performance benchmarks demonstrating the model’s practical performance benefits. TQI has also developed a cloud-based solution for running end-to-end AceCAST GPU-WRF workflows on AWS. This provides a solution for a wide range of users who would otherwise not have access to GPU-based compute resources, and automates a highly complex process that is a significant barrier for researchers and operational weather forecasters.","Samuel Elliott, Director of NWP Solutions, TempoQuest Inc.",Software / Cloud Services
NVIDIA GTC 2020,Advanced Scientific Visualization with NVIDIA Omniverse [S21973],"Historically, scientific visualization has served two main purposes: on one hand, for the scientist to gain insight into the data and the processes under investigation; on the other hand, for outreach and education, to communicate scientific results to the public and to funding bodies. The tools associated with these workflows are hugely diverse, often preventing scientists from getting access to the highest quality visuals for telling their stories. Especially in times where the general public has been spoiled with computer-generated content in movies, the gap in visual quality between scientific content and entertainment content has never been bigger. Tools that seamlessly fit into the scientific workflow while being capable of producing the highest quality visuals are therefore needed. We'll outline how Omniverse, NVIDIA’s collaboration platform for 3D content creation, can be leveraged to greatly simplify, accelerate, and enhance scientific visualization. We'll introduce the overall Omniverse architecture and then focus on the scientific workflow. We cover how content can be ingested into Omniverse, as well as how content from different sources can be fused and how to produce rich, high-quality visualizations. We'll then show how to augment these visualizations with the embedded real-time physics engine PhysX. Further, we'll demonstrate how, through Omniverse, virtual reality setups that incorporate scientific visualizations can be defined quickly and easily.","Kees van Kooten, Scientific Visualization Software Engineer, NVIDIA",Software / Cloud Services
NVIDIA GTC 2020,Advances in Real-Time 3D Hologram Generation [S21248],"We'll introduce VividQ's software pipeline, which facilitates the extraction of 3D point cloud data and conversion to a hologram to be displayed using diffractive optics. This allows for the real-time projection of scenes as truly 3D holographic images. For example, video games can now be viewed in true 3D with full depth of focus. We'll explain why this problem is computationally hard, and how using NVIDIA GPUs allows us to overcome it. Recent advances in hologram generation procedures will be explored, including benefits from newer GPU models and the application of multiple GPUs to computing for a binocular holographic system. We'll also discuss results from VividQ's latest holographic augmented reality head-mounted display prototype.","Tom Durrant, Chief Development Officer, VividQ",Software / Cloud Services
NVIDIA GTC 2020,A Faster Radix Sort Implementation [S21572],"We'll present a faster implementation of the least-significant-digit radix sort. Decoupled look-back is used to reduce the number of memory operations per partition pass from 3N to 2N. A faster partition implementation inside the thread block is used. For 32-bit keys, we use four partition passes, with 8 bits per digit. On V100 sorting 64 million random UInt32 keys, our implementation achieves the speed of 16 Gkey/s, which is more than 2x faster than cub::DeviceRadixSort.","Andrey Adinets, AI Devtech, NVIDIA",Software / Cloud Services
NVIDIA GTC 2020,AIaaS: Scaling AI Infrastructure for the Enterprise from DGX-1 to SuperPOD [S21996],"Are you getting the most out of your GPU-accelerated hardware? AIaaS (AI-as-a-Service) deployments provide powerful new ways for enterprises to stand up AI infrastructure on top of GPU-accelerated servers (such as NVIDIA DGX Servers). Data scientist users can interact with the cluster via GUI or a simplified command-line interface, where knowledge of underlying container, orchestration, and scheduling technologies is obfuscated. Such systems allow administrators to maximize use of resources and users to get more work done. We'll focus on the variety of AIaaS stacks, considerations, and how to deploy them. We'll demonstrate scaling common AI workloads from a single GPU on an NVIDIA DGX-1 to a SuperPOD cluster (64x DGX-2). We'll also discuss methods to integrate storage and manage datasets, tie in authentication and authorize users, train on NGC containers, and deploy models to production.","Michael Balint, Senior Product Manager, NVIDIA",Software / Cloud Services
NVIDIA GTC 2020,AI Argus: A Unique Insight Into Logistics [P21812],"As of August 2019, AI Argus, the leading domestic intelligent video analytics platform powered by NVIDIA TeslaT4 and Xavier servers, has been deployed and applied in more than 200 distribution centers and sorting centers in China. We'll focus on algorithms such as loading-rate detection and violated-action pattern detection. AI Argus introduced the Package Lifecycle Tracking System (PLTS), which uses the whole SF's 310,000-channel camera video data to match the Operator's barcode data collector for locating the courier package on each operating node. It shows that the detection of damage caused by penetrating damage, moisture, wrinkles/pressure is significant. We'll further develop and research product performance indicators based on T4 and Xavier. We expect that in the coming year, AI Argus will deploy thousands of edge servers and become the first truly large-scale edge computing platform in China's logistics industry.","Neo Song, Chief Engineer, SF Techology",Software / Cloud Services
NVIDIA GTC 2020,AI/ML with vGPU on Openstack or RHV Using Kubernetes [S22106],"By sharing GPU resources for AI/ML, you can better utilize on-premises hardware and gain flexibility without moving sensitive workflows into the cloud. Learn how Red Hat Openstack Platform and Red Hat Virtualization are bringing agility to AI/ML accelerated workloads with vGPU. We'll describe how Red Hat contributes to new vGPU capabilities like SR-IOV and live migration support. This makes setting up AI/ML within a Kubernetes system even simpler and more reliable, as the workloads can be migrated and SR-IOV functionality boosts the performance and usability of the vGPU device.","Erwan Gallen, Senior Principal Product Manager, Red Hat",Software / Cloud Services
NVIDIA GTC 2020,An Overview of GPU Recurrent Neural Network Performance [S21301],"We'll focus on performance of recurrent neural networks (RNNs) at the sub-framework level. There are several distinct methods one can use to implement an RNN, each with advantages and disadvantages, and the right choice will depend on both the target GPU and the network hyperparameters. We'll cover these methods in detail and give approximate hyper-parameter ranges for each method.","Jeremy Appleyard, AI DevTech, NVIDIA",Software / Cloud Services
NVIDIA GTC 2020,AutoFAQ: Automation of Customer Support for Most Common Questions [s21252],"In most support systems, up to 70% of user questions are very similar to each other. Instead of manually answering each of them, it makes sense to automate it. We'll discuss the business value of such systems, and how to integrate them into processes; how to develop an architecture for automating question-answering; how to set up a training loop, and which algorithms to choose; which models need to be trained, and why; and state-of-the-art language modeling models. Knowledge of Python, NLP, language models, and cloud computing will be helpful, but it's not essential.","Vitaly Davydov, CEO, Poteha Labs",Software / Cloud Services
NVIDIA GTC 2020,Building a Smart Language-Understanding System for Conversational AI with HuggingFace Transformers [S22647],"In this session, HuggingFace showcases an example of a natural language understanding pipeline to create an understanding of sentences, which can then be used to craft a simple rule-based system for conversation. They'll leverage the famous HuggingFace transformers and showcase the powerful yet customizable methods to implement tasks such as sequence classification, named-entity recognition, natural language generation, or question answering. These tasks will be joined to create a basic NLU pipeline to get the most out of a sentence or text, using transformer models such as BERT to provide state-of-the-art results. These methods leverage the PyTorch or TensorFlow numerical computation frameworks, which can leverage the power of GPUs to radically speed up the inference.","Lysandre Debut, Machine Learning Engineer, Hugging Face, Inc.",Software / Cloud Services
NVIDIA GTC 2020,CLOUDXR: Streaming AR and VR [S22178],"Learn how the NVIDIA CloudXR SDK can help you drive immersive extended reality (XR) experiences from anywhere using NVIDIA Quadro GPUs. NVIDIA CloudXR is the groundbreaking technology that delivers wireless virtual and augmented reality from NVIDIA RTX GPUs across performant networks. By dynamically adjusting to network conditions, CloudXR maximizes image quality and frame rates. Scale XR capabilities throughout your enterprise by combining CloudXR with NVIDIA GPU virtualization software to provide seamless experiences comparable to the most robust tethered configurations. Included in this presentation will be examples of partner ISVs extending the OpenVR applications into XR streaming applications.","Greg Jones, Senior Manager, Global Business Development for Enterprise XR, NVIDIA",Software / Cloud Services
NVIDIA GTC 2020,Combined Python/CUDA JIT for Flexible Acceleration in RAPIDS [S21393],"We'll introduce our design and implementation of a framework within RAPIDS/cuDF that enables compiling Python user-defined functions and inlining them into native CUDA kernels. Our framework uses the Numba Python compiler and Jitify CUDA just-in-time (JIT) compilation library to provide cuDF users the flexibility of Python with the performance of CUDA as a compiled language. An essential part of the framework is a parser that parses the CUDA PTX function, which is compiled from the Python UDF, into an equivalent CUDA device function that can be inlined into native CUDA C++ kernels. Learn how our approach makes it possible for non-expert Python users to extend optimized dataframe operations with their own Python UDFs, and enables more flexibility and generality for high-performance computations on dataframes in RAPIDS.","Jiqun Tu, NVIDIA",Software / Cloud Services
NVIDIA GTC 2020,CuPy Overview: NumPy Syntax Computation with Advanced CUDA Features [S22471],"We'll introduce CuPy, describing the advantages of the library and how it is cleanly exposing in Python multiple CUDA state-of-the art libraries such as cuTENSOR or cuDNN. Discover the CuPy advantages and how they can use it to experience performance gains in their NumPy codes without any major changes.","Crissman Loomis, Engineer/Business Development, Preferred Networks",Software / Cloud Services
NVIDIA GTC 2020,Distributed Deep Learning with Horovod on Spark [s21300],"We'll show how to scale distributed training of TensorFlow, PyTorch, and MXNet models with Horovod, a library designed to make distributed training fast and easy to use. We'll explain the role of Horovod in taking a model designed on a single GPU and training it on a cluster of GPU servers with just a few additional lines of Python code. We'll also explore how Horovod has been used across the industry to scale training to hundreds of GPUs, and the techniques that are used to maximize training performance. Although frameworks like TensorFlow and PyTorch simplify the design and training of deep learning models, difficulties usually arise when scaling models to multiple GPUs in a server or multiple servers in a cluster.","Travis Addair, Senior Software Engineer, Uber Technologies",Software / Cloud Services
NVIDIA GTC 2020,Document Understanding Platform: Extracting Structured Information from Financial Documents [S21459],"In today’s highly automated world of financial services, consumers, the self-employed, and small business owners still face the tedious and time-consuming task of entering data manually from paper documents. Document Understanding is a company-wide initiative at Intuit that aims to make data preparation and entry obsolete through the application of computer vision and machine learning. We'll describe the design and modeling methodologies used to build this platform-as-a-service. Intuit’s Document Understanding Platform orchestrates a variety of services and machine-learning capabilities using structured and unstructured documents uploaded by users, regardless of format (smartphone photos, PDFs, forms, etc.), and presents high-confidence results back within the company’s product ecosystem.","Joy Rimchala, Data Scientist, Intuit",Software / Cloud Services
NVIDIA GTC 2020,Efficient 3D Convolutional Network Design for Human Instance-Level Video Action Recognition [P22275],"We discuss the design factors of the human instance-level video action recognition, especially on edge computing hardware like a Jetson AGX Xavier embedded module. First, we present our research motivation in human action recognition fields. Second, we talk about our pipeline for human instance-level video action recognition. Third, we explain how to enhance the accuracy using person detectors such as YOLOv3 and Mask RCNN. Finally, we discuss how to utilize action recognition models in terms of accuracy to action inference speed ratio on the Xavier module, which is very useful in practice.","Inwoong Lee, Researcher, Artificial Intelligence Research Institute (AIRI)",Software / Cloud Services
NVIDIA GTC 2020,Espresso: A Fast End-to-End Neural Speech Recognition Toolkit [s21239],"We'll introduce Espresso, an open-source, modular, extensible, end-to-end neural automatic speech recognition (ASR) toolkit based on the deep learning library PyTorch and the popular neural machine translation toolkit fairseq. Espresso supports distributed training across GPUs and computing nodes, and features various decoding approaches commonly employed in ASR, including look-ahead word-based language model fusion, for which a fast, parallelized decoder is implemented. Espresso achieves state-of-the-art ASR performance on the WSJ, LibriSpeech, and Switchboard datasets, among other end-to-end systems without data augmentation, and is up to 11x faster for decoding than similar systems, such as ESPnet.","Yiming Wang, Ph.D. Student, Johns Hopkins University",Software / Cloud Services
NVIDIA GTC 2020,Evaluation of a Multi-GPU Optimized Non-Hydrostatic Ocean Model with Multigrid Preconditioned Conjugate Gradient Method [P21930],"The conjugate gradient method with multigrid preconditioners (MGCG) is used in scientific applications because of its high performance and scalability with many computational nodes. GPUs are thought to be good candidates for accelerating such applications, with many meshes where an MGCG solver could show high performance. No previous studies have evaluated and discussed the numerical character of an MGCG solver on GPUs. Consequently, we implemented and optimized our “kinaco” numerical ocean model with an MGCG solver on GPUs. We evaluated its performance and discussed inter-GPU communications on a coarse grid on which GPUs could be intrinsically problematic. We achieved 3.9x speedup compared to CPUs, and learned how inter-GPU communications depend on the number of GPUs and the aggregation level of information in a multigrid method.","Takateru Yamagishi, Chief, RIST",Software / Cloud Services
NVIDIA GTC 2020,EXtended Particle System (XPS): High-Performance Particle Simulation [P21775],"We'll present a framework for a GPU-based discrete element method solver for real-world particle problems featuring coupling ability with a CPU-based computational fluid dynamics solver provided by one of our partners. Current features include support of non-spherical shapes like multi-sphere particles and true bi-convex tablets, plus heat transfer and liquid transfer. We recently integrated a smoothed-particle hydrodynamics solver, as well as support for polyhedral-shaped solid particles.","Hermann Kureck, Scientist, RCPE GmbH",Software / Cloud Services
NVIDIA GTC 2020,Exterminating Buffer Overflows and Other Embarrassing Vulnerabilities with SPARK Ada on Tegra [S21122],"Since 2018, NVIDIA has been actively investigating the SPARK Ada programming language to develop their most sensitive pieces of firmware. We'll explain how users of the NVIDIA hardware can also benefit from this language choice when developing applications for the Tegra SoC. The benefits of the technology, from a cyber security point of view, will be demonstrated through the use of formal methods, allowing trivial proof of properties, such as absence of buffer overflows. We'll describe using this technology on top of ARM processor cores, as well as methodologies for applications leveraging the GPU, either through existing libraries interfaces/CUDA code, or through an experiential port of Ada/OpenACC, which allows applications directly written in Ada or SPARK to offload to the GPU.","Quentin Ochem, Lead of Business Development and Technical Account Management, AdaCore",Software / Cloud Services
NVIDIA GTC 2020,Fast Distributed Joins with RAPIDS and UCX [s21482],"There are numerous optimized single-GPU join implementations (such as RAPIDS cuDF), but scaling out to multiple GPUs across multiple nodes is challenging. The repartitioned join approach is one of the most popular distributed join algorithms, featuring all-to-all exchange as the main communication pattern. We'll show how to leverage UCX for efficient all-to-all implementation and demonstrate various optimization strategies, such as reusing communication buffers to speed up GPU-to-GPU transfers and overlapping compute with communications. The implementation is designed to reuse RAPIDS components for single-GPU, and scales to NVLINK and Infiniband systems. Our latest performance results demonstrate that a single DGX-2 can achieve 220 GB/s throughput for joining 8B/8B key-value pairs, while 18 DGX-1V nodes (144 GPUs) connected over IB achieve 503 GB/s, which is comparable with 244 CPU nodes (2K cores) in the best-known distributed CPU implementation.","Nikolay Sakharnykh, Developer Technology Engineer, NVIDIA",Software / Cloud Services
NVIDIA GTC 2020,From High to Low Level: A Comparative Study of Programming Approaches for NVIDIA GPUs [S21308],"Learn about various available methods to program NVIDIA GPUs, from using high-level GPU libraries in Python to optimized CUDA C programming. We'll discuss the development and performance of several implementations of software to simulate the 2D Ising model for spin systems, comparing the different programming approaches in terms of development effort and simulation performance. We'll show how Python, in combination with the Numba/CuPy packages, enables users to write programs, with all the productivity benefits of a high-level language, that can still provide competitive performance to lower-level implementations. We'll also highlight some performance pitfalls encountered with these tools and discuss how we addressed them. Finally, we'll compare performance against published results on other hardware platforms and show that even simple programming methods on GPUs can provide competitive performance, while our optimized low-level implementation can rapidly simulate lattices sizes outside the scope of comparable field-programmable gate array solutions.","Joshua Romero, Developer Technology Engineer, NVIDIA",Software / Cloud Services
NVIDIA GTC 2020,GPipe: Efficient Training of Giant Neural Networks Using Pipeline Parallelism [S21873],"Scaling up deep neural network capacity is an effective way to improve model quality for several different machine learning tasks. In many cases, increasing model capacity beyond the memory limit of a single accelerator has required developing special algorithms or infrastructure. These solutions are often architecture-specific, and do not transfer to other tasks. To address the need for efficient and task-independent model parallelism, we'll introduce GPipe, a pipeline parallelism library that allows scaling any network that can be expressed as a sequence of layers. By pipelining different sub-sequences of layers on separate accelerators, GPipe provides the flexibility of efficiently scaling a variety of different networks to gigantic sizes. Moreover, GPipe utilizes a novel batch-splitting pipelining algorithm, resulting in almost linear speedup when a model is partitioned across multiple accelerators.","Yanping Huang, Software Engineer, Google",Software / Cloud Services
NVIDIA GTC 2020,How to Build a Multi-Camera Media Server for AI Processing on Jetson [S22396],"We'll build a simple multi-camera media server for AI processing on a Jetson Board and demonstrate how, by using GStreamer Daemon, Interpipes, and DeepStream, you can develop a scalable and robust prototype to capture from multiple cameras using GMSL2 Virtual Channels. Besides achieving real-time deep learning inference, the server is completely dynamic. We'll show its flexibility by triggering actions such as taking a snapshot, recording a video, or starting a network streaming when a specific prediction is made. By the end of this session you'll have acquired the framework basics that will allow you to scale to your specific multimedia and AI needs.","Carlos Rodriguez, Embedded Software Team Lead, RidgeRun",Software / Cloud Services
NVIDIA GTC 2020,Knowledge Transfer Graph for Deep Collaborative Learning [P22383],"We present a new graph-based approach for more flexible and diverse combinations of knowledge transfer for deep collaborative learning. To achieve the knowledge transfer, we propose a novel graph representation called knowledge transfer graph that provides a unified view of the knowledge transfer and has the potential to represent diverse knowledge transfer patterns. We also propose four gate functions that are introduced into loss functions. The four gates, which control the gradient, can deliver diverse combinations of knowledge transfer. Searching the graph structure enables us to discover more-effective knowledge transfer methods than a manually designed one. Experimental results show that the proposed method achieved significant performance improvements and was able to find remarkable graph structures.","Hironobu Fujiyoshi, Professor, Chubu University",Software / Cloud Services
NVIDIA GTC 2020,Learning Human Objectives by Evaluating Hypothetical Behavior [P22312],We present ReQueST: an algorithm for training reinforcement learning agents from human feedback in the presence of unknown unsafe states.,"Siddharth Reddy, Ph.D. Student, University of California, Berkeley",Software / Cloud Services
NVIDIA GTC 2020,Modularizing Natural Language Processing [S21560],"Recent success and growth in natural language processing and artificial intelligence have given the world many new applications, techniques, models, and architectures. We'll show how appropriate abstraction and modularization can streamline both development and deployment of NLP technologies. We'll provide a systematic overview of NLP abstractions and breakdown, the insights of machine learning integration, and the designs of NLP systems for fast module development. You'll learn to use off-the-shelf tools to practice the modularized NLP and build practical applications. Our talk is suitable for researchers and practitioners with an intermediate understanding of NLP and ML concepts and applications, and a strong interest in real-world NLP application design and development.","Zhengzhong Liu, Research Assistant, Carnegie Mellon University",Software / Cloud Services
NVIDIA GTC 2020,Motion Reasoning for Goal-Based Imitation Learning [P22363],"We address goal-based imitation learning, where the aim is to output the symbolic goal from a third-person video demonstration. This enables the robot to plan for execution and reproduce the same goal in a completely different environment. The key challenge is that the goal of a video demonstration is often ambiguous at the level of semantic actions. The human demonstrators might unintentionally achieve certain subgoals in the demonstrations with their actions. Our main contribution is to propose a motion reasoning framework that combines task and motion planning to disambiguate the true intention of the demonstrator in the video demonstration. This allows us to robustly recognize the goals that cannot be disambiguated by previous action-based approaches.","De-An Huang, Ph.D. Student, Stanford University",Software / Cloud Services
NVIDIA GTC 2020,"Named Tensors, Model Quantization, and the Latest PyTorch Features [S22145]","PyTorch, the popular open-source ML framework, has continued to evolve rapidly since the introduction of PyTorch 1.0, which brought an accelerated workflow from research to production. We’ll deep dive on some of the most important new advances, including the ability to name tensors, support for quantization-aware training and post-training quantization, improved distributed training on GPUs, and streamlined mobile deployment. We’ll also cover new developer tools and domain-specific frameworks including Captum for model interpretability, Detectron2 for computer vision, and speech extensions for Fairseq.","Joseph Spisak, Product Manager, Facebook",Software / Cloud Services
NVIDIA GTC 2020,Opening Up the Black Box: Model Understanding with Captum and PyTorch [S22147],"PyTorch, the popular open-source ML framework, has continued to evolve rapidly since the introduction of PyTorch 1.0, which brought an accelerated workflow from research to production. We’ll deep dive on some of the most important new advances, including the ability to name tensors, support for quantization-aware training and post-training quantization, improved distributed training on GPUs, and streamlined mobile deployment. We’ll also cover new developer tools and domain-specific frameworks including Captum for model interpretability, Detectron2 for computer vision, and speech extensions for Fairseq.","Narine Kokhlikyan , Research Scientist, Facebook AI",Software / Cloud Services
NVIDIA GTC 2020,Optimized Image Classification on the Cheap [s21598],"We'll anchor on building an image classifier trained on the Stanford Cars dataset to evaluate two approaches to transfer learning — fine tuning and feature extraction — and the impact of hyperparameter optimization on these techniques. Once we define the most performant transfer-learning technique for Stanford Cars, we'll explore Bayesian Optimization as a black-box optimization technique to tune image-transformation parameters required to augment the model, using the downstream image classifier’s performance as the guide. Drawing on a rigorous set of experimental results can help us answer the question: How can resource-constrained teams make tradeoffs between efficiency and effectiveness using pre-trained models?","Meghana Ravikumar, Machine Learning Engineer, SigOpt",Software / Cloud Services
NVIDIA GTC 2020,Optimizing Work Scheduling and Memory Usage for Complex Database Queries on GPUs [P22017],"The growth of data volumes of online analytical processing systems has made the GPU an attractive platform for executing data analytical queries. Besides the large data sizes, most of the real-world analytical queries are very complex, with multiple joins performed across multiple tables. In this session, we'll show how CUDA Graphs and RAPIDS cuDF components enable a much easier acceleration of complex queries on the GPU. In addition to programmability improvements, our design achieves better utilization of system resources for complex queries, such as optimized data pipeline, maximized task parallelism, and more efficient memory management. Moreover, we compare CUDA Graphs approach with the traditional approach using multiple streams and showcase how we achieve temporary memory reusing across different joins.","Nikolay Sakharnykh, Developer Technology Engineer, NVIDIA",Software / Cloud Services
NVIDIA GTC 2020,Optuna: An Eager Hyperparameter Optimization Library [s21291],"We'll present Optuna, an open-source, next-generation hyperparameter optimization framework with three novel design criteria: (1) an eager API that allows users to concisely construct dynamic, nested, or conditional search spaces; (2) efficient implementation of both sampling and early-stopping strategies; and (3) an easy-to-set-up, versatile architecture that can be deployed for various purposes, ranging from simple scaling on distributed GPUs to lightweight experimentation, conducted via interactive interface. Optuna is the first optimization software designed to run in Eager mode. We'll present the basic usage of Optuna, describe the design techniques that we needed to meet the above criteria during development, and share our experiences leveraging Optuna to tune hyperparameters on single and multiple GPUs.","Shotaro Sano, Engineer, Preferred Networks",Software / Cloud Services
NVIDIA GTC 2020,Scaling Deep Learning for Automatic Speech Recognition [S21838],"We'll discuss challenges of scaling automatic speech recognition (ASR) workloads with wav2letter++, a fast C++ toolkit for ASR. We'll introduce distributed training techniques used to achieve almost linear scalability and compare wav2letter to other popular ASR toolkits. Constant increase in model and dataset sizes, along with current trends toward unsupervised and semi-supervised learning, require squeezing out every bit of performance. In addition to distributed training, we'll cover other approaches for faster training and for training large models.","Jacob Kahn, Research Engineer, Facebook",Software / Cloud Services
NVIDIA GTC 2020,Software-Based Compression for Analytical Workloads [S21597],"Real-world analytical pipelines have very large memory requirements and stress the CPU-GPU and GPU-GPU interconnects. The GPU memory size is limited, and the data is often offloaded to CPU memory for further processing on the GPU later. That can present a significant bottleneck for the end-to-end pipeline. Fast compression and decompression can improve performance by reducing the amount of data to be sent over the interconnect, or even completely eliminate the need to offload data by storing it in GPU memory in compressed format and performing subsequent operations on the compressed data. We'll survey various parallel compression algorithms, from LZ-based to run-length encoding, dictionary, and bit-packing. Efficient GPU implementations of cascaded schemes for analytical workloads will be presented, along with user-friendly interfaces for GPU applications. Our best approaches can achieve up to 77x compression ratio for columns from Fannie Mae's Loan Performance dataset, and maintain 250-350GBps compression/decompression speed on Tesla V100.","Nikolay Sakharnykh, Developer Technology Engineer, NVIDIA",Software / Cloud Services
NVIDIA GTC 2020,"The Future of GPU Rendering: Real-Time Raytracing, Holographic Displays, and Light Field Media [S22153]","We'll present our vision for the future of GPU rendering and how it will impact gaming, VFX, media, and design in the 2020s and beyond. We'll detail how the future of media lies in holographics, light field technologies, and real-time rendering, and how OTOY is working to help drive that future through OctaneRender.","Jules Urbach, CEO, OTOY Inc",Software / Cloud Services
NVIDIA GTC 2020,The SpeechBrain Project [s21648],"SpeechBrain is an open-source project that aims to develop an all-in-one speech toolkit based on PyTorch. Our goal is to create a single, flexible, user-friendly toolkit that can be used to easily develop state-of-the-art speech technologies. SpeechBrain will be a standalone framework that can significantly speed up research and development of speech and audio processing techniques. Indeed, it's a lot easier to familiarize oneself with a single toolkit than to learn several different frameworks, as one must do today. Moreover, using a single platform makes it easier to build a strong and fruitful community where members can share models, codes, baselines, and suggestions with a possible positive impact in the field of speech technologies. SpeechBrain is currently under development, and a first alpha version will be available in the next months. We'll describe the motivations, goals, and current status of the project.","Mirco Ravanelli, Ph.D., Mila - Université de Motréal",Software / Cloud Services
NVIDIA GTC 2020,TNL: Template Numerical Library for Modern Parallel Architectures [P21800],"We'll present a numerical library that's being developed int our department. The library's goal is to offer flexible data structures and algorithms for the high performance computing, and especially for GPUs. Its design benefits strongly from the modern features of C++. In many situations, the user of TNL may write code independently on the hardware platform. The library is available at www.tnl-project.org.","Tomáš Oberhuber, Researcher, Czech Technical University in Prague",Software / Cloud Services
NVIDIA GTC 2020,Toward Industrial LES/DNS in Aeronautics: Leveraging OpenACC for Massively Parallel CPU+GPU Simulations [s21958],"We'll describe recent advances toward industrial LES/DNS computational fluid dynamics within the scope of the EU TILDA (Towards Industrial LES/DNS in Aeronautics) project. The TILDA project aims to complete high-fidelity industrial LES/DNS simulations with upwards of 1 billion degrees of freedom, with a turnaround time on the order of one day. Achieving this requires near-linear efficiency on massively parallel, heterogeneous CPU+GPU compute resources. We'll describe the development of FineFR, a high-order CFD solver supporting heterogeneous CPU+GPU architectures. We'll emphasize the highly tuned OpenACC implementation, allowing very efficient data locality with minimal code intrusion. Finally, we'll present benchmark data and demonstration computations from the OLCF Summit Supercomputer showing near-linear scalability on upwards of 50,000 CPU cores and 7,000 NVIDIA GPUs.","David Gutzwiller, Software Engineer, head of HPC, Numeca",Software / Cloud Services
NVIDIA GTC 2020,Toward INT8 Inference: Deploying Quantization-Aware Trained Networks using TensorRT [s21664],"We'll describe how TensorRT can optimize the quantization ops and demonstrate an end-to-end workflow for running quantized networks. Accelerating deep neural networks (DNN) is a critical step in realizing the benefits of AI for real-world use cases. The need to improve DNN inference latency has sparked interest in lower precision, such as FP16 and INT8 precision, which offer faster inference time. Two prevalent techniques to convert FP32 DNNs to INT8 precision are post-training quantization and quantization-aware training (QAT). TensorRT, a platform for high-performance deep learning inference, supports post-training quantization by performing calibration on the trained model, which quantizes the weights and activations. However, in some cases post-training quantization can degrade accuracy when converting a FP32 model to its INT8 counterpart. QAT introduces quantization ops to achieve higher accuracy by simulating the process for lower-precision quantization during training.","Dheeraj Peri, Deep Learning Software Engineer, NVIDIA",Software / Cloud Services
NVIDIA GTC 2020,Training and Deploying Conversational AI Applications with NeMo and Jarvis [S21211],"Learn how to build speech recognition, natural language processing, and speech synthesis services with Neural Modules. First, we'll cover the basics of the NeMo toolkit for training and fine-tuning conversational AI models on your data. Then, we'll discuss how to use Jarvis to deploy and combine these services into a complete conversational AI solution.","Oleksii Kuchaiev, Senior Applied Scientist, NVIDIA",Software / Cloud Services
NVIDIA GTC 2020,XLNet Optimization Using CUDA [S21478],"XLNet, a generalized autoregressive pretraining method, achieved great results on several natural language processing tasks. Compared to the previous language model, XLNET has advantages like being able to process long sentences, and avoids the disadvantage of using special tokens. However, as far as we know, there still isn't proper performance optimization for XLNet using CUDA, which would demand more inference time and hinder XLNET's wide deployment. We first ran the performance analysis of XLNet using its Tensorflow code. Then we optimized XLNet with these aspects:
For relative positional encoding, we optimized its parallelization with the help of cuBlas;
We customized the corresponding self-attention architecture based on the attention code in FastTransformer; and
We used kernel fusion and other CUDA optimization strategies to speedup XLNet.","Christina Zhang, DevTech Engineering, NVIDIA",Software / Cloud Services
NVIDIA GTC 2020,3D Analysis Data Generation from X-ray Data for HealthTech by DGXs [P22304],"In Japan, research on regenerative medicine is advancing, various levels of treatment are advancing, and laboratories dealing with nearly 1,000 regenerative medicine cells are starting to appear every week. Research in these regenerative treatments requires new medical imaging—to increase the resolution of two-dimensional image data, for example. Rather than discovering new indicators that map annotations, it is easier to use existing indicators or to generate images for use. Currently, a 3D image is generated from a 2D image by X-ray, and a 3D curved surface learned by a Tesla chip is developed to generate a realistic 3D image. By generating even sliced images, we provide digital data materials that can be used as reference for proceeding to MRI after X-ray imaging.","Shigehisa Omatsu, CEO, Stellaplace, Inc.",Healthcare & Life Sciences
NVIDIA GTC 2020,Accelerate Quantitative Clinical Neuroimaging Analysis with AI: Steps Toward Personalized Disease Progression Monitoring in Clinical Neurology [S21421],"Quantitative neuroimaging analysis can further extract critical information from diagnostic imaging. For instance, brain-volume change has been considered as a critical biomarker in neurodegenerative disease progression. Instead of describing the brain shrinking process as ""moderate"" or ""severe"" through visual inspection, now we can, with computing, precisely measure the brain-tissue loss at the scale of 0.1% with conventional MRI scans. However, precision and accuracy of the analysis often require an expert level of quality control, usually carried out by certified imaging analysts at dedicated imaging reading centers. Deep learning and GPU acceleration make it possible to transfer complex and sophisticated analysis into fully automated assessment in daily clinics. We'll share views from both imaging scientists and clinicians on how AI is accelerating clinical imaging quantitative biomarker research.","Tim Wang, Director of Operations, Sydney Neuroimaging Analysis Centre",Healthcare & Life Sciences
NVIDIA GTC 2020,"Accelerating Cancer Research: VDI by Day, Compute by Night [S21845]","The Netherlands Cancer Institute–Antoni van Leeuwenhoek Hospital (NKI-AVL) is one of the top 10 comprehensive cancer centers in the world. By combining cancer care, research, and by exchanging knowledge internationally, they make a significant contribution to solving the cancer problem in the 21st century. To meet that challenge, NKI-AVL built a software-defined infrastructure to accelerate research and enhance efficiency for clinicians. During daytime, the VDI infrastructure will give health care professionals fast, remote, and secure access to patient data. At night, the same VDI platform is utilized by researchers to execute computational GPU workloads. As a result, the high-performance and flexible IT infrastructure enables physicians and nurses to spend more focused time on patient care, and researchers to advance new discoveries in cancer treatment.","Roel Sijstermans, IT Manager, Antoni van Leeuwenhoek Hospital and Netherlands Cancer Institute",Healthcare & Life Sciences
NVIDIA GTC 2020,Accelerating Quantum Chemistry Simulations with AI [S21273],"We'll discuss computational chemistry applications of machine learning covering three topics. First, we'll examine the use of neural networks and other machined-learned methods for describing a quantum-accurate potential energy surface. Second, we'll cover graph convolution neural networks and graph message-passing networks for predicting molecular properties at a fraction of the cost of traditional electronic structure calculations. Third, we'll discuss variational autoencoders for molecule discovery and illustrate their application to drug discovery.","Abe Stern, Senior Data Scientist, NVIDIA",Healthcare & Life Sciences
NVIDIA GTC 2020,Acceleration of Wavefront Analysis Using Zernike Polynomial Fitting [P21849],"Zernike polynomial fitting can analyze the wavefront map of biological cells with uniform contents. Aberrations not only indicate changes in the wavefront after passing through cells, but also provide angular position for tomogram reconstruction. However, Zernike polynomial fitting for wavefront analysis is time-consuming. To raise efficiency, we implemented Zernike polynomial fitting on a GPU card. We simulate a sequence of red blood cell phase maps with various rotation angles, and estimate the angles using the coefficients of defocus, vertical, and horizontal tilts from Zernike analysis. The error of angle estimation is under 0.7%, and we achieve video-rate processing at 128×128 pixels. This shows the possibility for implementing wavefront analysis on a GPU card and its benefit on tomogram reconstruction.","Yang-Hsien Lin, Ph.D. Candidate, National Taiwan University",Healthcare & Life Sciences
NVIDIA GTC 2020,Accurate Multiple Sclerosis Lesion Segmentation Using Deep Learning [P21889],"We present results of a comparison of multiple deep neural networks (DNNs) performing image segmentation for multiple sclerosis (MS) brain lesions. MS is an autoimmune disease that leads to demyelinating lesions in the central nervous system. Measuring disease progression via brain magnetic resonance imaging (MRI) is an important part of managing MS. Manual segmentation of MRI brain lesions by radiologists is time-consuming and subject to high user variability. Our poster shows the results of using three different DNN architectures, trained using NVIDIA GPU. You'll learn about using neural network for medical image segmentation, particularly applied to the measurement of MS brain lesions. You'll also learn about different DNN architectures, including Inception-based convolutional networks and U-Net based networks, as well as techniques used to compare automated segmentation results to each other and to human expert segmentation.","Ethan Ocasio, Chief Technology Officer, Girls Computing League",Healthcare & Life Sciences
NVIDIA GTC 2020,"AI Methods to Transfer Natural Language into Actionable Knowledge in Medicine: From Radiology, Pathology Reports to Social Media Posts [S22150]","Learn about the key types of use cases for AI-driven natural language processing (NLP) that will ultimately improve medical and public health practice by mining humongous amounts of free text from clinical notes and social media posts. First, we'll describe methods to leverage narrative reports associated with radiological scans to automatically generate labels for creating large annotated image datasets, and we'll highlight its application to different domains of radiology (CT, MR, US). Second, we'll discuss the challenge of clinical prediction and present innovative longitudinal XAI approaches (AI with explainability) to improve clinician acceptance. Finally, we'll discuss the emerging use of publicly available social media data in medicine and public health. While it is challenging to execute data mining from this resource, we'll present successful NLP pipelines that can convert this noisy language data into valuable and actionable knowledge.","Imon Banerjee, Assistant Professor, Emory University",Healthcare & Life Sciences
NVIDIA GTC 2020,A New Era of Medical Imaging [S22554],"Deep Learning has revolutionized medical imaging research and has become an essential element for every single step of the imaging pipeline. In this session, we will discuss some recent trends and share key learnings from related conferences. We will go through the general imaging pipeline - from signal, to image, to image understanding, to actionable insights - and provide examples how Deep Learning can accelerate, augment and improve various steps of the pipeline. And even enable the previously impossible.","Nicola Rieke, Senior Deep Learning Solution Architect - Healthcare, NVIDIA",Healthcare & Life Sciences
NVIDIA GTC 2020,"Animation, Segmentation, and Statistical Modeling of Biological Cells Using Microscopy Imaging and GPU Compute [s21944]","This presentation describes the use of GPU compute in the Allen Institute for Cell Science. The mission of the Allen Institute for Cell Science is to create dynamic and multi-scale visual models of cell organization, dynamics, and activities that capture experimental observation, theory, and prediction to understand and predict cellular behavior in its normal, regenerative, and pathological contexts. Our first project is to understand how the parts of the cells integrate to determine diverse cellular behaviors as revealed through 3D live-cell imaging, creating a dynamic and animated virtual model of the cell. We will describe three applications where we use GPU-compute solutions for our research; these include (1) animation, (2) segmentation, and (3) statistical modeling of biological cell images.","Theo Knijnenburg, Director Machine Learning and Computational Biology, Allen Institute for Cell Science",Healthcare & Life Sciences
NVIDIA GTC 2020,Building a Medical Imaging AI Ecosystem Using Clara SDKs [s22295],"This product talk will give an overview of Clara Imaging Application Framework and will focus on the latest feature updates for accelerating data annotation, domain-optimized training, Iterative & high performance experimentation for training AI models and AI inference workflows for deployment of multi-domain, multi-AI applications in smart hospitals. We will walk through the details of Clara Application Platform capabilities and our ecosystem partners. Using standardized tools for data management and annotation, model creation, validation, and deployment - Clara Imaging tools along with its ecosystem collaborators lower the barrier to adoption of AI in the medical imaging ecosystem.","Prerna Dogra, Product Manager, Clara Application Framework, NVIDIA",Healthcare & Life Sciences
NVIDIA GTC 2020,Building Blocks for Machine Learning Integration into Clinical Workflow [S22189],"Applications of machine learning in radiology image analysis continue to grow at an increasing pace. For these tools to make an impact in diagnostics, they need to be well integrated into a clinical workflow. We'll review the radiology diagnostic interpretation process and the role of several machine-learning algorithms that support radiologists in this effort. We'll then focus on seven generalizable building blocks that are needed to integrate algorithm results into clinical workflow, with roles ranging from quality control and results presentation to error correction and active learning. We'll discuss current standards and the need for new standards, and highlight our experience in applying these algorithms and building blocks in a large cancer center.","Krishna Juluru, Director, Radiology Informatics, Memorial Sloan Kettering Cancer Center",Healthcare & Life Sciences
NVIDIA GTC 2020,"Building Optimized, Low-Cost, Scalable Health-Care Enterprise Deep Learning Services Platform with NVIDIA TensorRT Inference Server and Kubernetes [s21570]","We'll illustrate how NVIDIA TensorRT Inference Server and Kubernetes helped us resolve several issues and build a scalable, low-cost and high-performance solution. Deep learning inference services are fundamentally different from web services and traditional machine learning services. DL services often require GPUs, and one must understand demand and GPU utilization so that load can be efficiently balanced across available inference resources. DL models may require significant compute and memory resources, and the need for serving multiple models and model versions compounds the complexity of a balanced inference deployment. Often, pre- and post-processing run on CPUs, while model inference runs on GPUs, so it is important to decouple them in order to be able to scale GPU and CPU resources independently. Health-care services also have rigorous security requirements and often require on-premises deployment.","Galina Grunin, Distinguished Engineer, Optum",Healthcare & Life Sciences
NVIDIA GTC 2020,Clara Developer Day: Clara Train SDK Performance Walkthrough and Deep Dive [S22717],"In this session, developers and data scientists will learn about the latest performance feature in Clara Train v3.0. There will be a deep dive session walk-through for how to enable different features on a dummy example as well as real segmentation task.","Ahmed Harouni, Solutions Architect, NVIDIA",Healthcare & Life Sciences
NVIDIA GTC 2020,Clara Developer Day: Federated Learning using Clara Train SDK [S22564],"Federated Learning techniques enable training robust AI models in a de-centralized manner – meaning that the models can learn from diverse data but that data doesn't leave the local site and always stays secure. This is achieved by sharing model-weights or partial model weights from each local client and aggregating these on a server that never accesses the source data. In this session we will deep dive into the federated learning architecture of latest Clara Train SDK. We will cover the core concepts of Federated Learning and the different collaborative learning techniques. Afterward, we will dive deeper into how using the Clara Train SDK enables privacy-preserving Federated Learning. This session will also cover the ease of bringing up Federated Learning clients and establishing communication between various clients and a server for model aggregation.","Nicola Rieke, Senior Deep Learning Solution Architect - Healthcare, NVIDIA",Healthcare & Life Sciences
NVIDIA GTC 2020,Clara Developer Day: Getting Started with Clara Train for High Performance & Iterative Experimentation with AutoML [S22563],"In this session, developers and data scientists will learn how using Clara Train SDK accelerates and standardizes model development for medical imaging. We will cover the SDKs core concepts and capabilities to define a training workflow with the option to “bring your own components”. The session will also include a hands-on deep dive on how optimize hyper-parameter using AutoML.","Ahmed Harouni, Solutions Architect, NVIDIA",Healthcare & Life Sciences
NVIDIA GTC 2020,Clara Developer Day: Scalable and Modular Deployment Powered by Clara Deploy SDK [S22565],"Clara Deploy SDK provides a reference framework for developers, data scientists and engineers to make seamless the process to turn trained AI models into operators. These operators can be stitched together to define an AI deployment pipeline, using reference DICOM adapters and sample pipelines that can interface with a medical imaging environment, like a PACS or VNA. In this session, we will do a walk-through of platform features that enable scalable deployment of multiple AI based pipelines in a hospital IT-like infrastructure. We will also do a hands-on session enabling end-end deployment of a Clara Train model. The users will interact with the SDK to deploy reference pipelines and learn how they can use the modular nature of the overall framework to power deployment of AI models in medical imaging workflows.","Jesse Tetreault, Solutions Architect, NVIDIA",Healthcare & Life Sciences
NVIDIA GTC 2020,Computer Vision in Agriculture: Racing with Pests and Diseases [P22001],"Modern technologies of GPU computing and machine learning allow us to create an autonomous system of greenhouse plant condition control. We combined multiple sensor systems with a photo/video surveillance system, which allows us to create a comprehensive control system helpful for early detection of pests and diseases. We strive to create a stable and reliable solution for AI-based early plant disease detection, disease classification, pest detection, and plant development monitoring. We present our working solutions for vertical farming and a prototype of a more general system.","Ivan Molodtsov, Head of Machine Learning, Fermata",Healthcare & Life Sciences
NVIDIA GTC 2020,Data-Driven Approach of Coronary Vessel Reconstruction Using X-ray Angiography [P21799],"Coronary artery disease is typically diagnosed using X-ray angiography. The percent stenosis, or narrowing of the blood vessel, is estimated via visual inspection of 2D angiography images; however, image artifacts and non-ideal projection angles can lead to miscalculation of disease severity. To address this problem, we propose a data-driven method to reconstruct the 3D geometry of the coronary vessels. A convolutional neural network was trained to segment vessels from angiography images. The segmented binary images were subsequently used as input images for several 3D reconstruction algorithms. The reconstructed 3D geometry can be used as input data for computational fluid dynamics analysis to characterize the hemodynamics of the diseased vessel and thus further improve diagnostic accuracy.","Kritika Iyer, Ph.D. Candidate, University of Michigan",Healthcare & Life Sciences
NVIDIA GTC 2020,Data-Efficient Weakly Supervised Histology Classification Using Contrastive Predictive Coding [P22025],"Neural network classification models can be trained on weakly annotated medical imaging data using multiple instance learning (MIL). However, due to weak supervisory signals, direct application of MIL suffers from overfitting when faced with limited labeled data and the network is unable to learn rich feature representations. To overcome such limitations, we propose a two-stage semi-supervised approach that combines data-efficient self-supervised feature learning via contrastive predictive coding (CPC) and attention-based MIL. Across five random splits, we report state-of-the-art performance with a mean validation accuracy of 95% and an area under the ROC curve of 0.968 on a breast cancer classification dataset. We evaluate the quality of features learned via CPC relative to simple transfer learning and show that strong classification performance using CPC features can be efficiently leveraged under the MIL framework using limited labels and with the feature encoder frozen.","Ming Yang Lu, Research Intern , Computational Pathology, Brigham and Women's Hospital, Harvard Medical School",Healthcare & Life Sciences
NVIDIA GTC 2020,"Deep Learning-Aided Label-Free, Real-Time and Time-Lapse “Cell Visualization” Technology that Enables Live/Dead Cell Discrimination and Counting [P21952]","""Cell visualization"" is a new technology that can predict cell properties, such as cell life and death, from bright-field cell images. It's necessary to build a deep-learning model in which the relationship between a teacher cell image, such as fluorescence labeling indicating cell properties, and the corresponding bright-field cell image is finely learned. Inputting unknown bright-field cell images to the constructed learning model generates a pseudo-fluorescent labeling image showing the characteristics of the cells, enabling ""visualization of cells."" The present technology may replace many cell assays in drug discovery by solving the invasiveness of fluorescence-based cell assays. This technology also realizes real-time monitoring of cell quality. This innovative cell digitization technology is expected to become essential in the fields of drug discovery and regenerative medicine.","Tamio Mizukami, Professor, Nagahama Institute of Bio-Science and Technology",Healthcare & Life Sciences
NVIDIA GTC 2020,Deep Learning in Health Care: from Voicebots to Disease Prediction [S21128],"We will focus on a few use cases where deep learning and NVIDIA GPUs help transform health care operations, automate prior authorization processes to reduce operational costs, suggest the next-best action to patients in order to help them manage their chronic conditions, and control call center volumes while increasing customer NPS scores. United Health Group receives over 30 million provider calls each year, out of which over 7 million result in call transfers. We've used a combination of deep learning models to identify the predictors of future calls and the providers likely to make them, enabling us to proactively reach out to them to prevent the incoming calls. If an incoming call is, however, detected, we use NVIDIA’s Nemo ASR / NLP framework to analyze the purpose of the call in real time and optimally route it to the appropriate agent. We'll also discuss our early successes in applying NVIDIA’s OpenSeq2Seq and Nemo frameworks to create voicebots — systems that users can interact with using voice. The quality of audio (smartphone versus phone line), domain (general versus clinical), speaker accents, and so on matter. We'll include an end-to-end overview of the tooling that we created for incremental data collection and model fine tuning on that data, along with metrics of quality and performance benchmarks. Finally, we'll cover our deep learning work on Next Best Action, where we use patients' history to predict the poly-chronic medical conditions and identify high-risk individuals. We then recommend the preventative next-best action, such as an ER visit or a hospitalization.","Julie Zhu, Distinguished Engineer/Chief Data Scientist, United Health Group, Optum Technology",Healthcare & Life Sciences
NVIDIA GTC 2020,"Deep, Self-Supervised Learning for Patient-Specific Anomaly Detection in Stereoelectroencephalography [S21962]","We'll discuss methods for accurate, real-time detection of anomalous events in medical time-series data, with particular application to stereoelectroencephalographic (SEEG) interpretation and event localization for patients with epilepsy. Previous deep learning-based approaches to detecting EEG anomalies have been plagued by high false-positive rates and inability to detect anomalous events that fall below a statically-set threshold. You'll learn the benefits of a nonparametric approach that utilizes a dynamic thresholding method for event detection, producing significant performance improvements. We'll discuss basic approaches and challenges in anomaly detection in medical time series data, limitations encountered by previous deep-learning approaches to anomaly detection, and how to apply a nonparametric dynamic thresholding procedure to improve performance and mitigate false positive results.","Anthony Costa, Director, Sinai BioDesign, Icahn School of Medicine at Mount Sinai",Healthcare & Life Sciences
NVIDIA GTC 2020,De Novo Protein Design of Epitope-Directed Inhibitors [s21348],"Protein drugs have revolutionized modern cancer therapeutics, as protein-based binders can effectively target molecules that are otherwise undruggable by small molecules. Presently, such binders are based on monoclonal antibodies, which are developed and engineered through lengthy and resource-intensive processes of iterative optimization that are empirically guided. Conversely, de novo protein design can offer a rational means for generating new binders with bespoke scaffolds, guided by physics-based computations. Here, we'll present our work on developing a purpose-built, GPU-accelerated computational pipeline for designing protein-based binders de novo. As a proof of principle, we designed proteins that target a key modulator of cancer metastasis. Our experimental characterization of only a few design candidates resulted in binders with strong binding affinities. Solving the structure of one design showed atomic-level agreement between the design model and the determined structure.","Mohammad ElGamacy, Postdoctoral Research Fellow, Friedrich Miescher Laboratory of the Max Planck Society",Healthcare & Life Sciences
NVIDIA GTC 2020,"Deploying FDA-Cleared, TRT-Powered AI Radiology Products to Improve Quality and Productivity in Clinical Radiology [S22400]","SubtlePET and SubtleMR are FDA-cleared AI software products developed by Subtle Medical that use deep learning to significantly improve the image quality and efficiency for PET-CT and MRI exams. We'll introduce how Subtle develops AI that is generalizable and seamlessly deployed in clinical settings. Join our session to:
Learn how to clinically evaluate and deploy AI software solutions at multiple hospitals and imaging centers, through Subtle’s experience of working with Stanford University, the University of California at San Francisco, Hoag hospital, U-C San Diego, Middlesex, RadNet, and others;
Understand how attention models and improved deep-learning architectures improve model performance;
Learn benefits of training algorithms on NVIDIA DGX Station and DGX-1 systems, and how it provides flexibility and efficiency from prototyping to product;
Learn how integrating with NVIDIA TensorRT can accelerate inference; SubtleMR will share there experience where turnaround time is critical, TRT provided additional speed of up to about 8.8x; and
Learn techniques to show how to show evidence of significant clinical values and financial ROI to customer hospitals and imaging centers.","Enhao Gong, Founder and CEO, Subtle Medical",Healthcare & Life Sciences
NVIDIA GTC 2020,Distributed Deep Learning for Automatic Disease Detection Systems [P22078],"Deep neural networks and other deep-learning approaches are providing diagnostic capabilities on a par with medical specialists. If deployed in the field, these DL applications and devices can help to detect health conditions early and develop preventive approaches. However, deploying such systems requires large compute infrastructure. To address the infrastructure issues, I developed a distributed cost-effective approach using edge devices and a cloud-based central hub for better predictions. To test its effectiveness, I applied it to diagnosing eye conditions using fundus images.","Ananya Gangavarapu, Student, Princeton International School of Mathematics and Science",Healthcare & Life Sciences
NVIDIA GTC 2020,Empowering Virtual Reality and Machine Learning in the Hospital [S21472],"A patient's journey through the hospital system involves long-established steps that, collectively, aim to define and provide the best treatment strategy. Two critical actors in this journey are the radiologist and the surgeon. They communicate via the patient's medical image. We'll discuss how we bring new GPU-based technologies, such as virtual reality and machine learning, to this very traditional place of work. We'll outline the degree to which these technologies have been useful in our medical collaborations, as well as how they can optimize outcomes for the patient.","Mohamed El Beheiry, Lead Developer, Institut Pasteur",Healthcare & Life Sciences
NVIDIA GTC 2020,Federated Learning for Medical Imaging: Collaborative AI without Sharing Patient Data [S21536],"While deep neural networks have shown promising results in various medical applications, they highly depend on the amount and diversity of the training data. In the context of medical imaging, this poses a major challenge because patient data needs to be protected and cannot easily be shared. The training data that is required to train a reliable and robust algorithm may not be available in a single institution due to the low incidence rate of some pathologies and limited numbers of patients. At the same time, it is often not feasible to collect and share patient data in a centralized data lake due to patient privacy concerns and regulations. Federated learning — as a collaborative machine learning paradigm — combined with an advanced privacy-preserving mechanism has the potential of solving this issue: models can be trained across several institutions without explicitly sharing patient data. When implementing and deploying a federated learning system into the real-world medical imaging ecosystem, participants can authenticate and communicate securely, and exchange model weights efficiently, enabling model training to be successful. In this talk, we present an introduction to the core concepts of federated learning and discuss the benefits as well as the unique considerations and challenges of implementing a federated-learning system in the context of health care.","Nicola Rieke, Senior Deep Learning Solution Architect - Healthcare, NVIDIA",Healthcare & Life Sciences
NVIDIA GTC 2020,Fostering a Strong Ecosystem for AI in Medical Imaging [S22631],"In order to fully leverage the possibilities that AI offers to drive higher-value health care, we need to create an effective collaboration between physicians and developers, policymakers and payers, and most importantly the patients we will serve. We'll explore ways in which those collaborations are already happening and highlight gaps and opportunities. The speaker, a practicing radiologist specializing in breast imaging, will discuss the ways in which she sees AI impacting her practice now and in the future.","Geraldine McGinty, Chief Strategy and Contracting Officer, Weill Cornell Medicine",Healthcare & Life Sciences
NVIDIA GTC 2020,Fully Automated Blood Analyzer Driving Early Detection for Leukemia Based on Cytomorphology [P22070],"The chances of leukemia survival depend on a variety of factors, including early detection and response to treatment. Cytomorphology introduces the methodology to characterize blood cell morphology to detect leukemia in the early stage. We developed a fully automated blood analyzer based on cytomorphology to count all types of different white blood cells, red blood cells, and blood platelets. The whole workflow of this system includes sample delivery, screening, segmentation, and classification. Compared with the other leading manufacturer, such as CellVision, the segmentation and classification in our system is driven by deep learning instead of pattern recognition, which lets our results achieve more than 95% accuracy based on the validation of a real dataset counted by doctors from a top hospital. It means that our system can be the first real-world health-care AI product in hematology. We'll introduce more detail about our workflow and AI technology accelerated by NVIDIA advanced GPU.","Jie Wang, AI Computing Specialist, Shanghai Jiao Tong University",Healthcare & Life Sciences
NVIDIA GTC 2020,Generating Microscopic Phase Images with Deep Learning [P22394],"This poster will present a deep-learning approach for generating microscopic phase images from brightfield images, contrasting it with current approaches. We'll highlight examples of novel techniques developed at PerkinElmer using supervised and self-supervised approaches to tackle common problems faced by classical approaches. We'll also discuss the challenges of building and testing these generative models for medical imaging and drug discovery applications.","Abdul Al-Haimi, Software Research Lead, PerkinElmer",Healthcare & Life Sciences
NVIDIA GTC 2020,GPU-Accelerated Animated Volume Rendering of Isogeometric Analysis Results [P22069],"Development of isogeometric analysis (IGA) has enabled tighter integration of engineering design and computational analysis. The core idea of IGA is to use same NURBS (Non-uniform rational B-splines) basis functions for representation of geometry in CAD and the approximation of solution fields in FEA. However, visualizing the results of volumetric IGA is compute-intensive; hence, current methods are not interactive. We developed a modified ray-casting and voxelization method for visualizing volumetric NURBS, which provides better interactive performance. This process has been highly parallelized using the GPU to produce interactive animated results in real time. We present an example of the utility of the approach in visualizing results of cardiac simulations.","Harshil Shah, Graduate Student, Iowa State University",Healthcare & Life Sciences
NVIDIA GTC 2020,GPU-Accelerated Genome Assembly: A Deep Dive into Clara Genomics Analysis SDK [S21968],"We'll present de novo genome assembly for long DNA reads using the Clara Genomics Analysis SDK and dig deep into the SDK implementation. We'll discuss the suitability of GPUs for genomics workloads and present algorithms suitable for massively parallel systems. We'll also review the challenges we faced while implementing the algorithms in CUDA, and present the solutions used in cudaAligner, cudaPoa, and cudamapper.","Andreas Hehn, Developer Technology Engineer, NVIDIA",Healthcare & Life Sciences
NVIDIA GTC 2020,"High Throughput Cryo-Electron Microscopy and Cryo-Electron Tomography Powered by GPU at the University of California, San Francisco [S21696]","As early as in 2009, GPU-based computing was introduced at the University of California, San Francisco (UCSF) to reconstruct electron tomographic volumes. Today, 10 years later, high-resolution cryo-electron microscopy (CryoEM) and cryo-electron tomography (CryoET) powered by state-of-the-art GPU technology are routinely used worldwide in structural biology. We'll present the development of GPU-based applications at UCSF to solve critical challenges in CryoEM and CryoET — namely, beam-induced motion, cryoET alignment, and deep-learning based de-noising of cryoEM low-dose images. You should be familiar with back- and forward-projections, Fourier Transforms, C++, and CUDA programming.","Shawn Zheng, Bioinformatics Specialist, Howard Hughes Medical Institute, University of California San Francisco",Healthcare & Life Sciences
NVIDIA GTC 2020,Holistic AI-Enhanced Workflows in Radiology [S21440],"We'll highlight the advantages of a holistic workflow in radiology, the opportunities and challenges of AI implementation in diagnostic workflows, and their impact on treatment decisions and patient outcomes. AI will have a tremendous impact on the way we think and work in medicine. Diseases are complex pathological processes and more than one algorithm is often needed to answer relevant medical questions. This requires a well-managed orchestration of the reporting workflow. We'll introduce the Smart Reporting platform as an innovative tool for synoptic AI-enhanced reporting, which massively improves interdisciplinary communication. Leveraging on NVIDIA Clara and GPU computing, we'll also present a novel pipeline for prostate cancer imaging, tumor detection, automatic report generation, communication of results, and improved treatment decisions.","Alvaro Sanchez, Principal Software Engineer, Smart Reporting GmbH",Healthcare & Life Sciences
NVIDIA GTC 2020,Improving Classification of Lymph Node Histopathology Patches Using Semi-Supervised Classification-GAN (SSC-GAN) [P21984],"The goal of our Semi-Supervised Classification-GAN (SSC-GAN) on histopathology images is to use generated synthetic images to aid in data augmentation and improving classification accuracy, beyond the regular classifiers such as CNNs, where access to annotated data is limited.","Aditya Mitkari, Machine Learning scientist, Onward Health",Healthcare & Life Sciences
NVIDIA GTC 2020,Improving CNN Performance with Spatial Context [S21388],"Deep learning with convolutional neural networks (CNN) is a powerful technique with wide-ranging applications. It has largely replaced traditional computer vision as the go-to method for solving image-analysis and classification problems. At its essence, however, training a CNN is an enormous global optimization problem which, like all optimizations, can fall victim to local extrema. We'll discuss ways of mitigating this issue using computer vision to add spatial context information to restrict the domain of optimization. These techniques not only speed up the training, but also improve the overall performance of the networks. We'll demonstrate results on real-world classification and segmentation problems.","Daniel Russakoff, Co-Founder and Principal Scientist, Voxeleron",Healthcare & Life Sciences
NVIDIA GTC 2020,Insight-Driven Machine Learning Design with Human Expert Collaborations [S21636],"The fundamental breakthroughs in machine learning, and the rapid advancements of the underlying deep neural network models have enabled the potential use of these systems in specialized, high stakes domains such as healthcare. However, despite this remarkable progress, the design of machine learning systems remains laborious, computationally expensive and opaque, sometimes resulting in catastrophic failures and significantly hindering their ability to work with human experts, who play critical roles in these settings. In this talk, I overview steps towards a more informed, intuitive design of machine learning systems, and methods to facilitate collaboration with human experts. I develop tools that enable the quantitative analysis of the complex hidden layers of deep neural networks, which in turn provides both fundamental insights and informs algorithms for efficiently training these systems. I demonstrate how these trained systems can be adapted to work effectively with human experts, resulting in better outcomes than either entity alone.","Maithra Raghu, Research Scientist, Google Brain",Healthcare & Life Sciences
NVIDIA GTC 2020,Machine Learning Cell Phenotypes with Immuno-Fluorescent Cancer Tissue [P22105],"Multiplexed immuno-fluorescent tissue imaging enables precise spatial assessment of protein expression in medical resection specimens. However, tissue sections are stained with a mixture of antibodies, DNA, and RNA markers. Detecting weak or broken edges due to fluorescent membrane-staining artifacts between touching or overlapping cells is a long-studied problem, and is an active research topic in biomedical image analysis. Sometimes, even humans can't visually detect these kinds of edges. We've built a GPU client-server and developed a hybrid system combining the stochastic random-reaction-seed (RRS) method and deep neural learning U-net to identify cell membranes accurately and automatically. Furthermore, we've designed a high-performance AI pipeline in quantifying spatial distribution of cell phenotypes from tissue images with various complexities.","Alvason Zhenhua Li, Postdoctoral Research Fellow, Fred Hutchinson Cancer Research Center",Healthcare & Life Sciences
NVIDIA GTC 2020,Making Radiology AI Models More Robust: Federated Learning and Other Approaches [S22037],"Learn about the key types of clinical use cases for AI methods in medical imaging and the critical challenges and progress in applying AI in these applications. We describe current challenges to creating robust AI models, such as insufficient quality labeled data and access to data. Next, we'll describe recent AI projects that tackle the challenges, including weak/observational learning and federated learning.","Daniel Rubin, Professor, Stanford University",Healthcare & Life Sciences
NVIDIA GTC 2020,Manipulating the StyleGAN Latent-Space: H&E Image Synthesis for Prostate Cancer Research [P22077],"The open-source contributions of NVIDIA research projects have been accelerating the effectiveness of generative adversarial networks since their popularization in 2014. The StyleGAN (Dec 2018) enables detailed control over the generator’s captured learnings. We explore the generator’s learned assignment of image features in the medical domain. Today, prostate cancer is one of the most common types in men. Diagnosticians examine slices of prostate tissue to analyze cancer growth and determine treatment strategies. Staining techniques provide unique lenses for diagnosis, highlighting stroma patterns, lumen areas, epithelium groupings, and nuclear areas. In our experiment, we input a dataset of H&E stained prostate tissue covering the entire spectrum of the disease. During model training, the network creates a mapping of all image features into a latent space. We aim to discover direction vectors between cancer-relevant morphological features.","Gagan Daroach, Undergraduate Researcher, Milwaukee School of Engineering",Healthcare & Life Sciences
NVIDIA GTC 2020,Medical Volume Raytracing in Virtual Reality [S22030],"Raytracing voxels in medical images is a relatively new technique that lets us see medical images in a new light. It allows for creating realistic-looking light effects, like soft shadows. Exploring medical images in virtual reality benefits because these advances make objects look more real, making it easier for a physician to interpret what they are seeing. We'll discuss the challenges with doing volume raytracing in VR, and will demonstrate a solution in CUDA involving rendering to an eye-tracked foveated/warped space. We'll discuss how we can stream this warped space from a server to a head-mounted display, and how to effectively de-noise the results for VR. We'll also show how to use the extra GPU budget that we created by foveated/warped rendering to improve visuals by including better material choices based on DL-generated label maps and define less stair-stepped implicit surfaces based on tricubic interpolation.","Jeroen Stinstra, Dev Tech Medical Imaging, NVIDIA",Healthcare & Life Sciences
NVIDIA GTC 2020,Multi-Task Learning for Sparse Sensor Body Tracking [P22340],Assistance and rehabilitation technologies built on inertial measurement unit-based human motion trackers often use a full-body sensor set. This is necessitated by the bio-mechanical model inspired by the skeleton structure. Reducing the number of sensors will make such technologies more affordable and easier to use for non-expert and unmonitored uses. We explore solutions and demonstrate that it's viable to reduce the number of sensors required for credible body segment acceleration and joint angle tracking. This is achieved by statistical learning on the reduced sensor set by developing inferences based on neural network and signal processing pipelines.,"Aditya Tewari, Machine Learning Scientist, Xsens Technologies",Healthcare & Life Sciences
NVIDIA GTC 2020,NVIDIA Quadro RTX for Healthcare - Improving Patient Outcomes [S22422],This talk covers how Quadro RTX features like real-time ray tracing and AI can reshape healthcare and the life sciences disciplines that support basic research leading to new treatment options and better patient outcomes,"CARL FLYGARE, Quadro Product Marketing Manager, PNY Technologies",Healthcare & Life Sciences
NVIDIA GTC 2020,PDGAN: An End-to-End Parkinson's Disease Data Analysis and Diagnosis System Using Deep Learning [P21110],"PDGAN is a deep-learning-powered tool capable of detecting early signs of Parkinson's disease — a prevalent disease that affects millions of people worldwide. Using MRIs, PDGAN can classify patients in the earliest stages of Parkinson's Disease with 96% accuracy. PDGAN uses a unique technique to solve the diagnosis problem, using generative adversarial networks to synthetically create MRI scans to augment the training dataset. Our poster describes the technical aspects of the dataset, the pipeline of deep-learning tools used to solve the problem, additional features that PDGAN uses that traditional deep-learning solutions don't utilize, and the benefits that each of them bring. It discusses using NVIDIA tools to solve problems that plague many machine-learning problems, including a slow training process and the lack of adequately large data using novel GAN-powered techniques.","Neeyanth Kopparapu, High School Student, Thomas Jefferson High School for Science and Technology",Healthcare & Life Sciences
NVIDIA GTC 2020,Practical Guidelines for Optimizing and Accurate Sizing of Medical Imaging Workflows [S21997],"Deep dive into the various considerations that could be critical to substantially improve existing medical imaging workflows. We'll motivate our discussions through a detailed analysis of two common medical imaging workflows: 2D classification and 3D segmentation. Our results suggest that controlling for aspects such as i/o format, selection of hyper-parameters, and mixed-precision training could all be key to maximizing hardware performance and reducing turnaround time for experiments. Furthermore, we aim to discuss other strategies, such as learning-rate warmup and scaling, effect of optimizers, scaling to multiple GPUs, and their potential effects on training throughput. We were able to show a 5x improvement for the 2D classification task through our ablation experiments. We'll use these results suggest best practices learned, and also build a sizing calculator for providing quantitative insights for hardware investments.","Anas Abidin, Solutions Architect, NVIDIA",Healthcare & Life Sciences
NVIDIA GTC 2020,Reconstruction of Under-Sampled Cartesian Data Using Deep Learning [P21927],"Magnetic resonance imaging (MRI) is a non-ionizing medical imaging modality that provides excellent contrast information about human tissues. One of the limitations of MRI is its long data-acquisition time. One way to reduce the MRI scan time is to collect less data that introduces aliasing in the resulting MR image. Dedicated MRI reconstruction algorithms can be used to remove the aliasing effects. Under-sampled Cartesian data, in conjunction with an advanced reconstruction algorithm, has been used in literature to get the aliasing-free image. We propose a convolution neural network (CNN)-based solution to reconstruct the MR image from under-sampled MRI data, thus helping to reduce the scan time. We use artifact power and signal-to-noise ratio to quantify the reconstruction quality. We compare the results to the state-of-the-art SENSE reconstruction algorithm.","Taquwa Aslam, M.S. Student, COMSATS University Islamabad, Islamabad Pakistan",Healthcare & Life Sciences
NVIDIA GTC 2020,Rethinking Impact Factor: an NLP-Driven Metric and Pipeline Using Generalized Autoregressive Pretraining on Medical Journals for Granular Knowledge [S21552],"Dramatic advances in NLP have reinvented performance on public leaderboards, such as the Stanford Question Answering Dataset and decathlon superGLUE. Nominally, approaches follow transformers-based architectures in a pretrain-finetune paradigm, with the bulk of compute in the pretrain phase. Where previous studies have elucidated finetune paradigms for recurrent neural network architectures, ours examines a multi-phase NLP paradigm for realizing expert-level domain-specific performance, specifically for the clinical task of unplanned 30-day hospital readmission. With exhaustive GPU studies and Bayesian optimization, in part with the NVIDIA Clara Train platform, we'll show that systems are only as good as what they read. From 20 top medical practice journals over the past 90 years, we determined a novel AI-impact factor for the clinical task that guides to state-of-the-art AUC of 0.74. We'll review best practices on training modern transformer-based architectures for medicine.","Leo Tam, Senior Applied Research Scientist, NVIDIA",Healthcare & Life Sciences
NVIDIA GTC 2020,Segmentation-Guided Pelvis Fracture Detection in Pelvis X-rays [P21979],"Pelvis fractures represent approximately 3% percent of all skeletal injuries and can be observed in any group of patients. Like much trauma, there is a bimodal distribution, with younger male patients involved in high-energy trauma and older female patients in minor trauma. In this work, we utilize GPU computing to process high-resolution pelvis X-ray images and segment into pelvis anatomical regions. Then, we train a binary classifier on segmented anatomical region to detect fracture.","jalaj jain, CTO, imera.ai",Healthcare & Life Sciences
NVIDIA GTC 2020,Segmentation of Organs at Risk in Chest Cavity Using 3D Deep Neural Network [P21943],"Our work considers the problem of segmentation of organs-at-risk in planning radiotherapy of lung and esophageal cancer. To solve this problem, we developed a system based on a 3D implementation of the U-Net neural network architecture with ResNet-50 encoder. One of the main problems of segmentation of 3D images is the limitation of computing power. We propose a multi-dimensional approach that effectively uses the available infrastructure for processing large image data. Overfitting due to the small size of the dataset is another important problem. We solve this using input mixup for regularization of neural networks. To train the deep-learning models, the NVIDIA TITAN X Pascal and NVIDIA Tesla V100 were compared, with and without Apex Optimization. The results demonstrate that the best performance was obtained using NVIDIA Tesla V100 16Gb with Apex Optimization.","Maksym Manko, Research Engineer, Ciklum",Healthcare & Life Sciences
NVIDIA GTC 2020,*Top 5 Poster Nominee - MolecularRNN: Generating Realistic Molecular Graphs with Optimized Properties [P22377],"We'll introduce MolecularRNN, the graph-generative model for molecular structures. Designing new molecules with a set of predefined properties is a core problem in drug discovery. There is a growing need for de-novo design methods. Our model generates diverse realistic graphs after likelihood pretraining on a big database of molecules. We perform an analysis of pretrained models on large-scale generated datasets of 1 million samples, then tune the model with a policy-gradient algorithm provided with a critic that estimates the reward for the property of interest. We see a significant distribution shift to the desired range for lipophilicity, drug-likeness, and melting point, outperforming state-of-the-art works. With the use of rejection sampling based on valency constraints, our model yields 100% validity. We'll show how invalid molecules provide a rich signal to the model through the use of structure penalties in our reinforcement learning pipeline.","Mariya Popova, Ph.D. Student, Carnegie Mellon University",Healthcare & Life Sciences
NVIDIA GTC 2020,3D Deep Learning in Function Space [s21764],"Recent advances in GPU technology and scalable algorithms have led to breakthroughs in deep learning. In particular, convolutional neural networks (CNNs) achieve state-of-the-art results in longstanding vision problems, such as image classification or object detection. However, autonomous agents that navigate and interact in our world need to reason in 3D. Unlike images in the 2D case, it is not clear how to represent 3D geometry and how to make it amenable for deep-learning techniques. We'll introduce our approach to learning 3D representations in function space. First, we'll show how this approach can represent arbitrary topologies without discretization at fixed memory cost. Then we'll extend this framework to learning to predict not only the shape of an object, but also its texture and motion. Finally, we'll show how we can scale our method to real-world scenarios using state-of-the art NVIDIA GPU technology.","Michael Niemeyer, Ph.D. Student, MPI-IS and University of Tübingen",Higher Education / Research
NVIDIA GTC 2020,Accelerated Computing Teaching Kit for University Educators: Introduction and Use Cases [S22414],"As performance and functionality requirements for computing applications rise, industry demand for new graduates familiar with accelerated computing with GPUs grows. This session introduces the newest version of the Accelerated Computing Teaching Kit: a comprehensive set of academic labs, university teaching material, and e-book for use in introductory and advanced parallel programming courses. The teaching materials start with the basics and focus on programming GPUs, and include advanced topics such as optimization, advanced architectural enhancements, and integration of a variety of programming languages. We'll present a successful course-adoption case at Iowa State University — one of many worldwide — along with student feedback. The course at Iowa State covers an introduction to parallel computing using GPUs and its application to solid modeling and CAD. As part of the course outcomes, students were able to demonstrate the use of GPU-accelerated computing in their research by working on an individual course project that uses some of the techniques taught in the class. Finally, we'll discuss brand-new teaching kit modules covering CUDA 11, multi-GPU, and the latest libraries.","Joseph Bungo, DLI Program Manager, NVIDIA",Higher Education / Research
NVIDIA GTC 2020,Accelerated Data Science in the Classroom: Teaching Analytics and Machine Learning with RAPIDS [S22417],"The demand for accelerated data-science skill sets among new graduate students grows rapidly as the computational demands for data analytics applications soar. This session introduces a novel yet reproducible approach to teaching data-science topics in a graduate data science course at the Georgia Institute of Technology, taught by Professor Polo Chau. Haekyu Park, a computer science Ph.D. student and teaching assistant of the course, will co-present key pedagogical considerations and solutions that help students learn GPU-accelerated data science and analytics using the open-source RAPIDS framework. For example, we present a hybrid, flexible approach for students to learn, where they can choose to experiment with RAPIDS using a local NVIDIA DGX-1 system, or the cloud, or both.","Polo Chau, Associate Professor, The Georgia Institute of Technology",Higher Education / Research
NVIDIA GTC 2020,Accelerated Light-Transport Simulation using Neural Networks [S21852],"Neural network-based techniques have taken many fields by storm, but until recently have seen relatively little use in the field of physically-based rendering. This has begun to change. We'll present techniques for accelerating Monte Carlo integration of light transport without introducing bias by utilizing functions learned by neural networks for variance reduction. Our techniques yield on-par or higher performance than competing machine learning-based techniques at equal sample counts and generalize beyond physically-based rendering, being applicable to other high-dimensional integration problems such as Bayesian inference and reinforcement learning.","Thomas Mueller, Senior Research Scientist, NVIDIA",Higher Education / Research
NVIDIA GTC 2020,Accelerating Large Seismic Simulation Code with PACC Framework [P21901],"The pipelined accelerator (PACC) helps lower the hurdle for implementing out-of-core stencil computation, such as large seismic simulations. However, the out-of-core applications themselves are facing data-movement bottlenecks because improvement of accelerators dwarfs that of interconnects. In this poster, we introduce temporal blocking techniques to reuse on-chip data, and propose a data-mapping scheme to eliminate data movement on the host side. The data-mapping scheme accelerates the program by 2.5x compared to previous work and by 35x compared to an OpenMP-based program. However, performance is still bound by data movement between the host and device, so we need to come up with further data-centric strategies. Moreover, the degradation in execution time of PACC code is about 25% compared to an in-core OpenACC code, which should be reduced if we can design further data-centric strategies.","Jingcheng Shen, Ph.D. Student, Osaka University",Higher Education / Research
NVIDIA GTC 2020,ALFRED: Action Learning From Realistic Environments and Directives [P22310],"ALFRED is a benchmark for learning a mapping from natural language instructions and egocentric vision to sequences of actions for household tasks. Long composition rollouts with non-reversible state changes are among the phenomena we include to shrink the gap between research benchmarks and real-world applications. ALFRED consists of expert demonstrations in interactive visual environments for 25,000 natural language directives. These directives contain both high-level goals like ""Rinse off a mug and place it in the coffee maker"" and low-level language instructions like ""Walk to the coffee maker on the right."" ALFRED tasks are more complex in terms of sequence length, action space, and language than existing vision-and-language task datasets. We show that a baseline model designed for recent embodied vision-and-language tasks performs poorly on ALFRED, suggesting that there is significant room for developing innovative grounded visual language understanding models with this benchmark.","Daniel Gordon, Ph.D. Student, University of Washington",Higher Education / Research
NVIDIA GTC 2020,Analytic Spherical Harmonic Gradients for Real-Time Rendering with Many Polygonal Area Lights [P22297],"We develop a novel analytic formula for the spatial gradients of the spherical harmonic coefficients for uniform polygonal area lights. The result is a significant generalization, involving the Reynolds transport theorem to reduce the problem to a boundary integral, for which we derive a new analytic formula, showing how to reduce a key term to an earlier recurrence for SH coefficients. The implementation requires only minor additions to existing code for SH coefficients. The results also hold implications for recent efforts on differentiable rendering. We show that SH gradients enable very sparse spatial sampling, followed by accurate Hermite interpolation. This enables scaling PRT to hundreds of area lights with minimal overhead and real-time frame rates. Moreover, the SH gradient formula is a new mathematical result that potentially enables many other graphics applications.","Lifan Wu, Ph.D. Student, University of California, San Diego",Higher Education / Research
NVIDIA GTC 2020,An Improved Immersed Boundary-Lattice Boltzmann Method for Incompressible Fluid-Flow Simulations on GPU [P21931],"We introduce the immersed boundary and the lattice Boltzmann method for fluid-solid interaction. The advantage of the lattice Boltzmann method is that it can be easily parallelized. We use a computational study to demonstrate how the immersed body discretization affects the numerical results. We introduce a modification that improves stability, and compare the performance of both methods using several benchmarks problems. These problems show how the immersed body discretization affects the numerical results, mainly the overall drag force and the permeability of the discretized body boundary. Finally, we discuss a performance analysis of different solvers for linear systems.","Pavel Eichler, Ph.D. Student, Faculty of Nuclear Sciences and Physical Engineering, Czech Technical University, Prague",Higher Education / Research
NVIDIA GTC 2020,A Reinforcement Learning Approach for Sequential Spatial Transformer Networks [s21941],"In this session, we discuss the applications of reinforcement learning (RL) for computer vision applications such as classification. We describe how we combine the idea of spatial transformer networks with RL to improve the classifiers' robustness against noise and clutter.","Fatemeh Azimi, Ph.D. Researcher, DFKI",Higher Education / Research
NVIDIA GTC 2020,Astaroth: an API for Three-dimensional Stencil Computations on GPUs [S21332],"We'll present the Astaroth library that uses multiple GPUs efficiently in 3D stencil computations. Astaroth provides a C99 API, a domain-specific language, and a compiler for translating programs written in that domain-specific language into efficient CUDA kernels. We'll show that it is possible to achieve near hand-tuned performance while still being able to describe problems using a high-level language. Astaroth meets the demands in computational sciences, where large stencils are often used to attain sufficient accuracy. Maximizing the utilization of caches is challenging, especially in computational physics, where multiple simulated fields interact with each other and should be held in caches simultaneously. Astaroth takes inspiration from graphics and image-processing pipelines and generates a pipeline optimized for processing large 3D stencils.","Johannes Pekkila, Research Assistant, Aalto University School of Science",Higher Education / Research
NVIDIA GTC 2020,Benanza: Automatic μBenchmark Generation to Compute ”Lower-bound” Latency and Inform Optimizations of Deep Learning Models on GPUs [P21858],"Current profiling tools lack the highly desired abilities to characterize ideal performance, identify sources of inefficiency, and quantify the benefits of potential optimizations. Such deficiencies have led to slow characterization/optimization cycles that can't keep up with the fast pace at which new DL models are introduced. We propose Benanza, a sustainable and extensible benchmarking and analysis design that speeds up the characterization/optimization cycle of DL models on GPUs.","Abdul Dakkak, Ph.D. Student, University of Illinois Urbana-Champaign",Higher Education / Research
NVIDIA GTC 2020,Bringing AI to the Classroom: NVIDIA's Deep Learning Teaching Kit [S22357],"The call for AI and deep-learning skills is soaring, and university classrooms are on the front lines of feeding the demand. NVIDIA Teaching Kits lower the barrier of incorporating AI and GPU computing in coursework. NVIDIA’s higher-education leadership and Pawel Morkisz, assistant professor of mathematics at AGH University of Science and Technology in Krakow, will discuss the Deep Learning Teaching Kit, co-developed with Professor Yann LeCun and his team at New York University. The kit was a starting point for preparing materials for the course dedicated for postgraduate students of mathematics. Using NVIDIA’s materials saved many days of work preparing lecture slides and source-level coding projects/solutions.","Joseph Bungo, DLI Program Manager, NVIDIA",Higher Education / Research
NVIDIA GTC 2020,Building MSOE’s GPU-Powered Infrastructure for AI Instruction and Research [s21423],"Last September, Milwaukee School of Engineering, with NVIDIA, deployed a new hybrid cluster to serve multiple AI and HPC computing demands including instruction, student and faculty research, and industry collaborations. We'll describe the optimized accelerated computing architecture of this cluster and the software stack enabling these uses. We'll explain the lessons learned through the design, build, and deploy processes. We'll provide a blueprint for other institutions that seek to build GPU-accelerated computing infrastructure that supports similar diverse requirements.","Bradley Palmer, Senior Solutions Architect, NVIDIA",Higher Education / Research
NVIDIA GTC 2020,CapsNet-Lite: A Lightweight and High Performance CapsNet Architecture [P21854],"The capsule network (CapsNet) includes a new type of layer composed by ""capsules"". A capsule is a vector that stores in each position the amount of a certain object feature that's present in the image. Then, those capsules are combined to output the label of the object. To learn how to combine the capsules, this network also needs a routing algorithm. This type of network achieves very high accuracy in classification problems. We propose a modified CapsNet architecture called CapsNet-Lite that outperforms the original proposal in accuracy, with a much faster training procedure.","Francisco José García, Ph.D. Student, Universidad Rey Juan Carlos",Higher Education / Research
NVIDIA GTC 2020,Capsule Networks for 3D Pose Estimation in Computer Graphics [P21924],"Pose estimation is an important task for novel engineering applications, such as virtual and augmented reality (VR/AR), pose-based video games, object reconstruction, target tracking, driving assistance, and recent sports analytics. Commonly, an efficient pose estimation system depends on the pose visualization given by a 3D configuration of location, orientation, and scaling parameters of the target. In this work, we implement Capsule Networks to solve 3D pose estimation in computer graphics of rigid objects using a multi-GPU architecture.","Kenia Picos, Professor, CETYS Universidad",Higher Education / Research
NVIDIA GTC 2020,CheetahDB: A System for High-Throughput Database Processing on GPUs [P22073],"GPU database is an active topic in academic research and industrial practice. However, existing systems have not shown significant performance advantages over CPU-based in-memory database management systems (DBMS). Two main factors contributed to such difficulties: First, the CUDA programming model, by focusing on HPC-type workloads, requires non-trivial basic research to address the many technical challenges in developing a DBMS system software; and second, I/O bottleneck between host and GPU offsets the performance gain of onboard query processing. CheetahDB is a high-performance in-memory DBMS generated from NSF-supported research at the database group in the University of South Florida and commercialized by Cheetah Data Systems, Inc. CheetahDB addresses the above challenges via a complete rethinking of the software architecture of a DBMS under today’s multi-core hardware environment.","Chengcheng Mou, Student, University of South Florida",Higher Education / Research
NVIDIA GTC 2020,Data Mining Pipeline for Predictive Synthesis of Advanced Materials [P22007],"Materials discovery is significantly facilitated and accelerated by high-throughput ab-initio computations. Being able to rapidly design advanced compounds has displaced the materials-innovation bottleneck to developing synthesis routes for the desired material. As there isn't a fundamental theory for materials synthesis, one might attempt a data-driven approach for predicting inorganic materials synthesis, but this is impeded by the lack of a comprehensive database of synthesis parameters. We've generated a dataset of “codified recipes” for solid-state synthesis automatically extracted from scientific publications. The dataset consists of about 20,000 synthesis entries retrieved from over 50,000 solid-state synthesis paragraphs by using text mining and natural language processing approaches. The dataset is publicly available and can be used for data mining of various aspects of inorganic materials synthesis.","Olga Kononova, Postdoctoral Scholar, University of California, Berkeley",Higher Education / Research
NVIDIA GTC 2020,Decoding Texture Information from Rat Somatosensory Cortical Neurons Using CNN [P21855],"Neurons in the neocortex often exhibit feature selectivity against external stimuli. Simple stimuli can be decoded from activity of a single neuron; however, it has been poorly understood whether more complicated features can be decoded from the neuronal activity. To address this question, we recorded local field potentials (LFPs), which reflect the activity of thousands of neurons, of rats given complex sensory stimuli and took advantage of a convolutional neural network (CNN) for decoding. To this end, we developed a novel electrode array that enables wide-range recording from the superficial layers of the cortex. We recorded LFPs from the primary somatosensory cortex (S1) of a rat exploring on either rough or smooth floor. Our CNN model yielded 78% accuracy on average for decoding the surface texture. This suggests that somatosensory LFPs contain enough information to decode a surface texture.","Kotaro Yamashiro, Ph.D. Student, The University Tokyo, Graduate School of Pharmaceutical Sciences",Higher Education / Research
NVIDIA GTC 2020,Deep Learning-Based Subcellular Phenotyping of Cell Edge Dynamics Reveals Fine-Grained Drug Responses [S21242],"We'll highlight how unsupervised deep learning can reveal subcellular drug responses hidden in the heterogeneity in live cell images. We'll present a feature-learning method for time-series data, which combines long short-term memory autoencoder and the prior information from traditional machine-learning analysis. We'll discuss how feature learning can be used to identify drug-related rare phenotypes and accelerate drug discovery. You need to have basic knowledge about autoencoder, clustering, microscopy, and cell biology.","Kwonmoo Lee, Assistant Professor, Worcester Polytechnic Institute",Higher Education / Research
NVIDIA GTC 2020,Dive into Deep Learning [T22537],"Nowadays, deep learning is transforming the world. However, realizing deep learning presents unique challenges, because any single application brings together various disciplines. To fulfill the strong wishes of simpler but more practical deep learning materials, Dive into Deep Learning (https://d2l.ai/), a unified resource of deep learning, was born to achieve these goals: • offer depth theory and runnable code, showing readers how to solve problems in practice; • be complemented by a forum for interactive discussions of technical details and to answer questions; and • be freely available for everyone. We're going to provide an overview of the in-depth convolutional neural networks (CNN) theory and handy Python code. More importantly, you'll be able to train a simple CNN model on our pre-setup cloud-computing instances for free. Here are the detailed schedule and materials: https://github.com/goldmermaid/gtc2020","Rachel Hu, Applied Scientist, Amazon Web Services",Higher Education / Research
NVIDIA GTC 2020,Efficient Simulations of Patient-Specific Electrical Heart Activity on the DGX-2 [P22031],"Patients who have suffered a heart attack have an elevated risk of developing arrhythmia. The use of computer simulations of the electrical activity in the hearts of these patients is emerging as an alternative to traditional, more invasive examinations performed by doctors today, and could provide not only safer but also more accurate results. One of the principal barriers to clinical use of such simulations is the tremendous amount of computational power they require. In this poster, we demonstrate a highly efficient code capable of running electrical heart activity simulations at 1/30 of real-time on the NVIDIA DGX-2, and we show that the achieved performance is close to optimal. Topics discussed include optimisations for sparse matrix-vector multiplications, strategies for handling inter-device communication for unstructured meshes, and lessons we learnt while programming the DGX-2.","Kristian Gregorius Hustad, Ph.D. Student, Simula Research Laboratory",Higher Education / Research
NVIDIA GTC 2020,Exploring the Impact of Functional Package Manager Over a Non-Traditional High Performance Computing System [P21980],"Nowadays, academia and industry are using high performance computing in a search for better ways to improve the performance of the systems, and to make constructing and installing the programs less complex. Our project focuses on two problems: First, we analyze embedded systems capable of HPC as a way to improve energy consumption, and Nix as a tool to manage programs more easily. Then, we analyze whether using these systems is valid in an HPC context in terms of energy efficiency and performance.","Carlos Gómez, Master's Student in Computer Science, Universidad Industrial de Santander",Higher Education / Research
NVIDIA GTC 2020,Few-Shot Adaptive Video-to-Video Synthesis [S21142],"Video-to-video synthesis (vid2vid) aims to convert an input semantic video, such as human poses or segmentation masks, to an output photorealistic video. However, existing approaches have limited generalization capability. For example, to generalize a trained human synthesis model to a new subject previously unseen in the training set requires collecting a dataset of the new subject, as well as retraining a new model. To address these limitations, we propose an adaptive vid2vid framework to synthesize previously unseen subjects or scenes by leveraging few example images of the target at test time. Our model achieves this few-shot generalization capability via a novel network weight-generation module utilizing an attention mechanism. We conduct extensive experimental validations with comparisons to strong baselines on different datasets.","Ting-Chun Wang, Senior Research Scientist, NVIDIA",Higher Education / Research
NVIDIA GTC 2020,Fireiron: A Scheduling Language for High-Performance Linear Algebra on GPUs [P22298],"Our poster introduces Fireiron, a DSL and compiler that allows the specification of high-performance GPU implementations as compositions of simple and reusable building blocks. We show how to use Fireiron to optimize matrix multiplication implementations, achieving performance matching hand-coded CUDA kernels, even when using specialized hardware such as NIVIDA tensor cores, and outperforming state-of-the-art implementations provided by cuBLAS by more than 2x.","Bastian Hagedorn, Ph.D. Student, University of Münster",Higher Education / Research
NVIDIA GTC 2020,GPU-Accelerated Next-Generation Sequencing Bioinformatic Pipeline [S21163],"Gold-standard next-generation sequencing (NGS) bioinformatic pipelines (BWA, PICARD, GATK, Samtools and variant annotators) utilize conventional CPUs. However, running pipelines for thousands of whole exome or whole genome sequencing (WES and WGS) samples requires several months, even on high performance computing CPU clusters. Graphical processing units have not been efficiently employed in pipeline acceleration, but have reduced computational runtime in biomedicine. Thus, we created a Kubernetes pipeline where we adapted and GPU-accelerated the code of key NGS software. We also tested the applicability and performance of GPU frameworks, and show through multiple experiments the extent of pipeline acceleration with varying combinations of multithreaded CPUs and GPUs. We show GPU’s effectiveness for bioinformatic analyses and how the mixed use of CPU and GPU will become the gold standard.","Margaret Linan, Computational Research Scientist, Icahn School of Medicine at Mount Sinai",Higher Education / Research
NVIDIA GTC 2020,Heterocomputing and Transprecision Computing in Unstructured Low-Order Finite-Element Analyses on Volta GPUs [P21857],"We show the effect of heterocomputing and transprecision computing on performance using Volta GPUs. Here, we target low-order finite-element analysis. It's a core application in manufacturing, and attaining good performance on GPUs is challenging due to such bottlenecks as memory-bound computations and random memory accesses. We describe detailed algorithms to relieve these bottlenecks by utilizing heterogeneous computing and transprecision computing. As an application example, we conduct an earthquake city simulation.","Takuma Yamaguchi, Ph.D. Student, University of Tokyo",Higher Education / Research
NVIDIA GTC 2020,High-Performance Deep-Learning Operators on NVIDIA GPUs via Multi-Dimensional Homomorphisms [P21790],"We present a holistic approach to CUDA code generation that provides performance, portability, and productivity for deep-learning operators (such as Matrix Multiplication and Convolutions) on NVIDIA GPUs. Our approach is based on our algebraic formalism of Multi-Dimensional Homomorphisms (MDHs). We show that important deep-learning operators can be conveniently expressed as MDHs, and that we can automatically generate CUDA code for MDHs that is specifically optimized for a particular GPU and input size. Our experimental results on real-world, deep-learning input data demonstrate that our automatically generated and optimized CUDA code achieves better performance than well-performing competitors — up to 2.91x better than NVIDIA’s hand-optimized libraries cuBLAS and cuBLASLt for deep-learning operator GEMM, and up to 3x better than NVIDIA’s cuDNN library and the popular deep-learning framework TVM for operator Convolution.","Ari Rasch, Ph.D. Student, University of Münster",Higher Education / Research
NVIDIA GTC 2020,High Performance Distributed Deep Learning: A Beginner's Guide [S21546],"Learn the current wave of advances in AI and HPC technologies to improve the performance of deep neural network training on NVIDIA GPUs. We'll discuss many exciting challenges and opportunities for HPC and AI researchers. Several modern DL frameworks (Caffe, TensorFlow, CNTK, PyTorch, and others) that offer ease-of-use and flexibility to describe, train, and deploy various types of DNN architectures have emerged. We'll provide an overview of interesting trends in DL frameworks from an architectural/performance standpoint. Most DL frameworks have utilized a single GPU to accelerate the performance of DNN training/inference. However, approaches to parallelize training are being actively explored. We'll highlight new challenges for message-passing interface runtimes to efficiently support DNN training, and how efficient communication primitives in MVAPICH2-GDR can support scalable DNN training. Finally, we'll discuss how we scale training of ResNet-50 using TensorFlow to 1,536 GPUs for MVAPICH2-GDR.","Ammar Ahmad Awan, Graduate Research Assistant, Ohio State University",Higher Education / Research
NVIDIA GTC 2020,Hybrid Molecular Mechanics: Artificial Intelligence Simulation Methods to Study Molecular Systems [S22090],"The advent of modern GPU architectures, and the development of molecular dynamics (MD) engines to exploit them efficiently, substantially impacted the performance and timescale achieved by MD. We'll present recent enhancements of MD capabilities by coupling to artificial intelligence methods. We'll present the interfaces enabling the coupling of the MD engine NAMD with the deep learning-based molecular descriptor Accurate NeurAl networK engINe (ANI). By using ANI to describe unparameterized molecules in the system — a drug bound to an enzyme, for example — instead of using a more expensive method such as quantum mechanical calculations, one can achieve performances of (nearly) classical MD simulations while maintaining quantum mechanical levels of accuracy. Moreover, the new NAMD platform lets you use AI-based enhanced sampling methods, such as reinforcement learning-based adaptive sampling, to achieve timescales prohibited by pure MD simulations.","David Hardy, Senior Research Programmer, University of Illinois at Urbana-Champaign",Higher Education / Research
NVIDIA GTC 2020,Implementation of Artificial Intelligence/Deep Learning Disruption Predictor into a Plasma Control System [P22517],"As described in NATURE (April 2019), Princeton's AI/deep-Learning software uses convolutional and recurrent neural network components to integrate complex information from both spatial and temporal big data to predict dangerous disruptive events in magnetic fusion plasmas with unprecedented accuracy and speed on top supercomputers featuring NVIDIA'S powerful Volta GPUs. Here, we report on improved capability of the software to output not only the “disruption score,” as an indicator of the probable onset of a disruption, but also a “sensitivity score” in real time to indicate the underlying reasons for the imminent disruption. As an indicator of possible causes for future disruptions, the “sensitivity score” can provide valuable physics-based interpretability for the deep-learning model results, and more importantly, provide targeted guidance for the control actuators when implemented into any modern plasma control system. This is a significant step in moving from modern deep-learning disruption prediction to real-time control—a major advance that brings novel AI-enabled capabilities needed for the future burning plasma ITER system.","Ge Dong, Research Associate, Princeton Plasma Physics Laboratory",Higher Education / Research
NVIDIA GTC 2020,Inference Path Optimization for Deep Reinforcement Learning on NVIDIA DGX-2 [P22339],"Our institution, the Shanghai Jiao Tong University High Performance Computing Center, is a school-level computing platform, supplying teachers and students in the whole school with computing power. In 2019, we bought eight DGX-2 for our new machine to support the school's research requirements. This poster presents one optimization case we did for our user to unleash the full power of the marvel machine. NVIDIA DGX is the most powerful computation system for artificial intelligence. However, there are still many challenges for a deep reinforcement learning system to take full advantage of the computing power of the DGX-2 platform, because of the limited PCI-e bandwidth, imbalance computation ability between CPU and GPU, and under-utilization of the GPU. We use Volta Multi-Process Service to increase GPU utilization. Introducing NUMA aware and workload balance further improves the computation efficiency. To alleviate the bottleneck of PCI-e bandwidth, we propose a residual compression method for states.","Jie Wang, AI Computing Specialist, Shanghai Jiao Tong University",Higher Education / Research
NVIDIA GTC 2020,MAGMA: Accelerating Linear Algebra Through Mixed-Precision and Tensor Cores [S21557],"The MAGMA library provides several GPU-accelerated linear algebra algorithms. We'll cover the mixed-precision (MP) algorithms for solving different linear algebra problems, such as linear systems of equations (Ax=b). Classic MP algorithms use two precisions to accelerate the solution of systems in double or double-complex precisions. Thanks to the introduction of half-precision in NVIDIA GPUs and the incredible performance of tensor cores, dual-precision MP algorithms can now accelerate systems in single precision as well as single-complex precision. Triple-precision MP algorithms can now solve systems in double and double-complex precisions. We'll show how to accelerate complex precisions using “half-complex” linear algebra kernels, which are not natively supported by the tensor core units.","Ahmad Abdelfattah, Research Scientist, University of Tennessee",Higher Education / Research
NVIDIA GTC 2020,MemEAPF for Mobile Robot Path Planning on Jetson Platform [P21986],"We present a parallel implementation on the NVIDIA Jetson TX2 of the membrane evolutionary artificial potential field (memEAPF) algorithm for mobile robot path planning. The memEAPF algorithm is employed to achieve feasible paths, considering minimum path length, safety, and smoothness. The memEAPF algorithm combines membrane computing with a genetic algorithm — specifically, a membrane-inspired evolutionary algorithm with a one-level membrane structure and the artificial potential field method to find the parameters to solve efficiently the robot path-planning problem. The path-planning problem in mobile robots is one of the most computationally intensive tasks, so heterogeneous computing helps to gain performance. Employing GPUs processes data-intensive tasks efficiently — in this work, the evaluation of solution paths.","Kenia Picos, Professor, CETYS Universidad",Higher Education / Research
NVIDIA GTC 2020,Mobile Manipulation Toward Cleaning Dining Tables [P22364],"Service robots are robots designed to work in human environments. Human environments naturally imply unstructured environments, and hence require advanced computer-vision algorithms to reliably operate. Historically, various attempts have been made at developing general-purpose robots capable of performing human tasks in cluttered environments; however, most fall short due to problems in perception and task/motion planning. With the rise of modern RGB-D sensors, deep-learning algorithms, and GPU-based hardware for deep learning, the circumstances are ripe for a new foray into developing a general-purpose robot with artificial general intelligence. Leveraging on these technologies, we’ll develop a service robot for use in a fast-food center to clear and clean tables. We'll present our experiences and findings during its development.","Ka-Shing Chung, Ph.D. Student, National University of Singapore",Higher Education / Research
NVIDIA GTC 2020,Multiphysics Software Development in the Age of AI [P21687],"We'll explain the main ideas of physics-informed neural networks (PINNs) in the context of an industrial multi-physics problem—namely, designing a heat sink for the next generation of NVIDIA DGX systems. PINNs leverage the underlying laws of physics, often described in the form of partial differential equations (PDEs), to solve forward, inverse/data-assimilation, and model discovery problems. Our work addresses several major drawbacks encountered with the traditional methods of solving PDEs in terms of usability (not requiring arduous meshing), speed (ability to solve multiple geometries simultaneously), scalability (being embarrassingly parallelizable across clusters of GPUs), and expertise (leveraging past experience). PINNs are also agnostic to the physics type (whether fluids, thermal, or solid) and are capable of composing and linking multiple physics representations directly from a description of the physical phenomena (that is, equations, initial/boundary conditions/data, and geometry).","Christopher Lamb, Vice President, Compute Software, NVIDIA",Higher Education / Research
NVIDIA GTC 2020,N-Body Simulation of Binary Star Mass Transfer [P21883],"Observations suggest that over 50% of the stars in our galaxy are part of a multiple-star system. Of these, binary systems are the most intensely studied. Their abundance and unique characteristics make binary systems invaluable sources of astrophysical data. Our study's concerned with contact-binary systems — a pair of stars in physical contact sharing a common envelope. Due to mass transfer between the stars, the structure and evolution of these systems differ greatly from single-star systems. Here, we develop an N-body model that simulates evolving contact-binary star systems. With it, we study the evolution of contact binaries, in particular the role mass transfer between stars plays in this process.","Bryant Wyatt, Math Professor, Tarleton State University",Higher Education / Research
NVIDIA GTC 2020,Overcoming Latency Barriers: Strong Scaling HPC Applications with NVSHMEM [s21673],"For scientific advancement through HPC, ever-increasing simulation capabilities are not the only key to success. Obtaining timely results is often even more important. Reducing the time-to-solution generally requires the application to be strong-scalable. However, scaling up improved single-GPU performance faces many obstacles. We'll show you how to improve the strong-scaling on systems equipped with NVIDIA GPUs. Avoid or hide latencies by exploiting GPU-centric communication with NVSHMEM, an implementation of OpenSHMEM for GPUs. After introducing NVSHMEM, we'll share best practices gathered from using NVSHMEM for QUDA, a library for Lattice QCD on GPUs used by codes as MILC and Chroma. We show results obtained on fat-GPU nodes like DGX-1/2, as well as scaling them to 1,000 GPUs in InfiniBand-connected systems, including Summit.","Mathias Wagner, Senior Developer Technology Engineer, NVIDIA",Higher Education / Research
NVIDIA GTC 2020,Parallel Index-Based Search Operation for Database Systems via GPUs [P21987],"Handling a large number of queries concurrently is a crucial characteristic of today’s in-memory database system. By supporting such characteristics, index structure has a vital role in a database system. In recent years, GPUs have become the leading hardware for parallel computing. The unique architecture of high-performance computation, however, provides abundant opportunities for optimizing the algorithm toward better performance and achieving high utilization of GPU resources. We present our recent study in designing and optimizing parallel algorithms for index-search on GPUs. We also present techniques to optimize the search operation on both equality and range searches by using a novel clustering technique that can maximize the utilization of an on-chip GPU cache system. To evaluate our index structure, we compare the searching time with the best CPU SIMI index-based searching.","Napath Pitaksirianan, Ph.D. Candidate, University Of South Florida",Higher Education / Research
NVIDIA GTC 2020,Rapid Pathogen Genomics using Nanopore Sequencing and GPUs [P22462],This project employs a combination of advanced genomics methods in the context of pathogen surveillance. The MinION device is used for nanopore or third-generation DNA sequencing. The MinION is small and portable and can be used to monitor microbial communities directly in the field. Recently released tools using Clara Genomics Analysis SDK enabled rapid genome analysis with GPU-enabled hardware. We envision that our methods will be further developed for field-forward applications of public health.,"Devin Drown, Assistant Professor, University of Alaska Fairbanks",Higher Education / Research
NVIDIA GTC 2020,Rebalancing the Load: Profile-Guided Optimization of the NAMD Molecular Dynamics Program for Modern GPUs using Nsight Systems [s21547],"We'll show how we more than doubled the performance of the Nanoscale Molecular Dynamics program on modern GPUs through the use of profile-guided optimization with Nsight Systems and Nsight Compute. NAMD has historically offloaded most computation to GPUs, leaving a much smaller amount of work and device management to the CPU. However, the tremendous performance available on modern GPUs has led to an imbalance, in which the CPU work becomes a bottleneck while the GPU is left idling. We'll show how we improve GPU utilization and overall application performance by moving the remaining workload to the GPU while reducing CPU overhead due to communication and synchronization, all discovered with the help of Nsight Systems. We'll also discuss ongoing efforts in tuning the performance of our most compute-intensive kernels with Nsight Compute.","Julio Maia, Research Programmer, University Of Illinois at Urbana-Champaign",Higher Education / Research
NVIDIA GTC 2020,Running Multi-Messenger Astrophysics with IceCube Across All Available GPUs in the Cloud [S22206],"We'll report on a computational experiment that marshaled all globally available for-sale NVIDIA GPUs across AWS, Azure, and GCP. The net result was a peak of about 51,000 GPUs of eight different kinds, with an aggregate peak of about 380 PFLOPS fp32. The experiment used simulations for the IceCube Neutrino Observatory, an array of some 5,000 optical sensors buried deep within a cubic kilometer of ice at the South Pole. The sensors detect the signatures of shock waves created by particles from neutrino interactions passing through the ancient ice sheets. Simulation is needed to properly account for natural ice imperfections, and photon propagation codes are a natural fit for GPGPU computing. We'll provide both a summary overview and technical details of the infrastructure needed to create a supercomputer-like environment across multiple cloud providers, as well as an overview of the science behind IceCube and how GPU compute helps in advancing the scientific goals.","Igor Sfiligoi, Lead Scientific Software Developer and Researcher, UC San Diego - San Diego Supercomputer Center",Higher Education / Research
NVIDIA GTC 2020,Runtime Analysis of Spatial Structure: A CUDA Implementation of Minkowski Functionals [P21829],"Interested in characterizing spatial structure inherent in 3D scalar fields from simulated or imaged data? This poster presents an accelerated solution of the widely-used Minkowski functionals, using both OpenACC and CUDA on commodity GPUs. We'll present methods to minimize the memory footprint, and hence reduce data-transfer costs. Based on measurement frequency, an OpenACC rather than a CUDA solution might be appropriate. Next steps highlight the additional methods to further enhance and fine-tune the performance of the CUDA solution. Minkowski functionals have been widely applied in cosmology, material science, engineering, microbial ecology, and health care.","Ruth Falconer, Head of Division, Abertay University",Higher Education / Research
NVIDIA GTC 2020,Scaling Deep Learning Interpretability by Visualizing Activation and Attribution Summarizations [S21808],"We'll present Summit, an interactive system that scalably and systematically summarizes and visualizes the features that a deep learning model has learned, and how those features interact to make predictions. Summit introduces two new scalable summarization techniques: activation aggregation discovers important neurons, and neuron-influence aggregation identifies relationships among such neurons. Summit combines these techniques to create the novel attribution graph that reveals and summarizes crucial neuron associations and substructures that contribute to a model’s outcomes. Summit scales to large data, such as the ImageNet dataset with 1.2 million images, and leverages neural network feature visualization and dataset examples to help users distill large, complex neural network models into compact, interactive visualizations. We'll present neural network exploration scenarios where Summit helps us discover multiple surprising insights into a prevalent, large-scale image classifier’s learned representations and informs future neural network architecture design. The Summit visualization runs in modern web browsers and is open sourced.","Polo Chau, Associate Professor, The Georgia Institute of Technology",Higher Education / Research
NVIDIA GTC 2020,Self-Supervised Robot Learning from Pixels [s21921],"How can robots learn manipulation skills from raw sensory input without external supervision? We'll propose an algorithm that acquires such general-purpose skills by combining unsupervised representation learning and reinforcement learning of goal-conditioned policies. Since the particular goals aren't known in advance, the agent performs a self-supervised ""practice"" phase where it imagines goals and tries to achieve them. We learn a visual representation with three distinct purposes: sampling goals for self-supervised practice, providing a structured transformation of raw sensory inputs, and computing a reward signal for goal-reaching. A retroactive goal-relabeling scheme further improves the sample efficiency of our method. Our off-policy algorithm is efficient enough to learn policies that operate on raw image observations and goals for a real-world robotic system, and substantially outperforms prior techniques.","Ashvin Nair, Student, University of California, Berkeley",Higher Education / Research
NVIDIA GTC 2020,Sim-to-Real: Virtual Guidance for Robot Navigation [P22224],"We present an effective, easy-to-implement, and low-cost modular framework for completing complex navigation tasks. Our proposed method is based on a single monocular camera to localize, plan, and navigate. A localization module in our framework first localizes and acquires the robot’s pose, which is then forwarded to our planner module to generate a global path and its intermediate waypoints. This information, along with the pose of the robot, is then reinterpreted by our framework to form the “virtual guide”, which serves as a virtual lure for enticing the robot to move toward a specific direction. We evaluate our framework on a Husky robot in a number of virtual and real-world environments, and validate that our framework is able to adapt to unfamiliar environments and demonstrate robustness to various environmental conditions.","Chun-Yi Lee, Associate Professor, National Tsing Hua University",Higher Education / Research
NVIDIA GTC 2020,*Top 5 Poster Nominee - Photometric Mesh Optimization for Video-Aligned 3D Object Reconstruction [P22276],"We address the problem of 3D object mesh reconstruction from RGB videos. Our approach combines the best of multi-view geometric and data-driven methods for 3D reconstruction by optimizing object meshes for multi-view photometric consistency while constraining mesh deformations with a shape prior. We pose this as a piecewise image alignment problem for each mesh face projection. Our approach allows us to update shape parameters from the photometric error without any depth or mask information. Moreover, we show how to avoid a degeneracy of zero photometric gradients via rasterizing from a virtual viewpoint. We demonstrate 3D object mesh reconstruction results from both synthetic and real-world videos with our photometric mesh optimization, which is unachievable with either naive mesh generation networks or traditional pipelines of surface reconstruction without heavy manual post-processing.","Chen-Hsuan Lin, Ph.D. Student, Carnegie Mellon University",Higher Education / Research
NVIDIA GTC 2020,Toward Detailed Organ-Scale Simulations in Cardiac Electrophysiology [P21982],"The poster presents previous and ongoing work in the area of cardiac electrophysiology. It introduces the problem and describes the previous state of the art, as well as the improvement obtained through the use of GPUs. Furthermore, it addresses the challenges of organ-scale simulations, including the fact that whole-heart simulation with realistic calcium handling constitutes an exascale problem.","Johannes Langguth, Research Scientist, Simula Research Laboratory",Higher Education / Research
NVIDIA GTC 2020,Toward Optimal Implementation of Lattice Boltzmann CFD Simulator for Multi-GPU Clusters [P21922],"We present our progress toward a general and efficient multi-GPU solver for single-phase fluid flow based on the lattice Boltzmann method (LBM). LBM is suitable for parallelization on GPUs. However, the amount of global memory available on a single GPU is the limiting factor. Recently, we've extended our LBM solver using the message-passing interface library in order to utilize many GPUs available on HPC clusters. Our poster describes the optimization strategies used in the solver, analyzes the weak-scaling efficiency, and shows the results of preliminary high-resolution simulations for problems related to our research projects.","Jakub Klinkovsk, Ph.D. Student, Department of Mathematics, Faculty of Nuclear Sciences and Physical Engineering, Czech Technical University, Prague",Higher Education / Research
NVIDIA GTC 2020,Uniform Tasking on GPUs and CPUs: Task Queues Reloaded [P21938],"GPU programming with a difference! The poster covers our insights into the development and optimization of a tasking framework for GPUs. We provide an overview of our CUDA C++ tasking framework for fine-grained task parallelism on GPUs. After a short trip into the world of persistent threads, synchronization mechanisms, and load balancing, we dig a bit deeper by finding and analyzing performance bottlenecks. On that point, we present diverse optimization strategies for task queues. First, we describe the implementation of task queues based on static memory allocation. Second, we show how to implement work sharing on a GPU through hierarchical task queues. Third, we present a thread coordination scheme to reduce contention on the task queues, thus keeping all threads busy. Finally, we analyze the performance gains reached through the described optimizations.","Laura Morgenstern, Ph.D. Student, Computer Science, Jülich Supercomputing Centre",Higher Education / Research
NVIDIA GTC 2020,Virtual GPU Computing for HPC: Improving System Utilization Through GPU Virtualization [S21827],"We'll discuss Clemson University's early experience of applying the NVIDIA vComputeServer in the high performance computing infrastructure to maximize their investment in GPU resources. We'll present two use cases: one for running the Metamoto simulation workload for the Open Connected Autonomous Vehicle project, and the other for general GPU-accelerated applications in a production HPC cluster.","Konstantin Cvetanov, Senior Solution Architect, NVIDIA",Higher Education / Research
NVIDIA GTC 2020,Affordable Modeling of Complex Extreme Events in the Built Environment Using GPU-accelerated CFD [S21373],"We'll explain how Arup and Zenotech are making progress in resolving speed, accuracy, and cost challenges with CFD algorithmic advances, here exemplified by Zenotech’s zCFD code, and GPU acceleration. Using a particularly challenging turbulent flow phenomenon exhibiting fleeting but extreme pressure peaks as a test case, we'll show the outcome of validation tests carried out on a DGX-1 system and compared with comprehensive, high-quality wind-tunnel test data. We'll also demonstrate how in-situ visualization of 3D flows, using NVIDIA IndeX and ParaView Catalyst, opens powerful new possibilities for finding insights into complex fluid dynamical behaviors.","Tariq Saeed, Engineer, Arup",Engineering / Transportation
NVIDIA GTC 2020,Anomaly Detection of Diesel Engine Using Deep Learning with Xavier [s21394],"We've developed an anomaly-detection machine-learning algorithm that can be easily applied to a real machine using NVIDIA's Xavier. Many studies using machine-learning algorithms have been conducted recently, but an embedded board is essential in the field, where PC-based computing power such as construction equipment cannot be used. Using embedded boards like NVIDIA's TX2 and Xavier, it was easy to apply the trained model to real equipment.","Gyebong Jang, Ph.D. Candidate, Yonsei University",Engineering / Transportation
NVIDIA GTC 2020,A Software Systolic Array on GPUs [P22277],"We propose a versatile high-performance execution model, inspired by systolic arrays, for memory-bound regular kernels running on CUDA-enabled GPUs. More specifically, we build a virtual systolic array on the top of CUDA architecture. We formulate a systolic model that shifts partial sums by CUDA warp primitives for the computation. We also employ register files as a cache resource in order to operate the entire model efficiently. We demonstrate the effectiveness and the versatility of the proposed model for a wide variety of stencil kernels that appear commonly in HPC, and also convolution kernels (increasingly important in deep-learning workloads). Our algorithm outperforms the top reported state-of-the-art stencil implementations, including implementations with sophisticated temporal and spatial blocking techniques. For 2D convolution of general filter sizes and shapes, our algorithm is on average 2.5 times faster than NVIDIA’s NPP on Tesla V100 and P100 GPUs.","Mohamed Wahib, Senior Researcher, AIST-Tokyo Tech Real World Big-Data Computation Open Innovation Laboratory, National Institute of Advanced Industrial Science and Technology",Engineering / Transportation
NVIDIA GTC 2020,Designing Better Cities with Deep Learning and Performance Simulation [S21493],"We'll present the use of conditional generative adversarial networks to augment the architectural design process with real-time performance analytics, including wind flow and internal daylight. Building-performance simulation tools have been used for decades in architecture and urban design, but their time and computational expense make them ineffective during the early stages when they are most relevant. We'll describe the challenges in building the dataset, encoding multi-variate parameters, and translating the model's output into the most useful format. We'll also present our exploration of different neural network architectures, selection of the appropriate loss function, and optimization of the training process to fully utilize the RTX 8000 GPU. Finally, we'll touch on our considerations in deploying the trained model with the goal of the real-time inference, and how we applied it on projects in practice.","Sarah Mokhtar, Computational Designer, Kohn Pedersen Fox Associates",Engineering / Transportation
NVIDIA GTC 2020,"Drones, Machetes, Fire, and VR: 21st-Century Tools for Social and Sustainable Impact [s21515]","We'll describe and compare workflows developed for projects on the islands of Kosrae, Micronesia and Vorovoro, Fiji. These projects—collaboratively executed with indigenous locals, universities, social-impact organizations, and technology corporations—built immersive experiences to support grant proposals, feasibility studies, and mitigation plans. Our climate is changing, and our global population is increasing. Remote, indigenous communities are developing to accommodate growth and responsible tourism in the face of catastrophic environmental and political forces. Technology is now available to support and promote development of remote communities in an environmentally and culturally sustainable manner. Specifically, photogrammetry and virtual reality enable effective decisions about mitigating the effects of climate change and developing sustainably by empowering stakeholders with intuitive, immersive experience of site conditions from new points of view.","Dace Campbell, Director of Product Management in Construction, McKinstry",Engineering / Transportation
NVIDIA GTC 2020,GPU Rendering for Architectural Visualization [S21657],"At Neoscape, we've created architectural visualization for over two decades. We've also waited days and weeks for our work to render in our CPU render farm — one of the biggest pain points for us and our clients. Now, NVIDIA’s RTX GPU technology rendering has turned days into hours. We'll explain how Neoscape has been experimenting and implementing some of these new technologies with different goals in mind for our diverse clientele, from Chaos group GPU rendering to Epic Games real-time ray tracing.","Carlos Cristerna, Principal, RadLab Director, Neoscape",Engineering / Transportation
NVIDIA GTC 2020,High-Resolution Image Reconstruction on Supercomputers [P21724],"Computed tomography (CT) is a widely used technology that requires compute-intense algorithms for image reconstruction. We propose an efficient implementation that takes advantage of the heterogeneity of GPU-accelerated systems by overlapping the filtering and back-projection stages on CPUs and GPUs, respectively. We also propose a distributed framework for high-resolution image reconstruction on state-of-the-art GPU accelerated supercomputers. The framework relies on an elaborate interleave of MPI collective communication steps to achieve scalable communication. We also demonstrate the scalability and instantaneous CT capability of the distributed framework by using up to 2,048 V100 GPUs to solve 4K and 8K problems within 30 seconds and 2 minutes, respectively.","Peng Chen, Ph.D. Student, Tokyo Institute of Technology",Engineering / Transportation
NVIDIA GTC 2020,Immersive Reality Improving the Construction Industry [S22168],"The health care industry is continually looking to improve how doctors, nurses, and patients view hospitals and care centers throughout the country. With a primary focus on making hospitals and immediate care centers more welcoming, building teams are embracing new technology and using virtual mock-ups to revolutionize how patients and customers see and interact with spaces. Use of virtual reality mock-ups will bring notable changes in planning, acquiring new equipment, combining old and new standards, and incorporating feedback from doctors into ROI results.","Tom Bossow, VDC Coordinator, Mortenson Construction",Engineering / Transportation
NVIDIA GTC 2020,Latest Advancements for Production Rendering with V-Ray GPU and Real-Time Raytracing with Project Lavina [S22197],"We'll cover the latest improvements in V-Ray GPU, including out-of-core rendering and RTX support through OptiX 7, as well as real-time raytracing with DXR and Project Lavina.","Alexander Soklev, V-Ray GPU Team Lead, Chaos Group",Engineering / Transportation
NVIDIA GTC 2020,Leveraging OptiX 7 for High-Performance Multi-GPU Raytracing on Head-Mounted Displays [S21425],"NVIDIA recently introduced OptiX 7, which provides low-level access to the RTX technology and raytracing (RT) cores of the Turing architecture. Its CUDA-centric nature enables direct control over GPU resources — particularly in a multi-GPU context — which allows for a much more efficient scalability compared to previous OptiX versions. We'll show how to utilize OptiX 7 for full-frame raytracing on professional head-mounted displays (HMDs). Specifically, we'll demonstrate how OptiX is used in a dual-GPU setup to rapidly generate stereoscopic output, tailored to the specific optical characteristics of an HMD. Finally, we'll provide an overview of how RTX and OptiX 7 are integrated into ESI’s in-house rendering engine Helios, and will demonstrate the potential of real-time raytracing with practical use cases.","Andreas Dietrich, Senior Software Developer, ESI Group",Engineering / Transportation
NVIDIA GTC 2020,Low-Cost OpenACC Porting of Matrix Solver with FP21-FP32-FP64 Computing: an Earthquake Application [P21886],"We show that OpenACC is useful for GPU porting of a practical application with FP21-FP32-FP64 mixed-precision computing. FP21 is our custom 21-bit floating-point data type for scientific computing. Here, we ported a soil liquefaction analysis solver that was developed for manycore CPU-based computers. The OpenACC-ported solver achieved a 10.7-fold speedup over the CPU-based solver on a system where the ratio of peak FP64 FLOPS was CPU:GPU = 1:10.2. It took only three weeks for a beginning GPU programmer to port the solver to GPU.","Takuma Yamaguchi, Ph.D. Student, University of Tokyo",Engineering / Transportation
NVIDIA GTC 2020,N-body Adaptive Optimization of Lattice Towers [P21881],"There are tens of millions of power transmission towers In the United States alone. The dilemma central to their cost optimization is that a beam’s cross-sectional area is proportional to both its strength and cost — that is, a thicker beam, while able to support a given load with less strain, will cost more and weigh more heavily on the beams supporting it. By varying the cross section of each beam, we want to make towers as light and inexpensive as possible without sacrificing their structural integrity. Known as truss-sizing optimization, this problem is differential in nature and heavily dependent on tower geometry, lending it to a computational approach. Drawing inspiration from the atrophy and hypertrophy of muscles, we developed and evaluated an optimization algorithm that adaptively resizes beams based on their stress — a process that produces rapid results and allows the application of both static and dynamic loads, setting it apart from popular algorithms in this intensely studied field.","Bryant Wyatt, Math Professor, Tarleton State University",Engineering / Transportation
NVIDIA GTC 2020,PBR Material Creation from a Single Picture in Substance Alchemist [S22194],"After presenting the first AI-powered Delighter at GTC 2019, and after Substance Alchemist was released during Adobe Max in Nov 2019, we'll present the next steps of the technology and the work engaged to recover a full high-resolution tileable PBR material and how the RTX Tensor Cores boost the performance for the Substance Alchemist release to come in 2020.","Rosalie Martin, Senior Software Engineer, Adobe",Engineering / Transportation
NVIDIA GTC 2020,Ray-Traced Virtual Reality in Omniverse [S22029],"Ominverse is a new platform developed by NVIDIA to share scenes and models between different editors and viewers. Ray tracing is used to accurately visualize content within the Omniverse Kit viewer. As quality ray-tracing effects (such as reflections, soft shadows, and ambient occlusion) are expensive to compute, we'll discuss how we were able to use eye-tracked foveation and warped-space rendering to achieve sufficient performance and quality gains for a virtual reality viewer. We'll also show how adding multi-frame explicit history reprojection to our de-noising strategy better handles the motion of VR interactions. To further improve performance, we'll discuss our strategies for dividing work between multiple GPUs. Streaming allows us to decouple the multi-GPU rendering server from the headset. Finally, we'll demonstrate our application to allow you to experience first-hand the benefits of eye-tracked foveation and ray tracing.","Jeroen Stinstra, Dev Tech Medical Imaging, NVIDIA",Engineering / Transportation
NVIDIA GTC 2020,Real-Time Ray-Traced Ambient Occlusion of Complex Scenes using Spatial Hashing [S22170],"Ambient occlusion is an effective way to approximate global illumination: in essence, the closer a point is to its surroundings, the darker it gets. For real-time rendering, this effect is often approximated using screen-space techniques, leading to visible artifacts. Ray tracing provides a unique way to increase the rendering fidelity by accurately computing the distance to the surrounding objects, but it introduces sampling noise. Using the NVIDIA RTX technology available with Vulkan, we propose a real-time ray-traced ambient occlusion technique in which noise is removed in world space. Using spatial hashing for efficient storage, we'll cover all the technical challenges to make ambient occlusion a production feature usable in CAD viewports with scenes comprising thousands of instances and hundreds of millions of polygons.","Pascal Gautron, Senior Developer Technology Engineer, NVIDIA",Engineering / Transportation
NVIDIA GTC 2020,Semi-Supervised Three-Dimensional Reconstruction with Generative Adversarial Networks [P21908],"Existing methods for 3D reconstruction often produce holes, distortions, and obscured parts in the reconstructed 3D models, or they can only reconstruct voxelized 3D models for simple isolated objects. So they are not adequate for real usage. This poster's focus is to achieve high-quality 3D reconstruction performance by adopting the generative adversarial network (GAN) approach. We'll propose a novel, semi-supervised 3D reconstruction framework (SS-3D-GAN), which can iteratively improve any raw 3D reconstruction models by training the GAN models to converge. This new model only takes real-time 2D observation images as the weak supervision and doesn't rely on prior knowledge of shape models or any referenced observations. Through the qualitative, quantitative experiments and analysis, SS-3D-GAN shows compelling advantages over the current state-of-the-art methods on the benchmark datasets. We'll also prove the acceleration effect of NVIDIA GPUs in the 3D reconstruction research and applications.","Chong Yu, Senior Architect, NVIDIA",Engineering / Transportation
NVIDIA GTC 2020,Tensor Core Accelerated Sparse GEMM [P21866],"Sparse matrix multiplication is an important tool in many scientific applications, like graph analytics and deformable object modeling. It's also valuable in deep-learning networks with sparse inputs. We'll present a novel way to utilize the tensor cores of the latest NVIDIA GPUs to accelerate sparse matrix-matrix multiplication. Our approach splits the operand matrices into tiles and schedules them to tensor cores for fast matrix multiplication, outperforming two state-of-the-art libraries, CUSP and cuSPARSE, by an average of 2x.","Orestis Zachariadis, Ph.D. Candidate, University of Cordoba",Engineering / Transportation
NVIDIA GTC 2020,Virtual Reality's Continued Impact in the World of AEC [s22129],"Not too long ago, it seemed that every enterprise company had an R&D team trying to decide if ""video game technology"" could be used for their visualization needs. The days of pilot projects have come and gone, and virtual reality is definitely here to stay. In this talk, Theia Interactive will introduce you to a variety of AEC customers using virtual reality and Unreal Engine on a daily basis to give them an edge in this new era of graphics.","Stephen Phillips, CTO, Theia Interactive",Engineering / Transportation
NVIDIA GTC 2020,A Study of Pedestrian Protection CAE Using GAN [S21438],"According to the World Health Organization, there are over 270,000 pedestrians involved in traffic fatal accidents. That is 22% of all traffic fatalities verified in the world in 2013. To reduce pedestrian fatalities, third-party organizations in many countries have held New Car Assessment Program (NCAP) tests to evaluate pedestrian-protection performance. For that reason, an efficient method to design pedestrian-protection performance is required for automobile development for all over the world. Computer-aided engineering (CAE) is often used to verify pedestrian-protection performance. But as the necessity for pedestrian protection expands globally, expectations to improve efficiency have recently risen. We decided to reduce CAE verification time and improve the accuracy of CAE using deep learning. We also studied visualizing the basis for judgment in the trained models.","Osamu Ito, Assistant Chief Engineer, Honda R&D",Engineering / Transportation
NVIDIA GTC 2020,Automating DNN Design for DRIVE AGX: Platform-Aware Neural Architecture Search [S21666],"In the past few years, wide applications of deep neural networks (DNN) have contributed to significant progress in various fields such as image classification, object detection, and segmentation. Most of the successful DNNs, such as VGG and ResNet, are designed by humans, which requires in-depth domain expertise and effort. While DNNs have become deeper and wider, the need for fast inference is increasing on (edge) computing devices, while accuracy must be maintained. Therefore, developing state-of-the-art neural networks for resource-constrained applications has become challenging. We'll present our progress on the automated design of neural networks using hardware-aware neural architecture search (NAS) techniques. We show concrete end-to-end examples from differentiable and latency-reflected search of optimal network architectures to their deployment on NVIDIA’s DRIVE AGX platforms using TensorRT for autonomous-driving-related applications.","Le An, Senior Deep Learning Software Engineer – Autonomous Vehicles, NVIDIA",Engineering / Transportation
NVIDIA GTC 2020,Beyond-Line-of-Sight (BLOS) Perception System for Autonomous Vehicles [P21842],"Recently, autonomous vehicle perception systems show great performance based on deep-learning technology. Cameras and lidar are commonly used for autonomous driving. However, onboard sensors naturally have limited line-of-sight. Our work introduces a BLOS (beyond-line-of-sight) perception system that mutually complements communicated perception with V2X technology and local perception with onboard sensors. The proposed system is integrated into a full-scale vehicle, and we validated the performance and potential by experiments in the real world.","Chanyoung Chung, Researcher, KAIST (Korea Advanced Institute of Science and Technology)",Engineering / Transportation
NVIDIA GTC 2020,Bike-Share Demand Prediction from Partial Future data Using Conditional Variational AutoEncoders [P21739],"Recently, bike-sharing services are working worldwide. One important aspect of bike-share management is to periodically rebalance the positions of the available bikes. Because the bike demand varies by and over time, the number of bikes at each bike-port tends to become unbalanced. To efficiently rebalance a bike-share system, it is essential to predict the number of bikes in each bike-port. In this poster, we propose a method to predict the bike demand at each bike-port every hour, up to 24 hours ahead. This method is based on Variational AutoEncoders and Sequence-to-Sequence Neural Networks. We called this method “Conditional Variational AutoEncoders considering Partial Future data”. In the experiment, our proposal method showed higher prediction accuracy than the other methods.","Mimura Tomohiro, Software Engineer, NTT DOCOMO, INC.",Engineering / Transportation
NVIDIA GTC 2020,Bringing the Autodesk VRED Raytracer to the GPU [s21344],"Autodesk VRED helps designers and engineers create product presentations, design reviews, and virtual prototypes using interactive raytracing and analytic render modes. We'll show how we brought the VRED raytracer to the GPU using NVIDIA Optix. We'll also talk about specific requirements and challenges we faced, and demonstrate the results we achieved utilizing NVIDIA RTX.","Michael Nikelsky, Senior Principal Engineer, Autodesk",Engineering / Transportation
NVIDIA GTC 2020,Coverage-Driven Verification for Ensuring AV and ADAS Safety [S22183],"One of the main evolutions the autonomous vehicle industry has seen over the last couple of years is that many companies have reached the point where they can make the fundamentals of AVs work just fine — sensing, planning routes, controlling the vehicle. The really hard part, though, is the validation and verification of all of the hazardous edge cases. We'll address these tough questions about how to enable sufficient verification to ensure AV safety:
How do you handle the infinite space of all the possible driving scenarios that need to be tested?
How can you control the various simulators and other execution platforms used for running the testing scenarios?
How can functional coverage help to systematically go over all the known and unknown hazardous edge cases?
How can you provide metrics that show that the remaining hazardous test cases result in a risk that is acceptable?","Ziv Binyamini, CEO, Foretellix",Engineering / Transportation
NVIDIA GTC 2020,Deep Learning for 3D Vision (Point Clouds) [S21112],"Learning on 3D point clouds is vital for a broad range of emerging applications such as autonomous driving, robot perception, VR/AR, gaming, and security. Such needs have increased recently due to the prevalence of 3D sensors such as lidar, 3D cameras, and RGB-D depth sensors. Point clouds consist of thousands to millions of points and are complementary to the traditional 2D cameras in the vision (or multimedia) community. 3D learning algorithms on point cloud data are new, and exciting, for numerous core problems such as 3D classification, detection, semantic segmentation, and face recognition. The tutorial covers the 3D sensors, 3D representations, emerging applications, core problems, state-of-the-art learning algorithms (for example, voxel-based and point-based), and future research opportunities. We'll also showcase our leading work in several 3D benchmarks such as ScanNet, KITTI, etc., and efficient neural network training (with data parallelism) by NVIDIA GPU platforms (for example, DGX-1).","Winston Hsu, Professor, National Taiwan University",Engineering / Transportation
NVIDIA GTC 2020,Drones as a Tool for Development and Safety Validation of Automated Driving Functions [P21913],"Automated driving heavily relies on data-driven methods. Large datasets of real-world measurement data, in the form of road-user trajectories, are crucial for several tasks, such as road-user prediction models or scenario-based safety validation. Using a drone has the major advantage of recording naturalistic behavior. Due to the ideal viewing angle, an entire scenario can be measured with significantly less occlusion than with sensors at ground level. Both the class and the trajectory of each road user can be extracted from the video recordings with high precision using state-of-the-art deep neural networks. Therefore, we have created a large-scale urban intersection dataset with naturalistic road-user behavior using camera-equipped drones. The resulting dataset contains road users including vehicles, bicyclists, and pedestrians at intersections in Germany, and is called inD. The dataset is available online for non-commercial research at: http://www.inD-dataset.com.","Julian Bock, Manager Artificial Intelligence, fka GmbH",Engineering / Transportation
NVIDIA GTC 2020,Eliminating Hidden Bias in Autonomy and Beyond [S22701],"Speaker: Deepti Mahajan, Machine Learning Engineer, Ford Greenfield Lab We tend to trust that the systems that govern our day-to-day life—standards followed by industry, regulations passed by governments—are based on thorough research. However, in many cases, the data used to design these systems fails to represent us all equally and can even reinforce or amplify existing biases. As designers and engineers of the next generation of mobility, how can we learn to recognize hidden bias in the data we utilize and information we take for granted, and thus work towards creating systems that serve everyone?","Deepti Mahajan, Machine Learning Research Engineer, Ford Motor Company",Engineering / Transportation
NVIDIA GTC 2020,"End-to-End Learning with Combined ""Real-World"" and ""Sim-World"" Datasets [P22060]","Training end-to-end learning for self driving requires large and diverse data. We can deal with that if we collect the data not just from the real world, but also simulations. Therefore, we built a customized map based on the real-world proving ground, KIAPI in Korea, using the Unreal Engine. By using the map in the open-source simulator, CARLA, we could collect both simulated and reality-based datasets of KIAPI in parallel. We integrated both datasets in several different ratios and evaluated their validity by training an end-to-end learning network with them.","Hyunki Seong, Master's Student, KAIST",Engineering / Transportation
NVIDIA GTC 2020,Generating Diverse and Photorealistic Synthetic Data for Real-World Perception Tasks [S21321],"We'll cover these two related topics: First, leveraging generative adversarial networks for style transfer to diversify simulated images rendered in simple domains (that is, easier to render realistically, such as daytime) into photorealistic images in different weather and lighting conditions, using domain translation models (such as day-to-night, clear-to-rainy, clear-to-snowy) learned once from generic real-world datasets; and second, investigating the role of the discriminator’s receptive field in unsupervised sim-to-real image translation. We'll show that reducing the discriminator’s receptive field is directly proportional to improved structural coherence during translation in scenarios where the real and simulated images used for training have mismatched content — a situation often encountered in real-world deployment. Prior knowledge in computer vision and deep learning will help you get the most out of this session.","Nikita Jaipuria, Research Scientist, Computer Vision & Machine Learning, Ford Motor Company",Engineering / Transportation
NVIDIA GTC 2020,Geolocation of Traffic Lights and Signs Using Dashcam: Toward Low-Cost Map Maintenance [P21912],"HD maps for autonomous driving need to be maintained as close to the current real world as possible, but it's quite costly and time-consuming to employ many workers or special mobile-mapping systems. To solve that, we're developing technologies and systems that extract information required for map maintenance from low-cost dashcam videos by utilizing rapidly advancing computer vision techniques. This poster introduces our current basic pipeline to detect and geolocate traffic lights and signs captured in dashcam videos. We developed our own dataset by collecting videos from vehicles running on various Japanese roads. Experimental evaluation using the dataset demonstrates that our system can detect objects with more than 85% accuracy, and can estimate their locations within less than 10 meters under the Global Navigation Satellite System.","Kazuyuki Miyazawa, AI Research Engineer, DeNA Co., Ltd.",Engineering / Transportation
NVIDIA GTC 2020,GPU-Accelerated Data Pipeline and Machine Learning on DRIVE AGX using RAPIDS [S21665],"We'll present the extended capability of RAPIDS on the DRIVE AGX platform by demonstrating how it enhances the in-car user experience, with examples. ML algorithms are used extensively to address various challenges in autonomous cars. They include complex, multi-stage data science pipeline, sensor data processing, modeling, and analytics to accomplish new ML-based applications. Potential applications include recommender systems through driver or vehicle personalization, visual analytics of driving data, classification of driver condition, driving scenario, and more. In many cases, application workload runs on CPUs or ECUs, leading to performance bottlenecks as data size and their computes increase. Application workload can be parallelized, causing significant speedup, using GPUs. NVIDIA developed RAPIDS to accelerate entire end-to-end data science and analytics pipelines on GPUs. In this talk, we present extended capability of RAPIDS on NVIDA DRIVE AGX platform by demonstrating how RAPIDS works.","Andy Park, Senior System Software Engineer, NVIDIA",Engineering / Transportation
NVIDIA GTC 2020,Improving Performance Limits of Obstacle-Avoidance Driving with Randomized Model Predictive Control Using GPU [P22242],"Our poster presents improvements in the performance of obstacle-avoidance driving enabled by a GPU-based controller. A 1:10-scale remote control car is driven in a 60-cm wide track with two parked cars acting as obstacles. The controller is model-predictive with nonlinear constraints. Since a sampling-based approach is used to solve this nonlinear optimization problem, computation speed becomes a constraint as sample size increases. We use parallel computing to solve the sampling-based optimizations. Experiments using our RC car showed that we could achieve >100% increase in driving speed and >40% improvement in cost-function values.","Hiroyuki Okuda, Assistant Professor, Nagoya Univeristy",Engineering / Transportation
NVIDIA GTC 2020,Object Recognition and Tracking Utilizing Millimeter-Wave Radar by Deep Neural Networks [s21188],"All-weather sensors are necessary for autonomous-driving Level 3 and higher. Millimeter-wave radar is the most robust sensor for adverse weather. However, the signal is noisy and fluctuated, and the resolution is low. Thus, recognition using the radar is difficult. Deep-learning algorithms are an effective solution. We'll show a method to classify and track objects in driving scenes with a high-resolution millimeter-wave radar applying long short-term memory. We designed and compared various types of input features and LSTM for our measured dataset and achieved high accuracy through cross validation. We'll also show a method to reconstruct shapes of parking spaces and cars with convolutional neural networks. Parking cars were scanned with side radar. The reflection signals were accumulated, and the shape was estimated by semantic segmentation framework, applying CNN for the ground-truth shape, annotated by a lidar.","Tokihiko Akita, Project Research Fellow, Toyota Technological Institute",Engineering / Transportation
NVIDIA GTC 2020,Optimizing TensorRt Conversion for Real-Time Inference On Autonomous Vehicles [S22198],"TensorRt optimizes neural-network computation for deployment on GPU, but not all operations are supported. Reduced precision inference speeds up computation, but can cause regressions in accuracy. We'll introduce Zoox TensorRt conversion pipeline that addresses these problems. TensorRt compatibility checks are involved at the early stages of neural-network training to ensure that incompatible ops are discovered before wasting time and resources on full-scale training. Inference accuracy checks can be invoked at each layer to identify operations not friendly to reduced-precision computation. Detailed profiling reveals unnecessary computations that aren't optimized inside TensorRt, but can be optimized by simple code changes during graph construction. With this pipeline, we've successfully provided TensorRt conversion support to neural networks performing various perception tasks on the Zoox autonomous driving platform.","Zejia Zheng, Software Engineer, Zoox",Engineering / Transportation
NVIDIA GTC 2020,Pacefish: GPU-Accelerated CFD from Scratch to Market [S21926],"Gain insights into the challenge of developing GPU-accelerated computational fluid dynamics software, from concept to product. Pacefish transforms the power of up to 16 NVIDIA GPUs into a predicted physical flow behavior around complex geometries like cars, buildings, cities, or in rooms. This helps engineers design magnificent cars having better driving capabilities at lower fuel consumption and higher cruising range, for example. Remarkable GPU-to-CPU-node speedup on the order of 20-30x allows Pacefish to drop the costs per simulation by a factor of 10. We'll talk about milestones on the road from scratch to market that we started in 2007, and show some results emphasizing the current product state. You'll learn the lessons from our most important technological decisions from the initiator, founder, and main developer himself.","Eugen Riegel, Managing Partner, Numeric Systems GmbH",Engineering / Transportation
NVIDIA GTC 2020,Panoptic Segmentation DNN for Autonomous Vehicles [S21879],"We'll present our NVIDIA DriveAV's Panoptic Segmentation Deep Neural Network (DNN), which can be used for semantic and instance segmentation of complex scenes for self-driving car scenarios, such as complex urban areas, congested traffic, construction zones with unusual activities, and so on. With Panoptic Segmentation DNN, input images can be accurately parsed for both semantic segmentation (which pixels represent which object class), as well as instance content (which pixels represent which object instance). Planning and control modules can use panoptic segmentation results to better inform autonomous driving decisions. We'll cover our highly accurate GT dataset, DNN architecture, our multi-task training process, and our real-time inference (that includes post-processing steps) on vehicles' AGX compute. Our network achieves state-of-the-art accuracy and runs at 7ms end-to-end on NVIDIA AGX GPUs. We'll show videos of our experiments on a real vehicle in various challenging conditions.","Ke Chen, Senior Deep Learning Scientist, NVIDIA",Engineering / Transportation
NVIDIA GTC 2020,Precise Ultra HD Map Data as Basis for NVIDIA DRIVE Sim Virtual Testing and Simulation [S22273],"Digital road data is the basis for virtual testing and simulation. The roads used for virtual testing and simulation have to be identical digital twins of the real-world roads. 3D mapping presents the technical solution for digitizing test tracks, race tracks, and public roads with high-end mobile surveying using high-resolution scanners and multiple cameras. The technology is used to produce precise high-definition reference maps in OpenDRIVE format, which are either used as basis for virtual simulation and testing or as reference map in the car for autonomous driving development. We'll show various project examples that were completed as the basis for NVIDIA DRIVE Sim through 2019.","Dr.-Ing. Gunnar Gräfe, CEO, 3D Mapping Solutions GmbH",Engineering / Transportation
NVIDIA GTC 2020,PredictionNet: Predicting the Future in Multi-Agent Environments for Autonomous Vehicle Applications [S21899],"Predicting the future trajectories of road agents is an import part of the planning&control stack in autonomous vehicles. Deep-learning approaches can be superior to classical methods in this domain, because neural networks can learn to use context and environment as a prior to improve prediction. We'll present PredictionNet — a deep neural network (DNN) that can be used for predicting future behavior/trajectories of road agents in autonomous-vehicle applications. Our DNN takes a rasterized top-down view of the world provided by the perception system and computes future predictions from past observations. We'll present its architecture, training-data collection process, and our training procedures. We'll also show video demos of live predictions on our self-driving car.","Alexey Kamenev, Principal Deep Learning Scientist, NVIDIA",Engineering / Transportation
NVIDIA GTC 2020,PyTorch-TensorRT: Accelerating Inference in PyTorch with TensorRT [S21671],"TensorRT is a deep-learning inference optimizer and runtime to optimize networks for GPUs and the NVIDIA Deep Learning Accelerator (DLA). Typically, the procedure to optimize models with TensorRT is to first convert a trained model to an intermediary format, such as ONNX, and then parse the file with a TensorRT parser. This works well for networks using common architectures and common operators; however, with the rapid pace of model development, sometimes a DL framework like Tensorflow has ops that are not supported in TensorRT. One solution is to implement plugins for these ops. Another is to use a tool like TF-TRT, which will convert supportable subgraphs to TensorRT and use Tensorflow implementations for the rest. We'll demonstrate the same ability with PyTorch with our new tool PTH-TRT, as well leveraging the PyTorch API's great composability features to allow users to reuse their TensorRT-compatible networks in larger, more complex ones.","Josh Park, Manager - Automotive Solutions Architect , NVIDIA",Engineering / Transportation
NVIDIA GTC 2020,RTX-accelerated Stellar GPU GI – Behind the Scenes of the SIGGRAPH 2019 Demo of the 3DS Global Illumination Renderer [s21315],"We'll give an overview of the technology behind the 3DS global illumination renderer demo, the challenges building the demo, the insights gained from it, and the features developed since. Stellar GPU GI is the GPU backend of the global illumination renderer developed by Dassault Systèmes. First launched in the 3DEXPERIENCE platform in early 2019, it enables rendering experts and novices alike, from design, simulation, and marketing, to generate interactive and offline high quality renderings. Its features and performance, especially in combination with RTX GPUs and RTX servers, were demonstrated to the world at SIGGRAPH 2019.","Jan Meseth, R&D Technology Stellar Physically Correct Director, Dassault Systemes",Engineering / Transportation
NVIDIA GTC 2020,Scalable Storage Environments Optimized for Autonomous Driving (Presented by DDN) [S22671],"Continued advances in the types and number of sensors used by autonomous vehicle companies is leading to developments. It is also leading to data management challenges on a massive scale. The autonomous mobility industry is an entirely new ecosystem combining sensors and other physical components, security, high performance computing technologies, consumer electronics, mapping and geolocation services and a variety of standard IT solutions. With vehicles generating up to 80TB per day, connected car initiatives can demand exabytes of data on a daily basis. We will explain how intelligent, optimized data environments for AI and HPC can streamline and tame the data onslaught. Parallel data paths, diverse data services, remote data caching and tight integration with advanced computing platforms, like NVIDIA DGX systems combine to enable these most scalable solutions.","James Coomer, Senior VP Product Management and Marketing, DDN",Engineering / Transportation
NVIDIA GTC 2020,"Sensor Processing with the NVIDIA DriveWorks SDK: Abstraction, Algorithms, and Acceleration [S21714]","Autonomous vehicles (AV) rely on sensors to represent the world around them, so onboard processing must react to rapidly changing environments. The NVIDIA DriveWorks SDK enables developers to implement such AV solutions by providing an exhaustive library of software modules and tools that leverage the computing power of the NVIDIA DRIVE AGX platform. With DriveWorks, developers can focus on their applications instead of spending time on fundamental functionality and infrastructure. Our session will cover the DriveWorks Sensor Abstraction Layer, a unified interface for sensor life-cycle management, timestamp synchronization, and recording. We’ll then discuss the DriveWorks optimized low-level image and point cloud processing modules for processing incoming sensor data to enable advanced AV algorithms. DriveWorks also supports the DRIVE AGX hardware engines so that these modules can seamlessly run across the Xavier SoC, providing flexibility and performance.","Hope Allen, Product Manager, Autonomous Vehicles, NVIDIA",Engineering / Transportation
NVIDIA GTC 2020,Smart Cities in the Cloud: NVIDIA Metropolis on Red Hat OpenShift [S21994],"We'll talk about the collaboration between NVIDIA and Red Hat, and demo the integration between the NVIDIA's Metropolis application running on EGX and Red Hat OpenShift (Kubernetes) in the public cloud.","Sujit Biswas, Principal Engineer and Data Scientist, NVIDIA",Engineering / Transportation
NVIDIA GTC 2020,Super SloMo: High-Quality Estimation of Multiple Intermediate Frames for Video Interpolation [P22420],We present a computational approach to synthesize slow-motion videos from a plain one.,"Huaizu Jiang, Ph.D. Student, University of Massachusetts, Amherst",Engineering / Transportation
NVIDIA GTC 2020,Temporal Information Prediction for Perception in Autonomous Vehicles [P22335],"Self-driving cars, which have gained significant interest in academia and industry, have become a reality for several automotive companies. Temporal information in an autonomous vehicle, including time-to-collision and object 2D/3D motion, are essential input information for different autonomous vehicle system components, such as automatic cruise control and automatic emergency braking. It allows the autonomous vehicle to accurately understand its surrounding obstacles and provide the critical information to the control system. It can also be used to predict movement of the other objects and estimate their intents. We propose using a recurrent neural network for temporal information prediction. This increased robustness to the temporal prediction for non-rigid objects, such as pedestrians; extended the detection range for faraway objects; reduced noise in prediction; and utilized context pixels of the obstacles.","Cheng-Chieh Yang, System Software Engineer, NVIDIA",Engineering / Transportation
NVIDIA GTC 2020,Terrain Traversability Estimation using Normal Distribution Transform on GPU in Amazon Scout [S21179],"Scout is a autonomous robot from Amazon for delivering packages. Mapping its surroundings in real time to identify traversable space is critical for autonomous driving. We'll discuss how we implemented a technique called ""3D Normal Distribution Transform"" (3D NDT) on NVIDIA TX2 to calculate surface traversability and help Scout navigate the neighborhood autonomously and safely. First, we'll introduce what 3D NDT is and what it is used for. Then we'll discuss how we implemented NDT in Amazon Scout to calculate surface traversability to help the robot to avoid obstacles and navigate safely. We'll highlight how we implemented NDT on NVIDIA TX2 GPU, achieving more than 10x faster run time than before, and how that greatly improved Scout's navigation. We'll also go into the challenges and innovation involved.","Ka Chen, Senior Graphics Engineer, Amazon",Engineering / Transportation
NVIDIA GTC 2020,Toward Large-Scale Steady-State Computational Fluid Dynamics Acceleration with Point Cloud Networks [P22049],"Computational fluid dynamics (CFD) simulation has accelerated product development in the automotive, aerospace, and biomedical industries. Instead of constructing expensive prototypes, a design’s performance can be characterized in simulation. Unfortunately, simulation itself has become a new bottleneck. CFD solvers can take hours, days, or even months to solve the partial differential equations that characterize a design. Deep neural networks trained on CFD solutions can provide approximate solutions in seconds. Point cloud networks can process the unstructured meshes that define most realistic CFD problems. We evaluate one such network (PointNet) on a 2D 131k point cloud dataset. The network makes accurate approximations to the true solution more than 230x faster than a competing GPU solver. With this approach, engineers can evaluate designs more quickly to build a superior product.","Oliver Hennigh, Software Engineer, NVIDIA",Engineering / Transportation
NVIDIA GTC 2020,Ultra-Fast Radar Simulation for Radar System Design and Automotive Applications [s21966],"Ultra-fast, physics-based radar simulations are required to design and deploy hardware-in-the-loop systems for applications such as autonomous vehicles and driver assistance systems (ADAS/AV). Rapid radar image generation is also an enabling technology for AI algorithms, vastly expanding the data sets to train and test the algorithms. We're developing an ultra-fast, end-to-end, GPU-accelerated radar image simulation engine for automotive, AI, and other applications. The shooting and bouncing rays (SBR) technique generates physics-based range-Doppler images of dynamic driving scenarios in urban settings. GPU kernels using CUDA, OptiX, and cuFFT propagate radar energy from the radar, through the scene, and back to the receiver to generate range-Doppler images displaying distance and relative velocity of surrounding objects. ANSYS will discuss the new solver and show results of automotive scenarios in busy, complex environments. The solver shows promise toward our goal of ultra-fast radar simulation.","Jeff Decker, Lead R&D Engineer, ANSYS",Engineering / Transportation
NVIDIA GTC 2020,"Virtual Validation of Automated Vehicle Safety: Challenges, Lessons Learned, and Opportunities [S21658]","We'll explain the need for virtual validation (VV) of automated vehicles (AV), identify the general strengths and weaknesses of VV, briefly review the present landscape of tools and solutions available to perform AV simulations, and list the major building blocks of a good virtual testing process (VTP). We'll also discuss the potential of NVIDIA DRIVE Constellation toward implementing a meaningful VTP for developers, regulators, and service providers. We'll outline some key challenges and opportunities to motivate further research. We'll also introduce two research projects at CETRAN (Nanyang Technological University, Singapore): first, modeling the sensing and perception (S&P) errors using a Perception Error Model (PEM) and studying their impact on decision-making (AV behavior), and second, developing methods to evaluate the holistic fidelity of simulation toolchain and judge its effectiveness in representing reality. We'll present some preliminary findings and a research roadmap.","Jim Cherian, Research Fellow and Lead (AV Simulation/Virtual Validation), CETRAN (Center for Testing and Research on Autonomous Vehicles - NTU), Nanyang Technological University, Singapore",Engineering / Transportation
