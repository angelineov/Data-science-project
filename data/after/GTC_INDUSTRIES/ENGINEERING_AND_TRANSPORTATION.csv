title,description,company,industry
Affordable Modeling of Complex Extreme Events in the Built Environment Using GPU-accelerated CFD [S21373],"We'll explain how Arup and Zenotech are making progress in resolving speed, accuracy, and cost challenges with CFD algorithmic advances, here exemplified by Zenotech’s zCFD code, and GPU acceleration. Using a particularly challenging turbulent flow phenomenon exhibiting fleeting but extreme pressure peaks as a test case, we'll show the outcome of validation tests carried out on a DGX-1 system and compared with comprehensive, high-quality wind-tunnel test data. We'll also demonstrate how in-situ visualization of 3D flows, using NVIDIA IndeX and ParaView Catalyst, opens powerful new possibilities for finding insights into complex fluid dynamical behaviors.","Tariq Saeed, Engineer, Arup",Engineering / Transportation
Anomaly Detection of Diesel Engine Using Deep Learning with Xavier [s21394],"We've developed an anomaly-detection machine-learning algorithm that can be easily applied to a real machine using NVIDIA's Xavier. Many studies using machine-learning algorithms have been conducted recently, but an embedded board is essential in the field, where PC-based computing power such as construction equipment cannot be used. Using embedded boards like NVIDIA's TX2 and Xavier, it was easy to apply the trained model to real equipment.","Gyebong Jang, Ph.D. Candidate, Yonsei University",Engineering / Transportation
A Software Systolic Array on GPUs [P22277],"We propose a versatile high-performance execution model, inspired by systolic arrays, for memory-bound regular kernels running on CUDA-enabled GPUs. More specifically, we build a virtual systolic array on the top of CUDA architecture. We formulate a systolic model that shifts partial sums by CUDA warp primitives for the computation. We also employ register files as a cache resource in order to operate the entire model efficiently. We demonstrate the effectiveness and the versatility of the proposed model for a wide variety of stencil kernels that appear commonly in HPC, and also convolution kernels (increasingly important in deep-learning workloads). Our algorithm outperforms the top reported state-of-the-art stencil implementations, including implementations with sophisticated temporal and spatial blocking techniques. For 2D convolution of general filter sizes and shapes, our algorithm is on average 2.5 times faster than NVIDIA’s NPP on Tesla V100 and P100 GPUs.","Mohamed Wahib, Senior Researcher, AIST-Tokyo Tech Real World Big-Data Computation Open Innovation Laboratory, National Institute of Advanced Industrial Science and Technology",Engineering / Transportation
Designing Better Cities with Deep Learning and Performance Simulation [S21493],"We'll present the use of conditional generative adversarial networks to augment the architectural design process with real-time performance analytics, including wind flow and internal daylight. Building-performance simulation tools have been used for decades in architecture and urban design, but their time and computational expense make them ineffective during the early stages when they are most relevant. We'll describe the challenges in building the dataset, encoding multi-variate parameters, and translating the model's output into the most useful format. We'll also present our exploration of different neural network architectures, selection of the appropriate loss function, and optimization of the training process to fully utilize the RTX 8000 GPU. Finally, we'll touch on our considerations in deploying the trained model with the goal of the real-time inference, and how we applied it on projects in practice.","Sarah Mokhtar, Computational Designer, Kohn Pedersen Fox Associates",Engineering / Transportation
"Drones, Machetes, Fire, and VR: 21st-Century Tools for Social and Sustainable Impact [s21515]","We'll describe and compare workflows developed for projects on the islands of Kosrae, Micronesia and Vorovoro, Fiji. These projects—collaboratively executed with indigenous locals, universities, social-impact organizations, and technology corporations—built immersive experiences to support grant proposals, feasibility studies, and mitigation plans. Our climate is changing, and our global population is increasing. Remote, indigenous communities are developing to accommodate growth and responsible tourism in the face of catastrophic environmental and political forces. Technology is now available to support and promote development of remote communities in an environmentally and culturally sustainable manner. Specifically, photogrammetry and virtual reality enable effective decisions about mitigating the effects of climate change and developing sustainably by empowering stakeholders with intuitive, immersive experience of site conditions from new points of view.","Dace Campbell, Director of Product Management in Construction, McKinstry",Engineering / Transportation
GPU Rendering for Architectural Visualization [S21657],"At Neoscape, we've created architectural visualization for over two decades. We've also waited days and weeks for our work to render in our CPU render farm — one of the biggest pain points for us and our clients. Now, NVIDIA’s RTX GPU technology rendering has turned days into hours. We'll explain how Neoscape has been experimenting and implementing some of these new technologies with different goals in mind for our diverse clientele, from Chaos group GPU rendering to Epic Games real-time ray tracing.","Carlos Cristerna, Principal, RadLab Director, Neoscape",Engineering / Transportation
High-Resolution Image Reconstruction on Supercomputers [P21724],"Computed tomography (CT) is a widely used technology that requires compute-intense algorithms for image reconstruction. We propose an efficient implementation that takes advantage of the heterogeneity of GPU-accelerated systems by overlapping the filtering and back-projection stages on CPUs and GPUs, respectively. We also propose a distributed framework for high-resolution image reconstruction on state-of-the-art GPU accelerated supercomputers. The framework relies on an elaborate interleave of MPI collective communication steps to achieve scalable communication. We also demonstrate the scalability and instantaneous CT capability of the distributed framework by using up to 2,048 V100 GPUs to solve 4K and 8K problems within 30 seconds and 2 minutes, respectively.","Peng Chen, Ph.D. Student, Tokyo Institute of Technology",Engineering / Transportation
Immersive Reality Improving the Construction Industry [S22168],"The health care industry is continually looking to improve how doctors, nurses, and patients view hospitals and care centers throughout the country. With a primary focus on making hospitals and immediate care centers more welcoming, building teams are embracing new technology and using virtual mock-ups to revolutionize how patients and customers see and interact with spaces. Use of virtual reality mock-ups will bring notable changes in planning, acquiring new equipment, combining old and new standards, and incorporating feedback from doctors into ROI results.","Tom Bossow, VDC Coordinator, Mortenson Construction",Engineering / Transportation
Latest Advancements for Production Rendering with V-Ray GPU and Real-Time Raytracing with Project Lavina [S22197],"We'll cover the latest improvements in V-Ray GPU, including out-of-core rendering and RTX support through OptiX 7, as well as real-time raytracing with DXR and Project Lavina.","Alexander Soklev, V-Ray GPU Team Lead, Chaos Group",Engineering / Transportation
Leveraging OptiX 7 for High-Performance Multi-GPU Raytracing on Head-Mounted Displays [S21425],"NVIDIA recently introduced OptiX 7, which provides low-level access to the RTX technology and raytracing (RT) cores of the Turing architecture. Its CUDA-centric nature enables direct control over GPU resources — particularly in a multi-GPU context — which allows for a much more efficient scalability compared to previous OptiX versions. We'll show how to utilize OptiX 7 for full-frame raytracing on professional head-mounted displays (HMDs). Specifically, we'll demonstrate how OptiX is used in a dual-GPU setup to rapidly generate stereoscopic output, tailored to the specific optical characteristics of an HMD. Finally, we'll provide an overview of how RTX and OptiX 7 are integrated into ESI’s in-house rendering engine Helios, and will demonstrate the potential of real-time raytracing with practical use cases.","Andreas Dietrich, Senior Software Developer, ESI Group",Engineering / Transportation
Low-Cost OpenACC Porting of Matrix Solver with FP21-FP32-FP64 Computing: an Earthquake Application [P21886],"We show that OpenACC is useful for GPU porting of a practical application with FP21-FP32-FP64 mixed-precision computing. FP21 is our custom 21-bit floating-point data type for scientific computing. Here, we ported a soil liquefaction analysis solver that was developed for manycore CPU-based computers. The OpenACC-ported solver achieved a 10.7-fold speedup over the CPU-based solver on a system where the ratio of peak FP64 FLOPS was CPU:GPU = 1:10.2. It took only three weeks for a beginning GPU programmer to port the solver to GPU.","Takuma Yamaguchi, Ph.D. Student, University of Tokyo",Engineering / Transportation
N-body Adaptive Optimization of Lattice Towers [P21881],"There are tens of millions of power transmission towers In the United States alone. The dilemma central to their cost optimization is that a beam’s cross-sectional area is proportional to both its strength and cost — that is, a thicker beam, while able to support a given load with less strain, will cost more and weigh more heavily on the beams supporting it. By varying the cross section of each beam, we want to make towers as light and inexpensive as possible without sacrificing their structural integrity. Known as truss-sizing optimization, this problem is differential in nature and heavily dependent on tower geometry, lending it to a computational approach. Drawing inspiration from the atrophy and hypertrophy of muscles, we developed and evaluated an optimization algorithm that adaptively resizes beams based on their stress — a process that produces rapid results and allows the application of both static and dynamic loads, setting it apart from popular algorithms in this intensely studied field.","Bryant Wyatt, Math Professor, Tarleton State University",Engineering / Transportation
PBR Material Creation from a Single Picture in Substance Alchemist [S22194],"After presenting the first AI-powered Delighter at GTC 2019, and after Substance Alchemist was released during Adobe Max in Nov 2019, we'll present the next steps of the technology and the work engaged to recover a full high-resolution tileable PBR material and how the RTX Tensor Cores boost the performance for the Substance Alchemist release to come in 2020.","Rosalie Martin, Senior Software Engineer, Adobe",Engineering / Transportation
Ray-Traced Virtual Reality in Omniverse [S22029],"Ominverse is a new platform developed by NVIDIA to share scenes and models between different editors and viewers. Ray tracing is used to accurately visualize content within the Omniverse Kit viewer. As quality ray-tracing effects (such as reflections, soft shadows, and ambient occlusion) are expensive to compute, we'll discuss how we were able to use eye-tracked foveation and warped-space rendering to achieve sufficient performance and quality gains for a virtual reality viewer. We'll also show how adding multi-frame explicit history reprojection to our de-noising strategy better handles the motion of VR interactions. To further improve performance, we'll discuss our strategies for dividing work between multiple GPUs. Streaming allows us to decouple the multi-GPU rendering server from the headset. Finally, we'll demonstrate our application to allow you to experience first-hand the benefits of eye-tracked foveation and ray tracing.","Jeroen Stinstra, Dev Tech Medical Imaging, NVIDIA",Engineering / Transportation
Real-Time Ray-Traced Ambient Occlusion of Complex Scenes using Spatial Hashing [S22170],"Ambient occlusion is an effective way to approximate global illumination: in essence, the closer a point is to its surroundings, the darker it gets. For real-time rendering, this effect is often approximated using screen-space techniques, leading to visible artifacts. Ray tracing provides a unique way to increase the rendering fidelity by accurately computing the distance to the surrounding objects, but it introduces sampling noise. Using the NVIDIA RTX technology available with Vulkan, we propose a real-time ray-traced ambient occlusion technique in which noise is removed in world space. Using spatial hashing for efficient storage, we'll cover all the technical challenges to make ambient occlusion a production feature usable in CAD viewports with scenes comprising thousands of instances and hundreds of millions of polygons.","Pascal Gautron, Senior Developer Technology Engineer, NVIDIA",Engineering / Transportation
Semi-Supervised Three-Dimensional Reconstruction with Generative Adversarial Networks [P21908],"Existing methods for 3D reconstruction often produce holes, distortions, and obscured parts in the reconstructed 3D models, or they can only reconstruct voxelized 3D models for simple isolated objects. So they are not adequate for real usage. This poster's focus is to achieve high-quality 3D reconstruction performance by adopting the generative adversarial network (GAN) approach. We'll propose a novel, semi-supervised 3D reconstruction framework (SS-3D-GAN), which can iteratively improve any raw 3D reconstruction models by training the GAN models to converge. This new model only takes real-time 2D observation images as the weak supervision and doesn't rely on prior knowledge of shape models or any referenced observations. Through the qualitative, quantitative experiments and analysis, SS-3D-GAN shows compelling advantages over the current state-of-the-art methods on the benchmark datasets. We'll also prove the acceleration effect of NVIDIA GPUs in the 3D reconstruction research and applications.","Chong Yu, Senior Architect, NVIDIA",Engineering / Transportation
Tensor Core Accelerated Sparse GEMM [P21866],"Sparse matrix multiplication is an important tool in many scientific applications, like graph analytics and deformable object modeling. It's also valuable in deep-learning networks with sparse inputs. We'll present a novel way to utilize the tensor cores of the latest NVIDIA GPUs to accelerate sparse matrix-matrix multiplication. Our approach splits the operand matrices into tiles and schedules them to tensor cores for fast matrix multiplication, outperforming two state-of-the-art libraries, CUSP and cuSPARSE, by an average of 2x.","Orestis Zachariadis, Ph.D. Candidate, University of Cordoba",Engineering / Transportation
Virtual Reality's Continued Impact in the World of AEC [s22129],"Not too long ago, it seemed that every enterprise company had an R&D team trying to decide if ""video game technology"" could be used for their visualization needs. The days of pilot projects have come and gone, and virtual reality is definitely here to stay. In this talk, Theia Interactive will introduce you to a variety of AEC customers using virtual reality and Unreal Engine on a daily basis to give them an edge in this new era of graphics.","Stephen Phillips, CTO, Theia Interactive",Engineering / Transportation
A Study of Pedestrian Protection CAE Using GAN [S21438],"According to the World Health Organization, there are over 270,000 pedestrians involved in traffic fatal accidents. That is 22% of all traffic fatalities verified in the world in 2013. To reduce pedestrian fatalities, third-party organizations in many countries have held New Car Assessment Program (NCAP) tests to evaluate pedestrian-protection performance. For that reason, an efficient method to design pedestrian-protection performance is required for automobile development for all over the world. Computer-aided engineering (CAE) is often used to verify pedestrian-protection performance. But as the necessity for pedestrian protection expands globally, expectations to improve efficiency have recently risen. We decided to reduce CAE verification time and improve the accuracy of CAE using deep learning. We also studied visualizing the basis for judgment in the trained models.","Osamu Ito, Assistant Chief Engineer, Honda R&D",Engineering / Transportation
Automating DNN Design for DRIVE AGX: Platform-Aware Neural Architecture Search [S21666],"In the past few years, wide applications of deep neural networks (DNN) have contributed to significant progress in various fields such as image classification, object detection, and segmentation. Most of the successful DNNs, such as VGG and ResNet, are designed by humans, which requires in-depth domain expertise and effort. While DNNs have become deeper and wider, the need for fast inference is increasing on (edge) computing devices, while accuracy must be maintained. Therefore, developing state-of-the-art neural networks for resource-constrained applications has become challenging. We'll present our progress on the automated design of neural networks using hardware-aware neural architecture search (NAS) techniques. We show concrete end-to-end examples from differentiable and latency-reflected search of optimal network architectures to their deployment on NVIDIA’s DRIVE AGX platforms using TensorRT for autonomous-driving-related applications.","Le An, Senior Deep Learning Software Engineer – Autonomous Vehicles, NVIDIA",Engineering / Transportation
Beyond-Line-of-Sight (BLOS) Perception System for Autonomous Vehicles [P21842],"Recently, autonomous vehicle perception systems show great performance based on deep-learning technology. Cameras and lidar are commonly used for autonomous driving. However, onboard sensors naturally have limited line-of-sight. Our work introduces a BLOS (beyond-line-of-sight) perception system that mutually complements communicated perception with V2X technology and local perception with onboard sensors. The proposed system is integrated into a full-scale vehicle, and we validated the performance and potential by experiments in the real world.","Chanyoung Chung, Researcher, KAIST (Korea Advanced Institute of Science and Technology)",Engineering / Transportation
Bike-Share Demand Prediction from Partial Future data Using Conditional Variational AutoEncoders [P21739],"Recently, bike-sharing services are working worldwide. One important aspect of bike-share management is to periodically rebalance the positions of the available bikes. Because the bike demand varies by and over time, the number of bikes at each bike-port tends to become unbalanced. To efficiently rebalance a bike-share system, it is essential to predict the number of bikes in each bike-port. In this poster, we propose a method to predict the bike demand at each bike-port every hour, up to 24 hours ahead. This method is based on Variational AutoEncoders and Sequence-to-Sequence Neural Networks. We called this method “Conditional Variational AutoEncoders considering Partial Future data”. In the experiment, our proposal method showed higher prediction accuracy than the other methods.","Mimura Tomohiro, Software Engineer, NTT DOCOMO, INC.",Engineering / Transportation
Bringing the Autodesk VRED Raytracer to the GPU [s21344],"Autodesk VRED helps designers and engineers create product presentations, design reviews, and virtual prototypes using interactive raytracing and analytic render modes. We'll show how we brought the VRED raytracer to the GPU using NVIDIA Optix. We'll also talk about specific requirements and challenges we faced, and demonstrate the results we achieved utilizing NVIDIA RTX.","Michael Nikelsky, Senior Principal Engineer, Autodesk",Engineering / Transportation
Coverage-Driven Verification for Ensuring AV and ADAS Safety [S22183],"One of the main evolutions the autonomous vehicle industry has seen over the last couple of years is that many companies have reached the point where they can make the fundamentals of AVs work just fine — sensing, planning routes, controlling the vehicle. The really hard part, though, is the validation and verification of all of the hazardous edge cases. We'll address these tough questions about how to enable sufficient verification to ensure AV safety:
How do you handle the infinite space of all the possible driving scenarios that need to be tested?
How can you control the various simulators and other execution platforms used for running the testing scenarios?
How can functional coverage help to systematically go over all the known and unknown hazardous edge cases?
How can you provide metrics that show that the remaining hazardous test cases result in a risk that is acceptable?","Ziv Binyamini, CEO, Foretellix",Engineering / Transportation
Deep Learning for 3D Vision (Point Clouds) [S21112],"Learning on 3D point clouds is vital for a broad range of emerging applications such as autonomous driving, robot perception, VR/AR, gaming, and security. Such needs have increased recently due to the prevalence of 3D sensors such as lidar, 3D cameras, and RGB-D depth sensors. Point clouds consist of thousands to millions of points and are complementary to the traditional 2D cameras in the vision (or multimedia) community. 3D learning algorithms on point cloud data are new, and exciting, for numerous core problems such as 3D classification, detection, semantic segmentation, and face recognition. The tutorial covers the 3D sensors, 3D representations, emerging applications, core problems, state-of-the-art learning algorithms (for example, voxel-based and point-based), and future research opportunities. We'll also showcase our leading work in several 3D benchmarks such as ScanNet, KITTI, etc., and efficient neural network training (with data parallelism) by NVIDIA GPU platforms (for example, DGX-1).","Winston Hsu, Professor, National Taiwan University",Engineering / Transportation
Drones as a Tool for Development and Safety Validation of Automated Driving Functions [P21913],"Automated driving heavily relies on data-driven methods. Large datasets of real-world measurement data, in the form of road-user trajectories, are crucial for several tasks, such as road-user prediction models or scenario-based safety validation. Using a drone has the major advantage of recording naturalistic behavior. Due to the ideal viewing angle, an entire scenario can be measured with significantly less occlusion than with sensors at ground level. Both the class and the trajectory of each road user can be extracted from the video recordings with high precision using state-of-the-art deep neural networks. Therefore, we have created a large-scale urban intersection dataset with naturalistic road-user behavior using camera-equipped drones. The resulting dataset contains road users including vehicles, bicyclists, and pedestrians at intersections in Germany, and is called inD. The dataset is available online for non-commercial research at: http://www.inD-dataset.com.","Julian Bock, Manager Artificial Intelligence, fka GmbH",Engineering / Transportation
Eliminating Hidden Bias in Autonomy and Beyond [S22701],"Speaker: Deepti Mahajan, Machine Learning Engineer, Ford Greenfield Lab We tend to trust that the systems that govern our day-to-day life—standards followed by industry, regulations passed by governments—are based on thorough research. However, in many cases, the data used to design these systems fails to represent us all equally and can even reinforce or amplify existing biases. As designers and engineers of the next generation of mobility, how can we learn to recognize hidden bias in the data we utilize and information we take for granted, and thus work towards creating systems that serve everyone?","Deepti Mahajan, Machine Learning Research Engineer, Ford Motor Company",Engineering / Transportation
"End-to-End Learning with Combined ""Real-World"" and ""Sim-World"" Datasets [P22060]","Training end-to-end learning for self driving requires large and diverse data. We can deal with that if we collect the data not just from the real world, but also simulations. Therefore, we built a customized map based on the real-world proving ground, KIAPI in Korea, using the Unreal Engine. By using the map in the open-source simulator, CARLA, we could collect both simulated and reality-based datasets of KIAPI in parallel. We integrated both datasets in several different ratios and evaluated their validity by training an end-to-end learning network with them.","Hyunki Seong, Master's Student, KAIST",Engineering / Transportation
Generating Diverse and Photorealistic Synthetic Data for Real-World Perception Tasks [S21321],"We'll cover these two related topics: First, leveraging generative adversarial networks for style transfer to diversify simulated images rendered in simple domains (that is, easier to render realistically, such as daytime) into photorealistic images in different weather and lighting conditions, using domain translation models (such as day-to-night, clear-to-rainy, clear-to-snowy) learned once from generic real-world datasets; and second, investigating the role of the discriminator’s receptive field in unsupervised sim-to-real image translation. We'll show that reducing the discriminator’s receptive field is directly proportional to improved structural coherence during translation in scenarios where the real and simulated images used for training have mismatched content — a situation often encountered in real-world deployment. Prior knowledge in computer vision and deep learning will help you get the most out of this session.","Nikita Jaipuria, Research Scientist, Computer Vision & Machine Learning, Ford Motor Company",Engineering / Transportation
Geolocation of Traffic Lights and Signs Using Dashcam: Toward Low-Cost Map Maintenance [P21912],"HD maps for autonomous driving need to be maintained as close to the current real world as possible, but it's quite costly and time-consuming to employ many workers or special mobile-mapping systems. To solve that, we're developing technologies and systems that extract information required for map maintenance from low-cost dashcam videos by utilizing rapidly advancing computer vision techniques. This poster introduces our current basic pipeline to detect and geolocate traffic lights and signs captured in dashcam videos. We developed our own dataset by collecting videos from vehicles running on various Japanese roads. Experimental evaluation using the dataset demonstrates that our system can detect objects with more than 85% accuracy, and can estimate their locations within less than 10 meters under the Global Navigation Satellite System.","Kazuyuki Miyazawa, AI Research Engineer, DeNA Co., Ltd.",Engineering / Transportation
GPU-Accelerated Data Pipeline and Machine Learning on DRIVE AGX using RAPIDS [S21665],"We'll present the extended capability of RAPIDS on the DRIVE AGX platform by demonstrating how it enhances the in-car user experience, with examples. ML algorithms are used extensively to address various challenges in autonomous cars. They include complex, multi-stage data science pipeline, sensor data processing, modeling, and analytics to accomplish new ML-based applications. Potential applications include recommender systems through driver or vehicle personalization, visual analytics of driving data, classification of driver condition, driving scenario, and more. In many cases, application workload runs on CPUs or ECUs, leading to performance bottlenecks as data size and their computes increase. Application workload can be parallelized, causing significant speedup, using GPUs. NVIDIA developed RAPIDS to accelerate entire end-to-end data science and analytics pipelines on GPUs. In this talk, we present extended capability of RAPIDS on NVIDA DRIVE AGX platform by demonstrating how RAPIDS works.","Andy Park, Senior System Software Engineer, NVIDIA",Engineering / Transportation
Improving Performance Limits of Obstacle-Avoidance Driving with Randomized Model Predictive Control Using GPU [P22242],"Our poster presents improvements in the performance of obstacle-avoidance driving enabled by a GPU-based controller. A 1:10-scale remote control car is driven in a 60-cm wide track with two parked cars acting as obstacles. The controller is model-predictive with nonlinear constraints. Since a sampling-based approach is used to solve this nonlinear optimization problem, computation speed becomes a constraint as sample size increases. We use parallel computing to solve the sampling-based optimizations. Experiments using our RC car showed that we could achieve >100% increase in driving speed and >40% improvement in cost-function values.","Hiroyuki Okuda, Assistant Professor, Nagoya Univeristy",Engineering / Transportation
Object Recognition and Tracking Utilizing Millimeter-Wave Radar by Deep Neural Networks [s21188],"All-weather sensors are necessary for autonomous-driving Level 3 and higher. Millimeter-wave radar is the most robust sensor for adverse weather. However, the signal is noisy and fluctuated, and the resolution is low. Thus, recognition using the radar is difficult. Deep-learning algorithms are an effective solution. We'll show a method to classify and track objects in driving scenes with a high-resolution millimeter-wave radar applying long short-term memory. We designed and compared various types of input features and LSTM for our measured dataset and achieved high accuracy through cross validation. We'll also show a method to reconstruct shapes of parking spaces and cars with convolutional neural networks. Parking cars were scanned with side radar. The reflection signals were accumulated, and the shape was estimated by semantic segmentation framework, applying CNN for the ground-truth shape, annotated by a lidar.","Tokihiko Akita, Project Research Fellow, Toyota Technological Institute",Engineering / Transportation
Optimizing TensorRt Conversion for Real-Time Inference On Autonomous Vehicles [S22198],"TensorRt optimizes neural-network computation for deployment on GPU, but not all operations are supported. Reduced precision inference speeds up computation, but can cause regressions in accuracy. We'll introduce Zoox TensorRt conversion pipeline that addresses these problems. TensorRt compatibility checks are involved at the early stages of neural-network training to ensure that incompatible ops are discovered before wasting time and resources on full-scale training. Inference accuracy checks can be invoked at each layer to identify operations not friendly to reduced-precision computation. Detailed profiling reveals unnecessary computations that aren't optimized inside TensorRt, but can be optimized by simple code changes during graph construction. With this pipeline, we've successfully provided TensorRt conversion support to neural networks performing various perception tasks on the Zoox autonomous driving platform.","Zejia Zheng, Software Engineer, Zoox",Engineering / Transportation
Pacefish: GPU-Accelerated CFD from Scratch to Market [S21926],"Gain insights into the challenge of developing GPU-accelerated computational fluid dynamics software, from concept to product. Pacefish transforms the power of up to 16 NVIDIA GPUs into a predicted physical flow behavior around complex geometries like cars, buildings, cities, or in rooms. This helps engineers design magnificent cars having better driving capabilities at lower fuel consumption and higher cruising range, for example. Remarkable GPU-to-CPU-node speedup on the order of 20-30x allows Pacefish to drop the costs per simulation by a factor of 10. We'll talk about milestones on the road from scratch to market that we started in 2007, and show some results emphasizing the current product state. You'll learn the lessons from our most important technological decisions from the initiator, founder, and main developer himself.","Eugen Riegel, Managing Partner, Numeric Systems GmbH",Engineering / Transportation
Panoptic Segmentation DNN for Autonomous Vehicles [S21879],"We'll present our NVIDIA DriveAV's Panoptic Segmentation Deep Neural Network (DNN), which can be used for semantic and instance segmentation of complex scenes for self-driving car scenarios, such as complex urban areas, congested traffic, construction zones with unusual activities, and so on. With Panoptic Segmentation DNN, input images can be accurately parsed for both semantic segmentation (which pixels represent which object class), as well as instance content (which pixels represent which object instance). Planning and control modules can use panoptic segmentation results to better inform autonomous driving decisions. We'll cover our highly accurate GT dataset, DNN architecture, our multi-task training process, and our real-time inference (that includes post-processing steps) on vehicles' AGX compute. Our network achieves state-of-the-art accuracy and runs at 7ms end-to-end on NVIDIA AGX GPUs. We'll show videos of our experiments on a real vehicle in various challenging conditions.","Ke Chen, Senior Deep Learning Scientist, NVIDIA",Engineering / Transportation
Precise Ultra HD Map Data as Basis for NVIDIA DRIVE Sim Virtual Testing and Simulation [S22273],"Digital road data is the basis for virtual testing and simulation. The roads used for virtual testing and simulation have to be identical digital twins of the real-world roads. 3D mapping presents the technical solution for digitizing test tracks, race tracks, and public roads with high-end mobile surveying using high-resolution scanners and multiple cameras. The technology is used to produce precise high-definition reference maps in OpenDRIVE format, which are either used as basis for virtual simulation and testing or as reference map in the car for autonomous driving development. We'll show various project examples that were completed as the basis for NVIDIA DRIVE Sim through 2019.","Dr.-Ing. Gunnar Gräfe, CEO, 3D Mapping Solutions GmbH",Engineering / Transportation
PredictionNet: Predicting the Future in Multi-Agent Environments for Autonomous Vehicle Applications [S21899],"Predicting the future trajectories of road agents is an import part of the planning&control stack in autonomous vehicles. Deep-learning approaches can be superior to classical methods in this domain, because neural networks can learn to use context and environment as a prior to improve prediction. We'll present PredictionNet — a deep neural network (DNN) that can be used for predicting future behavior/trajectories of road agents in autonomous-vehicle applications. Our DNN takes a rasterized top-down view of the world provided by the perception system and computes future predictions from past observations. We'll present its architecture, training-data collection process, and our training procedures. We'll also show video demos of live predictions on our self-driving car.","Alexey Kamenev, Principal Deep Learning Scientist, NVIDIA",Engineering / Transportation
PyTorch-TensorRT: Accelerating Inference in PyTorch with TensorRT [S21671],"TensorRT is a deep-learning inference optimizer and runtime to optimize networks for GPUs and the NVIDIA Deep Learning Accelerator (DLA). Typically, the procedure to optimize models with TensorRT is to first convert a trained model to an intermediary format, such as ONNX, and then parse the file with a TensorRT parser. This works well for networks using common architectures and common operators; however, with the rapid pace of model development, sometimes a DL framework like Tensorflow has ops that are not supported in TensorRT. One solution is to implement plugins for these ops. Another is to use a tool like TF-TRT, which will convert supportable subgraphs to TensorRT and use Tensorflow implementations for the rest. We'll demonstrate the same ability with PyTorch with our new tool PTH-TRT, as well leveraging the PyTorch API's great composability features to allow users to reuse their TensorRT-compatible networks in larger, more complex ones.","Josh Park, Manager - Automotive Solutions Architect , NVIDIA",Engineering / Transportation
RTX-accelerated Stellar GPU GI – Behind the Scenes of the SIGGRAPH 2019 Demo of the 3DS Global Illumination Renderer [s21315],"We'll give an overview of the technology behind the 3DS global illumination renderer demo, the challenges building the demo, the insights gained from it, and the features developed since. Stellar GPU GI is the GPU backend of the global illumination renderer developed by Dassault Systèmes. First launched in the 3DEXPERIENCE platform in early 2019, it enables rendering experts and novices alike, from design, simulation, and marketing, to generate interactive and offline high quality renderings. Its features and performance, especially in combination with RTX GPUs and RTX servers, were demonstrated to the world at SIGGRAPH 2019.","Jan Meseth, R&D Technology Stellar Physically Correct Director, Dassault Systemes",Engineering / Transportation
Scalable Storage Environments Optimized for Autonomous Driving (Presented by DDN) [S22671],"Continued advances in the types and number of sensors used by autonomous vehicle companies is leading to developments. It is also leading to data management challenges on a massive scale. The autonomous mobility industry is an entirely new ecosystem combining sensors and other physical components, security, high performance computing technologies, consumer electronics, mapping and geolocation services and a variety of standard IT solutions. With vehicles generating up to 80TB per day, connected car initiatives can demand exabytes of data on a daily basis. We will explain how intelligent, optimized data environments for AI and HPC can streamline and tame the data onslaught. Parallel data paths, diverse data services, remote data caching and tight integration with advanced computing platforms, like NVIDIA DGX systems combine to enable these most scalable solutions.","James Coomer, Senior VP Product Management and Marketing, DDN",Engineering / Transportation
"Sensor Processing with the NVIDIA DriveWorks SDK: Abstraction, Algorithms, and Acceleration [S21714]","Autonomous vehicles (AV) rely on sensors to represent the world around them, so onboard processing must react to rapidly changing environments. The NVIDIA DriveWorks SDK enables developers to implement such AV solutions by providing an exhaustive library of software modules and tools that leverage the computing power of the NVIDIA DRIVE AGX platform. With DriveWorks, developers can focus on their applications instead of spending time on fundamental functionality and infrastructure. Our session will cover the DriveWorks Sensor Abstraction Layer, a unified interface for sensor life-cycle management, timestamp synchronization, and recording. We’ll then discuss the DriveWorks optimized low-level image and point cloud processing modules for processing incoming sensor data to enable advanced AV algorithms. DriveWorks also supports the DRIVE AGX hardware engines so that these modules can seamlessly run across the Xavier SoC, providing flexibility and performance.","Hope Allen, Product Manager, Autonomous Vehicles, NVIDIA",Engineering / Transportation
Smart Cities in the Cloud: NVIDIA Metropolis on Red Hat OpenShift [S21994],"We'll talk about the collaboration between NVIDIA and Red Hat, and demo the integration between the NVIDIA's Metropolis application running on EGX and Red Hat OpenShift (Kubernetes) in the public cloud.","Sujit Biswas, Principal Engineer and Data Scientist, NVIDIA",Engineering / Transportation
Super SloMo: High-Quality Estimation of Multiple Intermediate Frames for Video Interpolation [P22420],We present a computational approach to synthesize slow-motion videos from a plain one.,"Huaizu Jiang, Ph.D. Student, University of Massachusetts, Amherst",Engineering / Transportation
Temporal Information Prediction for Perception in Autonomous Vehicles [P22335],"Self-driving cars, which have gained significant interest in academia and industry, have become a reality for several automotive companies. Temporal information in an autonomous vehicle, including time-to-collision and object 2D/3D motion, are essential input information for different autonomous vehicle system components, such as automatic cruise control and automatic emergency braking. It allows the autonomous vehicle to accurately understand its surrounding obstacles and provide the critical information to the control system. It can also be used to predict movement of the other objects and estimate their intents. We propose using a recurrent neural network for temporal information prediction. This increased robustness to the temporal prediction for non-rigid objects, such as pedestrians; extended the detection range for faraway objects; reduced noise in prediction; and utilized context pixels of the obstacles.","Cheng-Chieh Yang, System Software Engineer, NVIDIA",Engineering / Transportation
Terrain Traversability Estimation using Normal Distribution Transform on GPU in Amazon Scout [S21179],"Scout is a autonomous robot from Amazon for delivering packages. Mapping its surroundings in real time to identify traversable space is critical for autonomous driving. We'll discuss how we implemented a technique called ""3D Normal Distribution Transform"" (3D NDT) on NVIDIA TX2 to calculate surface traversability and help Scout navigate the neighborhood autonomously and safely. First, we'll introduce what 3D NDT is and what it is used for. Then we'll discuss how we implemented NDT in Amazon Scout to calculate surface traversability to help the robot to avoid obstacles and navigate safely. We'll highlight how we implemented NDT on NVIDIA TX2 GPU, achieving more than 10x faster run time than before, and how that greatly improved Scout's navigation. We'll also go into the challenges and innovation involved.","Ka Chen, Senior Graphics Engineer, Amazon",Engineering / Transportation
Toward Large-Scale Steady-State Computational Fluid Dynamics Acceleration with Point Cloud Networks [P22049],"Computational fluid dynamics (CFD) simulation has accelerated product development in the automotive, aerospace, and biomedical industries. Instead of constructing expensive prototypes, a design’s performance can be characterized in simulation. Unfortunately, simulation itself has become a new bottleneck. CFD solvers can take hours, days, or even months to solve the partial differential equations that characterize a design. Deep neural networks trained on CFD solutions can provide approximate solutions in seconds. Point cloud networks can process the unstructured meshes that define most realistic CFD problems. We evaluate one such network (PointNet) on a 2D 131k point cloud dataset. The network makes accurate approximations to the true solution more than 230x faster than a competing GPU solver. With this approach, engineers can evaluate designs more quickly to build a superior product.","Oliver Hennigh, Software Engineer, NVIDIA",Engineering / Transportation
Ultra-Fast Radar Simulation for Radar System Design and Automotive Applications [s21966],"Ultra-fast, physics-based radar simulations are required to design and deploy hardware-in-the-loop systems for applications such as autonomous vehicles and driver assistance systems (ADAS/AV). Rapid radar image generation is also an enabling technology for AI algorithms, vastly expanding the data sets to train and test the algorithms. We're developing an ultra-fast, end-to-end, GPU-accelerated radar image simulation engine for automotive, AI, and other applications. The shooting and bouncing rays (SBR) technique generates physics-based range-Doppler images of dynamic driving scenarios in urban settings. GPU kernels using CUDA, OptiX, and cuFFT propagate radar energy from the radar, through the scene, and back to the receiver to generate range-Doppler images displaying distance and relative velocity of surrounding objects. ANSYS will discuss the new solver and show results of automotive scenarios in busy, complex environments. The solver shows promise toward our goal of ultra-fast radar simulation.","Jeff Decker, Lead R&D Engineer, ANSYS",Engineering / Transportation
"Virtual Validation of Automated Vehicle Safety: Challenges, Lessons Learned, and Opportunities [S21658]","We'll explain the need for virtual validation (VV) of automated vehicles (AV), identify the general strengths and weaknesses of VV, briefly review the present landscape of tools and solutions available to perform AV simulations, and list the major building blocks of a good virtual testing process (VTP). We'll also discuss the potential of NVIDIA DRIVE Constellation toward implementing a meaningful VTP for developers, regulators, and service providers. We'll outline some key challenges and opportunities to motivate further research. We'll also introduce two research projects at CETRAN (Nanyang Technological University, Singapore): first, modeling the sensing and perception (S&P) errors using a Perception Error Model (PEM) and studying their impact on decision-making (AV behavior), and second, developing methods to evaluate the holistic fidelity of simulation toolchain and judge its effectiveness in representing reality. We'll present some preliminary findings and a research roadmap.","Jim Cherian, Research Fellow and Lead (AV Simulation/Virtual Validation), CETRAN (Center for Testing and Research on Autonomous Vehicles - NTU), Nanyang Technological University, Singapore",Engineering / Transportation
