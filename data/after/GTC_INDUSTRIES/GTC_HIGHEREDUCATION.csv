title,description,company,industry
3D Deep Learning in Function Space [s21764],"Recent advances in GPU technology and scalable algorithms have led to breakthroughs in deep learning. In particular, convolutional neural networks (CNNs) achieve state-of-the-art results in longstanding vision problems, such as image classification or object detection. However, autonomous agents that navigate and interact in our world need to reason in 3D. Unlike images in the 2D case, it is not clear how to represent 3D geometry and how to make it amenable for deep-learning techniques. We'll introduce our approach to learning 3D representations in function space. First, we'll show how this approach can represent arbitrary topologies without discretization at fixed memory cost. Then we'll extend this framework to learning to predict not only the shape of an object, but also its texture and motion. Finally, we'll show how we can scale our method to real-world scenarios using state-of-the art NVIDIA GPU technology.","Michael Niemeyer, Ph.D. Student, MPI-IS and University of Tübingen",Higher Education / Research
Accelerated Computing Teaching Kit for University Educators: Introduction and Use Cases [S22414],"As performance and functionality requirements for computing applications rise, industry demand for new graduates familiar with accelerated computing with GPUs grows. This session introduces the newest version of the Accelerated Computing Teaching Kit: a comprehensive set of academic labs, university teaching material, and e-book for use in introductory and advanced parallel programming courses. The teaching materials start with the basics and focus on programming GPUs, and include advanced topics such as optimization, advanced architectural enhancements, and integration of a variety of programming languages. We'll present a successful course-adoption case at Iowa State University — one of many worldwide — along with student feedback. The course at Iowa State covers an introduction to parallel computing using GPUs and its application to solid modeling and CAD. As part of the course outcomes, students were able to demonstrate the use of GPU-accelerated computing in their research by working on an individual course project that uses some of the techniques taught in the class. Finally, we'll discuss brand-new teaching kit modules covering CUDA 11, multi-GPU, and the latest libraries.","Joseph Bungo, DLI Program Manager, NVIDIA",Higher Education / Research
Accelerated Data Science in the Classroom: Teaching Analytics and Machine Learning with RAPIDS [S22417],"The demand for accelerated data-science skill sets among new graduate students grows rapidly as the computational demands for data analytics applications soar. This session introduces a novel yet reproducible approach to teaching data-science topics in a graduate data science course at the Georgia Institute of Technology, taught by Professor Polo Chau. Haekyu Park, a computer science Ph.D. student and teaching assistant of the course, will co-present key pedagogical considerations and solutions that help students learn GPU-accelerated data science and analytics using the open-source RAPIDS framework. For example, we present a hybrid, flexible approach for students to learn, where they can choose to experiment with RAPIDS using a local NVIDIA DGX-1 system, or the cloud, or both.","Polo Chau, Associate Professor, The Georgia Institute of Technology",Higher Education / Research
Accelerated Light-Transport Simulation using Neural Networks [S21852],"Neural network-based techniques have taken many fields by storm, but until recently have seen relatively little use in the field of physically-based rendering. This has begun to change. We'll present techniques for accelerating Monte Carlo integration of light transport without introducing bias by utilizing functions learned by neural networks for variance reduction. Our techniques yield on-par or higher performance than competing machine learning-based techniques at equal sample counts and generalize beyond physically-based rendering, being applicable to other high-dimensional integration problems such as Bayesian inference and reinforcement learning.","Thomas Mueller, Senior Research Scientist, NVIDIA",Higher Education / Research
Accelerating Large Seismic Simulation Code with PACC Framework [P21901],"The pipelined accelerator (PACC) helps lower the hurdle for implementing out-of-core stencil computation, such as large seismic simulations. However, the out-of-core applications themselves are facing data-movement bottlenecks because improvement of accelerators dwarfs that of interconnects. In this poster, we introduce temporal blocking techniques to reuse on-chip data, and propose a data-mapping scheme to eliminate data movement on the host side. The data-mapping scheme accelerates the program by 2.5x compared to previous work and by 35x compared to an OpenMP-based program. However, performance is still bound by data movement between the host and device, so we need to come up with further data-centric strategies. Moreover, the degradation in execution time of PACC code is about 25% compared to an in-core OpenACC code, which should be reduced if we can design further data-centric strategies.","Jingcheng Shen, Ph.D. Student, Osaka University",Higher Education / Research
ALFRED: Action Learning From Realistic Environments and Directives [P22310],"ALFRED is a benchmark for learning a mapping from natural language instructions and egocentric vision to sequences of actions for household tasks. Long composition rollouts with non-reversible state changes are among the phenomena we include to shrink the gap between research benchmarks and real-world applications. ALFRED consists of expert demonstrations in interactive visual environments for 25,000 natural language directives. These directives contain both high-level goals like ""Rinse off a mug and place it in the coffee maker"" and low-level language instructions like ""Walk to the coffee maker on the right."" ALFRED tasks are more complex in terms of sequence length, action space, and language than existing vision-and-language task datasets. We show that a baseline model designed for recent embodied vision-and-language tasks performs poorly on ALFRED, suggesting that there is significant room for developing innovative grounded visual language understanding models with this benchmark.","Daniel Gordon, Ph.D. Student, University of Washington",Higher Education / Research
Analytic Spherical Harmonic Gradients for Real-Time Rendering with Many Polygonal Area Lights [P22297],"We develop a novel analytic formula for the spatial gradients of the spherical harmonic coefficients for uniform polygonal area lights. The result is a significant generalization, involving the Reynolds transport theorem to reduce the problem to a boundary integral, for which we derive a new analytic formula, showing how to reduce a key term to an earlier recurrence for SH coefficients. The implementation requires only minor additions to existing code for SH coefficients. The results also hold implications for recent efforts on differentiable rendering. We show that SH gradients enable very sparse spatial sampling, followed by accurate Hermite interpolation. This enables scaling PRT to hundreds of area lights with minimal overhead and real-time frame rates. Moreover, the SH gradient formula is a new mathematical result that potentially enables many other graphics applications.","Lifan Wu, Ph.D. Student, University of California, San Diego",Higher Education / Research
An Improved Immersed Boundary-Lattice Boltzmann Method for Incompressible Fluid-Flow Simulations on GPU [P21931],"We introduce the immersed boundary and the lattice Boltzmann method for fluid-solid interaction. The advantage of the lattice Boltzmann method is that it can be easily parallelized. We use a computational study to demonstrate how the immersed body discretization affects the numerical results. We introduce a modification that improves stability, and compare the performance of both methods using several benchmarks problems. These problems show how the immersed body discretization affects the numerical results, mainly the overall drag force and the permeability of the discretized body boundary. Finally, we discuss a performance analysis of different solvers for linear systems.","Pavel Eichler, Ph.D. Student, Faculty of Nuclear Sciences and Physical Engineering, Czech Technical University, Prague",Higher Education / Research
A Reinforcement Learning Approach for Sequential Spatial Transformer Networks [s21941],"In this session, we discuss the applications of reinforcement learning (RL) for computer vision applications such as classification. We describe how we combine the idea of spatial transformer networks with RL to improve the classifiers' robustness against noise and clutter.","Fatemeh Azimi, Ph.D. Researcher, DFKI",Higher Education / Research
Astaroth: an API for Three-dimensional Stencil Computations on GPUs [S21332],"We'll present the Astaroth library that uses multiple GPUs efficiently in 3D stencil computations. Astaroth provides a C99 API, a domain-specific language, and a compiler for translating programs written in that domain-specific language into efficient CUDA kernels. We'll show that it is possible to achieve near hand-tuned performance while still being able to describe problems using a high-level language. Astaroth meets the demands in computational sciences, where large stencils are often used to attain sufficient accuracy. Maximizing the utilization of caches is challenging, especially in computational physics, where multiple simulated fields interact with each other and should be held in caches simultaneously. Astaroth takes inspiration from graphics and image-processing pipelines and generates a pipeline optimized for processing large 3D stencils.","Johannes Pekkila, Research Assistant, Aalto University School of Science",Higher Education / Research
Benanza: Automatic μBenchmark Generation to Compute ”Lower-bound” Latency and Inform Optimizations of Deep Learning Models on GPUs [P21858],"Current profiling tools lack the highly desired abilities to characterize ideal performance, identify sources of inefficiency, and quantify the benefits of potential optimizations. Such deficiencies have led to slow characterization/optimization cycles that can't keep up with the fast pace at which new DL models are introduced. We propose Benanza, a sustainable and extensible benchmarking and analysis design that speeds up the characterization/optimization cycle of DL models on GPUs.","Abdul Dakkak, Ph.D. Student, University of Illinois Urbana-Champaign",Higher Education / Research
Bringing AI to the Classroom: NVIDIA's Deep Learning Teaching Kit [S22357],"The call for AI and deep-learning skills is soaring, and university classrooms are on the front lines of feeding the demand. NVIDIA Teaching Kits lower the barrier of incorporating AI and GPU computing in coursework. NVIDIA’s higher-education leadership and Pawel Morkisz, assistant professor of mathematics at AGH University of Science and Technology in Krakow, will discuss the Deep Learning Teaching Kit, co-developed with Professor Yann LeCun and his team at New York University. The kit was a starting point for preparing materials for the course dedicated for postgraduate students of mathematics. Using NVIDIA’s materials saved many days of work preparing lecture slides and source-level coding projects/solutions.","Joseph Bungo, DLI Program Manager, NVIDIA",Higher Education / Research
Building MSOE’s GPU-Powered Infrastructure for AI Instruction and Research [s21423],"Last September, Milwaukee School of Engineering, with NVIDIA, deployed a new hybrid cluster to serve multiple AI and HPC computing demands including instruction, student and faculty research, and industry collaborations. We'll describe the optimized accelerated computing architecture of this cluster and the software stack enabling these uses. We'll explain the lessons learned through the design, build, and deploy processes. We'll provide a blueprint for other institutions that seek to build GPU-accelerated computing infrastructure that supports similar diverse requirements.","Bradley Palmer, Senior Solutions Architect, NVIDIA",Higher Education / Research
CapsNet-Lite: A Lightweight and High Performance CapsNet Architecture [P21854],"The capsule network (CapsNet) includes a new type of layer composed by ""capsules"". A capsule is a vector that stores in each position the amount of a certain object feature that's present in the image. Then, those capsules are combined to output the label of the object. To learn how to combine the capsules, this network also needs a routing algorithm. This type of network achieves very high accuracy in classification problems. We propose a modified CapsNet architecture called CapsNet-Lite that outperforms the original proposal in accuracy, with a much faster training procedure.","Francisco José García, Ph.D. Student, Universidad Rey Juan Carlos",Higher Education / Research
Capsule Networks for 3D Pose Estimation in Computer Graphics [P21924],"Pose estimation is an important task for novel engineering applications, such as virtual and augmented reality (VR/AR), pose-based video games, object reconstruction, target tracking, driving assistance, and recent sports analytics. Commonly, an efficient pose estimation system depends on the pose visualization given by a 3D configuration of location, orientation, and scaling parameters of the target. In this work, we implement Capsule Networks to solve 3D pose estimation in computer graphics of rigid objects using a multi-GPU architecture.","Kenia Picos, Professor, CETYS Universidad",Higher Education / Research
CheetahDB: A System for High-Throughput Database Processing on GPUs [P22073],"GPU database is an active topic in academic research and industrial practice. However, existing systems have not shown significant performance advantages over CPU-based in-memory database management systems (DBMS). Two main factors contributed to such difficulties: First, the CUDA programming model, by focusing on HPC-type workloads, requires non-trivial basic research to address the many technical challenges in developing a DBMS system software; and second, I/O bottleneck between host and GPU offsets the performance gain of onboard query processing. CheetahDB is a high-performance in-memory DBMS generated from NSF-supported research at the database group in the University of South Florida and commercialized by Cheetah Data Systems, Inc. CheetahDB addresses the above challenges via a complete rethinking of the software architecture of a DBMS under today’s multi-core hardware environment.","Chengcheng Mou, Student, University of South Florida",Higher Education / Research
Data Mining Pipeline for Predictive Synthesis of Advanced Materials [P22007],"Materials discovery is significantly facilitated and accelerated by high-throughput ab-initio computations. Being able to rapidly design advanced compounds has displaced the materials-innovation bottleneck to developing synthesis routes for the desired material. As there isn't a fundamental theory for materials synthesis, one might attempt a data-driven approach for predicting inorganic materials synthesis, but this is impeded by the lack of a comprehensive database of synthesis parameters. We've generated a dataset of “codified recipes” for solid-state synthesis automatically extracted from scientific publications. The dataset consists of about 20,000 synthesis entries retrieved from over 50,000 solid-state synthesis paragraphs by using text mining and natural language processing approaches. The dataset is publicly available and can be used for data mining of various aspects of inorganic materials synthesis.","Olga Kononova, Postdoctoral Scholar, University of California, Berkeley",Higher Education / Research
Decoding Texture Information from Rat Somatosensory Cortical Neurons Using CNN [P21855],"Neurons in the neocortex often exhibit feature selectivity against external stimuli. Simple stimuli can be decoded from activity of a single neuron; however, it has been poorly understood whether more complicated features can be decoded from the neuronal activity. To address this question, we recorded local field potentials (LFPs), which reflect the activity of thousands of neurons, of rats given complex sensory stimuli and took advantage of a convolutional neural network (CNN) for decoding. To this end, we developed a novel electrode array that enables wide-range recording from the superficial layers of the cortex. We recorded LFPs from the primary somatosensory cortex (S1) of a rat exploring on either rough or smooth floor. Our CNN model yielded 78% accuracy on average for decoding the surface texture. This suggests that somatosensory LFPs contain enough information to decode a surface texture.","Kotaro Yamashiro, Ph.D. Student, The University Tokyo, Graduate School of Pharmaceutical Sciences",Higher Education / Research
Deep Learning-Based Subcellular Phenotyping of Cell Edge Dynamics Reveals Fine-Grained Drug Responses [S21242],"We'll highlight how unsupervised deep learning can reveal subcellular drug responses hidden in the heterogeneity in live cell images. We'll present a feature-learning method for time-series data, which combines long short-term memory autoencoder and the prior information from traditional machine-learning analysis. We'll discuss how feature learning can be used to identify drug-related rare phenotypes and accelerate drug discovery. You need to have basic knowledge about autoencoder, clustering, microscopy, and cell biology.","Kwonmoo Lee, Assistant Professor, Worcester Polytechnic Institute",Higher Education / Research
Dive into Deep Learning [T22537],"Nowadays, deep learning is transforming the world. However, realizing deep learning presents unique challenges, because any single application brings together various disciplines. To fulfill the strong wishes of simpler but more practical deep learning materials, Dive into Deep Learning (https://d2l.ai/), a unified resource of deep learning, was born to achieve these goals: • offer depth theory and runnable code, showing readers how to solve problems in practice; • be complemented by a forum for interactive discussions of technical details and to answer questions; and • be freely available for everyone. We're going to provide an overview of the in-depth convolutional neural networks (CNN) theory and handy Python code. More importantly, you'll be able to train a simple CNN model on our pre-setup cloud-computing instances for free. Here are the detailed schedule and materials: https://github.com/goldmermaid/gtc2020","Rachel Hu, Applied Scientist, Amazon Web Services",Higher Education / Research
Efficient Simulations of Patient-Specific Electrical Heart Activity on the DGX-2 [P22031],"Patients who have suffered a heart attack have an elevated risk of developing arrhythmia. The use of computer simulations of the electrical activity in the hearts of these patients is emerging as an alternative to traditional, more invasive examinations performed by doctors today, and could provide not only safer but also more accurate results. One of the principal barriers to clinical use of such simulations is the tremendous amount of computational power they require. In this poster, we demonstrate a highly efficient code capable of running electrical heart activity simulations at 1/30 of real-time on the NVIDIA DGX-2, and we show that the achieved performance is close to optimal. Topics discussed include optimisations for sparse matrix-vector multiplications, strategies for handling inter-device communication for unstructured meshes, and lessons we learnt while programming the DGX-2.","Kristian Gregorius Hustad, Ph.D. Student, Simula Research Laboratory",Higher Education / Research
Exploring the Impact of Functional Package Manager Over a Non-Traditional High Performance Computing System [P21980],"Nowadays, academia and industry are using high performance computing in a search for better ways to improve the performance of the systems, and to make constructing and installing the programs less complex. Our project focuses on two problems: First, we analyze embedded systems capable of HPC as a way to improve energy consumption, and Nix as a tool to manage programs more easily. Then, we analyze whether using these systems is valid in an HPC context in terms of energy efficiency and performance.","Carlos Gómez, Master's Student in Computer Science, Universidad Industrial de Santander",Higher Education / Research
Few-Shot Adaptive Video-to-Video Synthesis [S21142],"Video-to-video synthesis (vid2vid) aims to convert an input semantic video, such as human poses or segmentation masks, to an output photorealistic video. However, existing approaches have limited generalization capability. For example, to generalize a trained human synthesis model to a new subject previously unseen in the training set requires collecting a dataset of the new subject, as well as retraining a new model. To address these limitations, we propose an adaptive vid2vid framework to synthesize previously unseen subjects or scenes by leveraging few example images of the target at test time. Our model achieves this few-shot generalization capability via a novel network weight-generation module utilizing an attention mechanism. We conduct extensive experimental validations with comparisons to strong baselines on different datasets.","Ting-Chun Wang, Senior Research Scientist, NVIDIA",Higher Education / Research
Fireiron: A Scheduling Language for High-Performance Linear Algebra on GPUs [P22298],"Our poster introduces Fireiron, a DSL and compiler that allows the specification of high-performance GPU implementations as compositions of simple and reusable building blocks. We show how to use Fireiron to optimize matrix multiplication implementations, achieving performance matching hand-coded CUDA kernels, even when using specialized hardware such as NIVIDA tensor cores, and outperforming state-of-the-art implementations provided by cuBLAS by more than 2x.","Bastian Hagedorn, Ph.D. Student, University of Münster",Higher Education / Research
GPU-Accelerated Next-Generation Sequencing Bioinformatic Pipeline [S21163],"Gold-standard next-generation sequencing (NGS) bioinformatic pipelines (BWA, PICARD, GATK, Samtools and variant annotators) utilize conventional CPUs. However, running pipelines for thousands of whole exome or whole genome sequencing (WES and WGS) samples requires several months, even on high performance computing CPU clusters. Graphical processing units have not been efficiently employed in pipeline acceleration, but have reduced computational runtime in biomedicine. Thus, we created a Kubernetes pipeline where we adapted and GPU-accelerated the code of key NGS software. We also tested the applicability and performance of GPU frameworks, and show through multiple experiments the extent of pipeline acceleration with varying combinations of multithreaded CPUs and GPUs. We show GPU’s effectiveness for bioinformatic analyses and how the mixed use of CPU and GPU will become the gold standard.","Margaret Linan, Computational Research Scientist, Icahn School of Medicine at Mount Sinai",Higher Education / Research
Heterocomputing and Transprecision Computing in Unstructured Low-Order Finite-Element Analyses on Volta GPUs [P21857],"We show the effect of heterocomputing and transprecision computing on performance using Volta GPUs. Here, we target low-order finite-element analysis. It's a core application in manufacturing, and attaining good performance on GPUs is challenging due to such bottlenecks as memory-bound computations and random memory accesses. We describe detailed algorithms to relieve these bottlenecks by utilizing heterogeneous computing and transprecision computing. As an application example, we conduct an earthquake city simulation.","Takuma Yamaguchi, Ph.D. Student, University of Tokyo",Higher Education / Research
High-Performance Deep-Learning Operators on NVIDIA GPUs via Multi-Dimensional Homomorphisms [P21790],"We present a holistic approach to CUDA code generation that provides performance, portability, and productivity for deep-learning operators (such as Matrix Multiplication and Convolutions) on NVIDIA GPUs. Our approach is based on our algebraic formalism of Multi-Dimensional Homomorphisms (MDHs). We show that important deep-learning operators can be conveniently expressed as MDHs, and that we can automatically generate CUDA code for MDHs that is specifically optimized for a particular GPU and input size. Our experimental results on real-world, deep-learning input data demonstrate that our automatically generated and optimized CUDA code achieves better performance than well-performing competitors — up to 2.91x better than NVIDIA’s hand-optimized libraries cuBLAS and cuBLASLt for deep-learning operator GEMM, and up to 3x better than NVIDIA’s cuDNN library and the popular deep-learning framework TVM for operator Convolution.","Ari Rasch, Ph.D. Student, University of Münster",Higher Education / Research
High Performance Distributed Deep Learning: A Beginner's Guide [S21546],"Learn the current wave of advances in AI and HPC technologies to improve the performance of deep neural network training on NVIDIA GPUs. We'll discuss many exciting challenges and opportunities for HPC and AI researchers. Several modern DL frameworks (Caffe, TensorFlow, CNTK, PyTorch, and others) that offer ease-of-use and flexibility to describe, train, and deploy various types of DNN architectures have emerged. We'll provide an overview of interesting trends in DL frameworks from an architectural/performance standpoint. Most DL frameworks have utilized a single GPU to accelerate the performance of DNN training/inference. However, approaches to parallelize training are being actively explored. We'll highlight new challenges for message-passing interface runtimes to efficiently support DNN training, and how efficient communication primitives in MVAPICH2-GDR can support scalable DNN training. Finally, we'll discuss how we scale training of ResNet-50 using TensorFlow to 1,536 GPUs for MVAPICH2-GDR.","Ammar Ahmad Awan, Graduate Research Assistant, Ohio State University",Higher Education / Research
Hybrid Molecular Mechanics: Artificial Intelligence Simulation Methods to Study Molecular Systems [S22090],"The advent of modern GPU architectures, and the development of molecular dynamics (MD) engines to exploit them efficiently, substantially impacted the performance and timescale achieved by MD. We'll present recent enhancements of MD capabilities by coupling to artificial intelligence methods. We'll present the interfaces enabling the coupling of the MD engine NAMD with the deep learning-based molecular descriptor Accurate NeurAl networK engINe (ANI). By using ANI to describe unparameterized molecules in the system — a drug bound to an enzyme, for example — instead of using a more expensive method such as quantum mechanical calculations, one can achieve performances of (nearly) classical MD simulations while maintaining quantum mechanical levels of accuracy. Moreover, the new NAMD platform lets you use AI-based enhanced sampling methods, such as reinforcement learning-based adaptive sampling, to achieve timescales prohibited by pure MD simulations.","David Hardy, Senior Research Programmer, University of Illinois at Urbana-Champaign",Higher Education / Research
Implementation of Artificial Intelligence/Deep Learning Disruption Predictor into a Plasma Control System [P22517],"As described in NATURE (April 2019), Princeton's AI/deep-Learning software uses convolutional and recurrent neural network components to integrate complex information from both spatial and temporal big data to predict dangerous disruptive events in magnetic fusion plasmas with unprecedented accuracy and speed on top supercomputers featuring NVIDIA'S powerful Volta GPUs. Here, we report on improved capability of the software to output not only the “disruption score,” as an indicator of the probable onset of a disruption, but also a “sensitivity score” in real time to indicate the underlying reasons for the imminent disruption. As an indicator of possible causes for future disruptions, the “sensitivity score” can provide valuable physics-based interpretability for the deep-learning model results, and more importantly, provide targeted guidance for the control actuators when implemented into any modern plasma control system. This is a significant step in moving from modern deep-learning disruption prediction to real-time control—a major advance that brings novel AI-enabled capabilities needed for the future burning plasma ITER system.","Ge Dong, Research Associate, Princeton Plasma Physics Laboratory",Higher Education / Research
Inference Path Optimization for Deep Reinforcement Learning on NVIDIA DGX-2 [P22339],"Our institution, the Shanghai Jiao Tong University High Performance Computing Center, is a school-level computing platform, supplying teachers and students in the whole school with computing power. In 2019, we bought eight DGX-2 for our new machine to support the school's research requirements. This poster presents one optimization case we did for our user to unleash the full power of the marvel machine. NVIDIA DGX is the most powerful computation system for artificial intelligence. However, there are still many challenges for a deep reinforcement learning system to take full advantage of the computing power of the DGX-2 platform, because of the limited PCI-e bandwidth, imbalance computation ability between CPU and GPU, and under-utilization of the GPU. We use Volta Multi-Process Service to increase GPU utilization. Introducing NUMA aware and workload balance further improves the computation efficiency. To alleviate the bottleneck of PCI-e bandwidth, we propose a residual compression method for states.","Jie Wang, AI Computing Specialist, Shanghai Jiao Tong University",Higher Education / Research
MAGMA: Accelerating Linear Algebra Through Mixed-Precision and Tensor Cores [S21557],"The MAGMA library provides several GPU-accelerated linear algebra algorithms. We'll cover the mixed-precision (MP) algorithms for solving different linear algebra problems, such as linear systems of equations (Ax=b). Classic MP algorithms use two precisions to accelerate the solution of systems in double or double-complex precisions. Thanks to the introduction of half-precision in NVIDIA GPUs and the incredible performance of tensor cores, dual-precision MP algorithms can now accelerate systems in single precision as well as single-complex precision. Triple-precision MP algorithms can now solve systems in double and double-complex precisions. We'll show how to accelerate complex precisions using “half-complex” linear algebra kernels, which are not natively supported by the tensor core units.","Ahmad Abdelfattah, Research Scientist, University of Tennessee",Higher Education / Research
MemEAPF for Mobile Robot Path Planning on Jetson Platform [P21986],"We present a parallel implementation on the NVIDIA Jetson TX2 of the membrane evolutionary artificial potential field (memEAPF) algorithm for mobile robot path planning. The memEAPF algorithm is employed to achieve feasible paths, considering minimum path length, safety, and smoothness. The memEAPF algorithm combines membrane computing with a genetic algorithm — specifically, a membrane-inspired evolutionary algorithm with a one-level membrane structure and the artificial potential field method to find the parameters to solve efficiently the robot path-planning problem. The path-planning problem in mobile robots is one of the most computationally intensive tasks, so heterogeneous computing helps to gain performance. Employing GPUs processes data-intensive tasks efficiently — in this work, the evaluation of solution paths.","Kenia Picos, Professor, CETYS Universidad",Higher Education / Research
Mobile Manipulation Toward Cleaning Dining Tables [P22364],"Service robots are robots designed to work in human environments. Human environments naturally imply unstructured environments, and hence require advanced computer-vision algorithms to reliably operate. Historically, various attempts have been made at developing general-purpose robots capable of performing human tasks in cluttered environments; however, most fall short due to problems in perception and task/motion planning. With the rise of modern RGB-D sensors, deep-learning algorithms, and GPU-based hardware for deep learning, the circumstances are ripe for a new foray into developing a general-purpose robot with artificial general intelligence. Leveraging on these technologies, we’ll develop a service robot for use in a fast-food center to clear and clean tables. We'll present our experiences and findings during its development.","Ka-Shing Chung, Ph.D. Student, National University of Singapore",Higher Education / Research
Multiphysics Software Development in the Age of AI [P21687],"We'll explain the main ideas of physics-informed neural networks (PINNs) in the context of an industrial multi-physics problem—namely, designing a heat sink for the next generation of NVIDIA DGX systems. PINNs leverage the underlying laws of physics, often described in the form of partial differential equations (PDEs), to solve forward, inverse/data-assimilation, and model discovery problems. Our work addresses several major drawbacks encountered with the traditional methods of solving PDEs in terms of usability (not requiring arduous meshing), speed (ability to solve multiple geometries simultaneously), scalability (being embarrassingly parallelizable across clusters of GPUs), and expertise (leveraging past experience). PINNs are also agnostic to the physics type (whether fluids, thermal, or solid) and are capable of composing and linking multiple physics representations directly from a description of the physical phenomena (that is, equations, initial/boundary conditions/data, and geometry).","Christopher Lamb, Vice President, Compute Software, NVIDIA",Higher Education / Research
N-Body Simulation of Binary Star Mass Transfer [P21883],"Observations suggest that over 50% of the stars in our galaxy are part of a multiple-star system. Of these, binary systems are the most intensely studied. Their abundance and unique characteristics make binary systems invaluable sources of astrophysical data. Our study's concerned with contact-binary systems — a pair of stars in physical contact sharing a common envelope. Due to mass transfer between the stars, the structure and evolution of these systems differ greatly from single-star systems. Here, we develop an N-body model that simulates evolving contact-binary star systems. With it, we study the evolution of contact binaries, in particular the role mass transfer between stars plays in this process.","Bryant Wyatt, Math Professor, Tarleton State University",Higher Education / Research
Overcoming Latency Barriers: Strong Scaling HPC Applications with NVSHMEM [s21673],"For scientific advancement through HPC, ever-increasing simulation capabilities are not the only key to success. Obtaining timely results is often even more important. Reducing the time-to-solution generally requires the application to be strong-scalable. However, scaling up improved single-GPU performance faces many obstacles. We'll show you how to improve the strong-scaling on systems equipped with NVIDIA GPUs. Avoid or hide latencies by exploiting GPU-centric communication with NVSHMEM, an implementation of OpenSHMEM for GPUs. After introducing NVSHMEM, we'll share best practices gathered from using NVSHMEM for QUDA, a library for Lattice QCD on GPUs used by codes as MILC and Chroma. We show results obtained on fat-GPU nodes like DGX-1/2, as well as scaling them to 1,000 GPUs in InfiniBand-connected systems, including Summit.","Mathias Wagner, Senior Developer Technology Engineer, NVIDIA",Higher Education / Research
Parallel Index-Based Search Operation for Database Systems via GPUs [P21987],"Handling a large number of queries concurrently is a crucial characteristic of today’s in-memory database system. By supporting such characteristics, index structure has a vital role in a database system. In recent years, GPUs have become the leading hardware for parallel computing. The unique architecture of high-performance computation, however, provides abundant opportunities for optimizing the algorithm toward better performance and achieving high utilization of GPU resources. We present our recent study in designing and optimizing parallel algorithms for index-search on GPUs. We also present techniques to optimize the search operation on both equality and range searches by using a novel clustering technique that can maximize the utilization of an on-chip GPU cache system. To evaluate our index structure, we compare the searching time with the best CPU SIMI index-based searching.","Napath Pitaksirianan, Ph.D. Candidate, University Of South Florida",Higher Education / Research
Rapid Pathogen Genomics using Nanopore Sequencing and GPUs [P22462],This project employs a combination of advanced genomics methods in the context of pathogen surveillance. The MinION device is used for nanopore or third-generation DNA sequencing. The MinION is small and portable and can be used to monitor microbial communities directly in the field. Recently released tools using Clara Genomics Analysis SDK enabled rapid genome analysis with GPU-enabled hardware. We envision that our methods will be further developed for field-forward applications of public health.,"Devin Drown, Assistant Professor, University of Alaska Fairbanks",Higher Education / Research
Rebalancing the Load: Profile-Guided Optimization of the NAMD Molecular Dynamics Program for Modern GPUs using Nsight Systems [s21547],"We'll show how we more than doubled the performance of the Nanoscale Molecular Dynamics program on modern GPUs through the use of profile-guided optimization with Nsight Systems and Nsight Compute. NAMD has historically offloaded most computation to GPUs, leaving a much smaller amount of work and device management to the CPU. However, the tremendous performance available on modern GPUs has led to an imbalance, in which the CPU work becomes a bottleneck while the GPU is left idling. We'll show how we improve GPU utilization and overall application performance by moving the remaining workload to the GPU while reducing CPU overhead due to communication and synchronization, all discovered with the help of Nsight Systems. We'll also discuss ongoing efforts in tuning the performance of our most compute-intensive kernels with Nsight Compute.","Julio Maia, Research Programmer, University Of Illinois at Urbana-Champaign",Higher Education / Research
Running Multi-Messenger Astrophysics with IceCube Across All Available GPUs in the Cloud [S22206],"We'll report on a computational experiment that marshaled all globally available for-sale NVIDIA GPUs across AWS, Azure, and GCP. The net result was a peak of about 51,000 GPUs of eight different kinds, with an aggregate peak of about 380 PFLOPS fp32. The experiment used simulations for the IceCube Neutrino Observatory, an array of some 5,000 optical sensors buried deep within a cubic kilometer of ice at the South Pole. The sensors detect the signatures of shock waves created by particles from neutrino interactions passing through the ancient ice sheets. Simulation is needed to properly account for natural ice imperfections, and photon propagation codes are a natural fit for GPGPU computing. We'll provide both a summary overview and technical details of the infrastructure needed to create a supercomputer-like environment across multiple cloud providers, as well as an overview of the science behind IceCube and how GPU compute helps in advancing the scientific goals.","Igor Sfiligoi, Lead Scientific Software Developer and Researcher, UC San Diego - San Diego Supercomputer Center",Higher Education / Research
Runtime Analysis of Spatial Structure: A CUDA Implementation of Minkowski Functionals [P21829],"Interested in characterizing spatial structure inherent in 3D scalar fields from simulated or imaged data? This poster presents an accelerated solution of the widely-used Minkowski functionals, using both OpenACC and CUDA on commodity GPUs. We'll present methods to minimize the memory footprint, and hence reduce data-transfer costs. Based on measurement frequency, an OpenACC rather than a CUDA solution might be appropriate. Next steps highlight the additional methods to further enhance and fine-tune the performance of the CUDA solution. Minkowski functionals have been widely applied in cosmology, material science, engineering, microbial ecology, and health care.","Ruth Falconer, Head of Division, Abertay University",Higher Education / Research
Scaling Deep Learning Interpretability by Visualizing Activation and Attribution Summarizations [S21808],"We'll present Summit, an interactive system that scalably and systematically summarizes and visualizes the features that a deep learning model has learned, and how those features interact to make predictions. Summit introduces two new scalable summarization techniques: activation aggregation discovers important neurons, and neuron-influence aggregation identifies relationships among such neurons. Summit combines these techniques to create the novel attribution graph that reveals and summarizes crucial neuron associations and substructures that contribute to a model’s outcomes. Summit scales to large data, such as the ImageNet dataset with 1.2 million images, and leverages neural network feature visualization and dataset examples to help users distill large, complex neural network models into compact, interactive visualizations. We'll present neural network exploration scenarios where Summit helps us discover multiple surprising insights into a prevalent, large-scale image classifier’s learned representations and informs future neural network architecture design. The Summit visualization runs in modern web browsers and is open sourced.","Polo Chau, Associate Professor, The Georgia Institute of Technology",Higher Education / Research
Self-Supervised Robot Learning from Pixels [s21921],"How can robots learn manipulation skills from raw sensory input without external supervision? We'll propose an algorithm that acquires such general-purpose skills by combining unsupervised representation learning and reinforcement learning of goal-conditioned policies. Since the particular goals aren't known in advance, the agent performs a self-supervised ""practice"" phase where it imagines goals and tries to achieve them. We learn a visual representation with three distinct purposes: sampling goals for self-supervised practice, providing a structured transformation of raw sensory inputs, and computing a reward signal for goal-reaching. A retroactive goal-relabeling scheme further improves the sample efficiency of our method. Our off-policy algorithm is efficient enough to learn policies that operate on raw image observations and goals for a real-world robotic system, and substantially outperforms prior techniques.","Ashvin Nair, Student, University of California, Berkeley",Higher Education / Research
Sim-to-Real: Virtual Guidance for Robot Navigation [P22224],"We present an effective, easy-to-implement, and low-cost modular framework for completing complex navigation tasks. Our proposed method is based on a single monocular camera to localize, plan, and navigate. A localization module in our framework first localizes and acquires the robot’s pose, which is then forwarded to our planner module to generate a global path and its intermediate waypoints. This information, along with the pose of the robot, is then reinterpreted by our framework to form the “virtual guide”, which serves as a virtual lure for enticing the robot to move toward a specific direction. We evaluate our framework on a Husky robot in a number of virtual and real-world environments, and validate that our framework is able to adapt to unfamiliar environments and demonstrate robustness to various environmental conditions.","Chun-Yi Lee, Associate Professor, National Tsing Hua University",Higher Education / Research
*Top 5 Poster Nominee - Photometric Mesh Optimization for Video-Aligned 3D Object Reconstruction [P22276],"We address the problem of 3D object mesh reconstruction from RGB videos. Our approach combines the best of multi-view geometric and data-driven methods for 3D reconstruction by optimizing object meshes for multi-view photometric consistency while constraining mesh deformations with a shape prior. We pose this as a piecewise image alignment problem for each mesh face projection. Our approach allows us to update shape parameters from the photometric error without any depth or mask information. Moreover, we show how to avoid a degeneracy of zero photometric gradients via rasterizing from a virtual viewpoint. We demonstrate 3D object mesh reconstruction results from both synthetic and real-world videos with our photometric mesh optimization, which is unachievable with either naive mesh generation networks or traditional pipelines of surface reconstruction without heavy manual post-processing.","Chen-Hsuan Lin, Ph.D. Student, Carnegie Mellon University",Higher Education / Research
Toward Detailed Organ-Scale Simulations in Cardiac Electrophysiology [P21982],"The poster presents previous and ongoing work in the area of cardiac electrophysiology. It introduces the problem and describes the previous state of the art, as well as the improvement obtained through the use of GPUs. Furthermore, it addresses the challenges of organ-scale simulations, including the fact that whole-heart simulation with realistic calcium handling constitutes an exascale problem.","Johannes Langguth, Research Scientist, Simula Research Laboratory",Higher Education / Research
Toward Optimal Implementation of Lattice Boltzmann CFD Simulator for Multi-GPU Clusters [P21922],"We present our progress toward a general and efficient multi-GPU solver for single-phase fluid flow based on the lattice Boltzmann method (LBM). LBM is suitable for parallelization on GPUs. However, the amount of global memory available on a single GPU is the limiting factor. Recently, we've extended our LBM solver using the message-passing interface library in order to utilize many GPUs available on HPC clusters. Our poster describes the optimization strategies used in the solver, analyzes the weak-scaling efficiency, and shows the results of preliminary high-resolution simulations for problems related to our research projects.","Jakub Klinkovsk�, Ph.D. Student, Department of Mathematics, Faculty of Nuclear Sciences and Physical Engineering, Czech Technical University, Prague",Higher Education / Research
Uniform Tasking on GPUs and CPUs: Task Queues Reloaded [P21938],"GPU programming with a difference! The poster covers our insights into the development and optimization of a tasking framework for GPUs. We provide an overview of our CUDA C++ tasking framework for fine-grained task parallelism on GPUs. After a short trip into the world of persistent threads, synchronization mechanisms, and load balancing, we dig a bit deeper by finding and analyzing performance bottlenecks. On that point, we present diverse optimization strategies for task queues. First, we describe the implementation of task queues based on static memory allocation. Second, we show how to implement work sharing on a GPU through hierarchical task queues. Third, we present a thread coordination scheme to reduce contention on the task queues, thus keeping all threads busy. Finally, we analyze the performance gains reached through the described optimizations.","Laura Morgenstern, Ph.D. Student, Computer Science, Jülich Supercomputing Centre",Higher Education / Research
Virtual GPU Computing for HPC: Improving System Utilization Through GPU Virtualization [S21827],"We'll discuss Clemson University's early experience of applying the NVIDIA vComputeServer in the high performance computing infrastructure to maximize their investment in GPU resources. We'll present two use cases: one for running the Metamoto simulation workload for the Open Connected Autonomous Vehicle project, and the other for general GPU-accelerated applications in a production HPC cluster.","Konstantin Cvetanov, Senior Solution Architect, NVIDIA",Higher Education / Research
