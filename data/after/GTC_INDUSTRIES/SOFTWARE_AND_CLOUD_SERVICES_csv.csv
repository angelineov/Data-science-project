title,description,company,industry
"Accelerate ML Lifecycle with Containers, Kubernetes and NVIDIA GPUs (Presented by Red Hat) [S22516]","Data scientists desire a self-service, cloud-like experience to easily access ML modeling tools, data, and GPUs to rapidly build, scale, reproduce, and share ML modeling results with peers and developers. Containers and kubernetes platforms, integrated with NVIDIA GPUs, provide these capabilities to accelerate the training, testing, and deploying the ML models in production quickly. We'll provide an overview of how these technologies are helping solve real-world customer challenges. We'll walk through the various customer use cases and solutions associated with the combination of these technologies. We'll review the key capabilities required in a containers and kubernetes platform to help data scientists easily use technologies (such as Jupyter Notebooks, ML frameworks, etc.) to innovate faster. Finally, we'll share the platform options (for example, Red Hat OpenShift, Kubeflow), and examples of how data scientists are accelerating ML initiatives with containers and Kubernetes.","Tushar Katarki, Product Manager, Red Hat",Software / Cloud Services
AWDF: An Adaptive Weighted Deep Fusion Architecture for Multi-Modality Learning [P21985],"Deep model fusion is getting lots of attention in dealing with multi-modality learning problems. We propose an Adaptive Weighted Deep Fusion scheme (AWDF) to capture potential relationships among various input sources. It integrates the feature-level and decision-level fusion in one framework. To address the limitations of existing fusing models with fixed weights, we also propose a new scheme named Cross Decision Weights Method (CDWM). It can dynamically learn the weight for each input branch during the fusion process instead of utilizing predefined weights. To evaluate the performance of AWDF, we conducted experiments on three different real-world datasets: Wild Business Terms, Iceberg Detection, and CareerCon. Our experiments demonstrate that AWDF outperforms other fusion systems by a large margin. (This work has been accepted by IEEE Big Data 2019 Special Session on Intelligent Data Mining).","Qinghan Xue, Applied Data Scientist, IBM",Software / Cloud Services
Container-Based Artificial Intelligence Applications Deployment Platform for GPU High Performance Computing Clusters [P21902],"A container-based AI applications deployment platform was proposed and implemented in our center to solve root privileges and network issues in HPC clusters. A virtual machine is used to simulate the targeted HPC cluster, which has the same or similar runtime environment including compilers, shared libraries, and GPU drivers based on a GPU card attached by PCI pass-through way. Users can have root privileges of virtual machines to set up containers. Further, a user can create containers and deploy AI applications by network tools, because each user has an exclusive virtual machine running on an independent server that connects to internet. The deploy platform and the HPC clusters are connected by a 10Gb/s, or even 100Gb/s network so that users can transfer the container images quickly and easily.","Rongqiang Cao, Associate Professor, Computer Network Information Center, Chinese Academy of Sciences",Software / Cloud Services
CTR Inference Optimization on GPU [s21416],"The CTR (click-through-rate) model is one of the most important models in internet businesses such as search, recommendation, and advertising. The performance of CTR online service has become a critical impact on user experience/enterprise revenue. With the development of deep learning technology, the CTR model began to adopt deeper and more complex DNN structure, and the computation scale and complexity continued to rise, which demanded more computing power. With the evolution of the CTR model in recent years and the promotion of NVIDIA GPU computing platform, more and more companies have begun to use NVIDIA GPU to accelerate the CTR online inference model, and achieved significant acceleration and got commercial benefits. We'll introduce how to profile and locate the issues when doing optimization CTR inference model on NVIDIA GPU, and provide the general methods on how to solve the issues and get satisfying speedup.","David Wu, Senior Software Engineer, NVIDIA",Software / Cloud Services
Deploying your Models to GPU with ONNX Runtime for Inferencing in Cloud and Edge Endpoints [s21379],"Models are mostly trained targeting high-powered data centers for deployment — not low-power, low-bandwidth, compute-constrained edge devices. There is a need to accelerate the execution of the ML algorithm with GPU to speed up performance. GPUs are used in the cloud, and now increasingly on the edge. And the number of edge devices that need ML model execution is exploding, with more than 5 billion total endpoints expected by 2022. ONNX Runtime is the inference engine for accelerating your ONNX models on GPU across cloud and edge. We'll discuss how to build your AI application using AML Notebooks and Visual Studio, use prebuild/custom containers, and, with ONNX Runtime, run the same application code across cloud GPU and edge devices like the Azure Stack Edge with T4 and low-powered device like Jetson Nano. We'll also demonstrate distribution strategies for those models, using hosted services like Azure IoT Edge. You'll take away an understanding of the various tradeoffs for moving ML to the edge, and how to optimize for a variety of specific scenarios.","Manash Goswami, Principal Program Manager, Microsoft",Software / Cloud Services
Edge Computing for Building Machine Learning Pipelines Using Azure Stack (Presented by Microsoft) [S22473],"Machine learning models need to be built closer to the source due to latency and data sovereignty requirements. Microsoft offers industry-leading hybrid cloud solutions, such as Azure Stack Hub and Azure Stack Edge in partnership with NVIDIA GPUs, to drive innovation and deliver a consistent cloud experience for these edge applications. Learn how to unlock customer-use cases by building production-scale models using data-science pipelines on the edge.","Kirtana Venkatraman, Program Manager 2, Microsoft",Software / Cloud Services
Faster Transformer [s21417],"Recently, models such as BERT and XLNet, which adopt a stack of transformer layers as key components, show breakthrough performance in various deep learning tasks. Consequently, the inference performance of the transformer layer greatly limits the possibility that such models can be adopted in online services. First, we'll show how Faster Transformer optimizes the inference computation of both the transformer encoder and decoder layers. In addition to optimizations on the standard transformer, we'll get into how to customize Faster Transformer to accelerate a pruned transformer encoder layer together with the CUTLASS library.","Bo Yang Hsueh, DevTech , NVIDIA",Software / Cloud Services
From Hours to Minutes: The Journey of Optimizing Mask-RCNN and BERT Using MXNet [S22483],"Training large deep learning models like Mask R-CNN and BERT takes lots of time and compute resources. Using MXNet, the Amazon Web Services deep learning framework team has been working with NVIDIA to optimize many different areas to cut the training time from hours to minutes.","Haibin Lin, Applied Scientist, AWS AI, Amazon",Software / Cloud Services
Google Cloud AutoML Video and Edge Deployment [S22022],"Google Cloud will be presenting AutoML Edge Video, a solution using NVIDIA GPUs. AutoML Edge Video allows users to train customized models without knowing how to tune parameters, using Google’s AutoML. In this talk we will focus on our end to end solution for Video Classification and Video Object tracking. To train a model, we use multiple NVIDIA GPUs in parallel for hyper-parameter tuning and transfer learning, allowing us to return high-performance models quickly. We will show how to export models to the edge using a frozen Tensorflow graph, and how to optimize for NVIDIA GPUs including Jetson and Tesla T4.","Yongzhe Wang, Software Engineer, Google",Software / Cloud Services
GradZip: Gradient Compression Using Alternating Matrix Factorization for Large-scale Deep Learning [P22289],"Our poster first explains the need for a dense and homomorphic algorithm for gradient compression in deep learning, in contrast to the conventional sparse-encoding-based approaches. Then, it details the thinking that came up with GradZip step by step, mainly modifying the existing matrix-factorization in the context of deep learning that reuses the latest all-reduce result as a part of alternating matrix-factorization. The final section captures the key performance result against the popular ResNet50 to demonstrate the effectiveness and efficiency of the proposed compression algorithm.","Minsik Cho, Program Director, IBM",Software / Cloud Services
Improve ML Training Performance with Amazon SageMaker Debugger (Presented by Amazon Web Services) [S22493],"During ML model training, it’s challenging to ensure that models are progressively learning the correct values for different parameters and to analyze and debug model characteristics without building additional tools, making the process time-consuming and cumbersome. With Amazon SageMaker Debugger, developers can get complete insights into the training process by automating data capture and analysis from training runs without code changes. We'll take a closer look at how you can define rules to monitor and analyze tensors and watch for issues in your model. By monitoring training flow, developers can improve GPU utilization, reduce troubleshooting time during training, and build high-quality models.","Shashank Prasanna, Sr. Developer Advocate, AI/ML, Amazon Web Services",Software / Cloud Services
Is the Label Trustworthy? Training Better Deep-Learning Models via Uncertainty Mining Net [P22055],"In this work, we consider a new problem of training deep neural networks on partially labeled data with label noise. To our best knowledge, there have been very few efforts to tackle such problems. We present a novel end-to-end deep generative pipeline for improving classifier performance when dealing with such data problems. We call it Uncertainty Mining Net (UMN). During the training stage, we utilize all the available data (labeled and unlabeled) to train the classifier via a semi-supervised generative framework. During training, UMN estimates the uncertainly of the labels to focus on clean data for learning. More precisely, UMN applies the sample-wise label uncertainty estimation scheme. Extensive experiments and comparisons against state-of-the-art methods on several popular benchmark datasets demonstrate that UMN can reduce the effects of label noise and significantly improve classifier performance.","Yang Sun, Applied Research Scientist, IBM",Software / Cloud Services
Manifold Regularization For Time Series Embedding [P22063],"The wide use of multivariate time-series data has also brought up challenges for humans to parse the long sequences, in its raw form, to identify patterns, create labeled data for supervised modeling approaches, summarize, and explain cause and effect. Variational AutoEncoder (VAE) is a class of deep-generative models that have been used to learn a concise representation of the input data while minimizing information loss. However, when dealing with time-series data these models do not take into account the continuity in the input time-series data. In this work, we use the VAE model along with manifold regularization on the embeddings to learn a smooth representation for the time-series data that can be used to effectively visualize different patterns.","Abhishek Kolagunda, Applied Deep Learning Scientist, IBM",Software / Cloud Services
Performance Optimization on Quantized Deep-Learning Models [S22484],"Quantization (8-bit) has been broadly adapted in computer vision-related deep-learning models for better inference performance. We present a set of techniques to speed up inference performance on a quantized model (8-bit). At graph level, we proposed a quantization-aware global layout transformation and graph optimization to minimize the data-layout transformation between 32-bit float and 8-bit integer. At the kernel level, we proposed an algorithm to fused the IM2COL with GEMM to save both the GPU memory usage and CUDA launch time caused by the process to generate IM2COL matrix. In addition, we proposed a double-buffering technique to improve the concurrency and reduce the data dependency. By comparing with cuDNN 7.1, our proposed method got up to 5x performance improvement.","Zhenyu Gu, Staff Engineer, Alibaba Group",Software / Cloud Services
PyTorch from Research to Production [S21928],"Learn how to get your neural network from the PyTorch framework into production. Explore ways to handle complex neural network architectures during deployment. We'll show how to transform a neural network developed in PyTorch into a model ready for a production environment and exemplify the workflow on a conversational AI system. For full understanding, you should be familiar with PyTorch framework and have some interest in model deployment for inference. We’ll demonstrate the neural network system on TensorRT Inference Server (TRTIS).","Grzegorz Karch, Engineer, ASIC, NVIDIA",Software / Cloud Services
qCUDA-ARM: Virtualization for Embedded GPU Architectures [P21851],"The emergence of the internet of things (IoT) is changing the ways to compute resource acquisition, from centralized cloud data centers to distributed pervasive edge nodes. We investigated two research trends to cope with the small amount of diversity problem for IoT devices and applications for the system design of edge nodes: heterogeneity and virtualization. Our poster presents the integration of those two important trends, and a virtualization system for embedded GPU architectures, called qCUDA-ARM. We evaluated the performance of qCUDA-ARM with three CUDA benchmarks and two-real world applications. For computationally intensive jobs, qCUDA-ARM can perform similar to the native system; for memory-bound programs, qCUDA-ARM can reach up to 90% of native performance.","Che-Rung Lee, Professor, National Tsing Hua University",Software / Cloud Services
Recommendation Systems Using Distributed Graph Convolutional Networks (GCN) on GPUs (Presented by Amazon Web Services) [S22491],"Deep learning so far has been mostly applied to simple and structured data, such as images, or sequences, such as time-series and language. Most of the information in the world is, however, non-Euclidean and complex and can be presented as graph. Molecules, social network relatedness, and relationships between products and consumers are among the more popular examples. We'll represent an intuitive theory of Graph Convolutional Networks and provide a walkthrough on an implementation for recommender systems using Deep Graph Library and Apache MXNet.","Cyrus Vahid, Principal Solutions Engineer, AI Platforms, Amazon Web Services",Software / Cloud Services
SPTAG and GPU Acceleration for Approximate Nearest Neighborhood Search [s21561],"The Approximate Nearest Neighborhood (ANN) search algorithm is essential to many machine-learning applications. For instance, vector similarity search, multimedia search, and duplicate entry search all employ ANN for handling very large datasets efficiently. There are two main approaches to implementing ANN: space partitioning trees and locality sensitive hashing. Although hashing methods are accelerated on the GPUs (RAPIDS and FAISS), to our knowledge there is no GPU-accelerated space partitioning ANN algorithm in the literature. We'll present the GPU acceleration of the SPTAG library, where both space partition tree and neighborhood graph construction are accelerated on GPUs. We'll discuss the data structures and algorithms developed for efficient GPU implementation. Finally, we'll discuss the performance characteristics and compare the execution times against a single-socket GPU.","Mingqin Li, Principal Software Engineer Manager",Software / Cloud Services
Visual Anomaly Detection using NVIDIA DeepStream with Azure IoT [S22675],"In this workshop, you'll discover how to build a solution that can process up to 8 real-time video streams with an AI model on a $100 device, how to remotely operate your device, and demonstrate how you can deploy custom AI models to it. With this solution, you can transform pixels from a camera into insights to know when there is an available parking spot, a missing product on a retail store shelf, an anomaly on a solar panel, a worker approaching a hazardous zone., etc. We'll build this solution using NVIDIA DeepStream on a NVIDIA Jetson Nano device connected to Azure via Azure IoT Edge. DeepStream is a highly-optimized video processing pipeline, capable of running deep neural networks. It is a must-have tool whenever you have complex video analytics requirements like real-time object detection or when employing cascading AI models. IoT Edge gives you the possibility to run this pipeline next to your cameras, where the video data is being generated, thus lowering your bandwidth costs and enabling scenarios with poor internet connectivity or privacy concerns. We'll operate this solution with an aesthetic UI provided by IoT Central and customize the objects detected in video streams using Custom Vision, a service that automatically generates computer vision AI models from pictures.","Paul DeCarlo, Senior Cloud Advocate, Microsoft",Software / Cloud Services
Wide and Deep Recommender Inference on GPU [S21559],"We'll discuss using GPUs to accelerate so-called ""wide and deep"" models in the recommendation inference setting. Machine learning-powered recommender systems permeate modern online platforms. Wide and deep models have become a popular choice for recommendation problems due to their high expressiveness compared to more traditional machine learning models, and the ease with which they can be trained and deployed using Tensorflow. We'll demonstrate simple APIs to convert trained canned Tensorflow estimators to TensorRT executable engines and deploy them for inference using NVIDIA’s TensorRT Inference Server. The generated TensorRT engines can also be configured to enable reduced-precision computation that leverages tensor cores in NVIDIA GPUs. Finally, we'll show how to integrate these served models into an optimized inference pipeline, exploiting shared request-level features across batches of items to minimize network traffic and fully leverage GPU acceleration.","Alec Gunny, Solutions Architect, NVIDIA",Software / Cloud Services
Accelerated Analytics Fit for Purpose: Scaling Out and Up (Presented by OmniSci) [S22556],"OmniSci has demonstrated the massive scaling possible using GPUs for computation and visualization. However, not every analytics problem or user persona requires massive scale; rather, our customers say they want proper-sized tools for the various problems they encounter across the enterprise. Our talk will outline the vision for scaling the OmniSci platform from trillions of records in a giant data store to hundreds of millions of records on a laptop, and every form factor in between. Whether you have a massive cluster of servers, a data science workstation, a GPU-enabled laptop, or even a CPU-only laptop, OmniSci can provide the same accelerated analytics experience appropriate for the problem at hand.","Venkat Krishnamurthy , VP Product, OmniSci",Software / Cloud Services
Accelerating AI Workflows with NGC [S22421],"AI has moved beyond research into mission-critical production. AI is now solving real-world problems for organizations around the globe, who are looking to move faster and do more with their data. NVIDIA provides a range of SDKs that simplify training, inference, and deployment of AI for industries including health care, smart cities, robotics, and telecommunications. We'll cover how NGC, through containers, pre-trained models, helm charts, and SDKs, allows data scientists and developers to build AI solutions faster, DevOps to streamline the development-to-production process, and IT teams to quickly provide compute platforms that the users need. We'll demo how you can take advantage of an SDK to easily build and deploy your AI solution on-premises, at the edge, or in the cloud.","Adel El Hallak, Director of Product Management for NGC, NVIDIA",Software / Cloud Services
Accelerating Applications for the NERSC Perlmutter Supercomputer Using OpenMP [s21387],"Learn about the NERSC/NVIDIA effort to support OpenMP target offload on the forthcoming NERSC-9 Perlmutter supercomputer with next-generation NVIDIA GPUs. NVIDIA's HPC compilers for C, C++, and Fortran will support a subset of OpenMP 4.5/5.0 for GPU offload on Perlmutter. We'll review the primary features included in this subset, walk through the reasons some features were included and others were left out, and highlight the characteristics that make some OpenMP applications better candidates for GPU acceleration than others. Selected teams from the NERSC Exascale Science Applications Program (NESAP) are porting their applications to NVIDIA GPUs using this subset of OpenMP for target offload. We'll share their early experiences and those of other NERSC developers working with early releases of NVIDIA's OpenMP-offload-enabled compilers.","Christopher Daley, HPC Performance Engineer, Lawrence Berkeley National Laboratory",Software / Cloud Services
Accelerating Chemistry Modules in Atmospheric Models Using GPUs [S22005],"We’ll explore a novel solution to speed up chemistry modules in atmospheric models. The Multiscale Online Nonhydrostatic AtmospheRe CHemistry model (MONARCH), a chemical weather prediction system developed by the Barcelona Supercomputing Center, is used as our test bed. The model implements a new flexible treatment for gas- and aerosol-phase chemical processes, Chemistry Across Multiple Phases (CAMP), that allows multiple chemical processes (such as gas- and aerosol-phase chemical reactions, emissions, deposition, photolysis, and mass-transfer) to be solved simultaneously as a single system. We’ll discuss innovative ways to speed up the CAMP module, including reducing memory accesses, multiple-cell chemistry solving, adaptation of the most time-consuming functions to GPUs, and heterogeneous computation approaches for CPU/GPU execution. We'll compare the optimized model to state-of-the art chemistry solvers commonly used in the earth sciences community (for example, EBI and KPP).","Christian Guzman-Ruiz, Junior Engineer, Barcelona Supercomputing Center",Software / Cloud Services
Accelerating Hyperparameter Tuning with Container-Level GPU Virtualization [S21463],"Many people think that hyperparameter tuning requires a large number of GPUs to get optimal results quickly. That's generally true, but to what extent? We'll present what we've learned about finding a sweet spot to balance both costs and accuracy by exploiting partitioned GPUs with Backend.AI's container-level GPU virtualization. Our benchmark includes distributed mnist, cifar10 transfer learning and TGS salt identification cases using AutoML with network morphism and ENAS tuner with NNI running on NGC-optimized containers on Backend.AI. You'll get a tangible guide to plan and deploy your GPU infrastructure capacity in a more cost-effective way.","Jeongkyu Shin, CEO/Researcher, Lablup Inc.",Software / Cloud Services
Acceleration of Test Data Quality Assurance Technology Using Neuron Coverage [P21809],"“Neuron coverage test” is one way to test quality in deep-learning systems. We've implemented “DeepXplore,” which is a quality test method for DL systems that applies our unique high-speed inference library. DeepXplore is a framework that can systematically test DL systems and automatically generate misrecognition images from the coverage rate of the trained model and the output difference of the comparison model. However, a large amount of inference would be necessary to automatically generate these misrecognition images, and that could require a lot of processing time. We sped up processing by using our unique high-speed inference library to implement DeepXplore.","Yuki Shindo, Staff, Computermind Corp.",Software / Cloud Services
AceCAST GPU-Enabled Weather Research and Forecasting Model Development and Applications [P22064],"The Weather Research and Forecasting (WRF) model is an open-source, mesoscale numerical weather prediction system designed to serve both operational forecasting and atmospheric research needs. It is the most widely used regional weather forecasting model and a top 5 HPC application worldwide. TQI has implemented an OpenACC/CUDA-based version of WRF to take advantage of NVIDIA GPUs. By utilizing GPUs, the measured performance benefits enable better forecasting through higher resolution, temporal/geographical extents, and so on. Our poster discusses the GPU implementation of the model as well as performance benchmarks demonstrating the model’s practical performance benefits. TQI has also developed a cloud-based solution for running end-to-end AceCAST GPU-WRF workflows on AWS. This provides a solution for a wide range of users who would otherwise not have access to GPU-based compute resources, and automates a highly complex process that is a significant barrier for researchers and operational weather forecasters.","Samuel Elliott, Director of NWP Solutions, TempoQuest Inc.",Software / Cloud Services
Advanced Scientific Visualization with NVIDIA Omniverse [S21973],"Historically, scientific visualization has served two main purposes: on one hand, for the scientist to gain insight into the data and the processes under investigation; on the other hand, for outreach and education, to communicate scientific results to the public and to funding bodies. The tools associated with these workflows are hugely diverse, often preventing scientists from getting access to the highest quality visuals for telling their stories. Especially in times where the general public has been spoiled with computer-generated content in movies, the gap in visual quality between scientific content and entertainment content has never been bigger. Tools that seamlessly fit into the scientific workflow while being capable of producing the highest quality visuals are therefore needed. We'll outline how Omniverse, NVIDIA’s collaboration platform for 3D content creation, can be leveraged to greatly simplify, accelerate, and enhance scientific visualization. We'll introduce the overall Omniverse architecture and then focus on the scientific workflow. We cover how content can be ingested into Omniverse, as well as how content from different sources can be fused and how to produce rich, high-quality visualizations. We'll then show how to augment these visualizations with the embedded real-time physics engine PhysX. Further, we'll demonstrate how, through Omniverse, virtual reality setups that incorporate scientific visualizations can be defined quickly and easily.","Kees van Kooten, Scientific Visualization Software Engineer, NVIDIA",Software / Cloud Services
Advances in Real-Time 3D Hologram Generation [S21248],"We'll introduce VividQ's software pipeline, which facilitates the extraction of 3D point cloud data and conversion to a hologram to be displayed using diffractive optics. This allows for the real-time projection of scenes as truly 3D holographic images. For example, video games can now be viewed in true 3D with full depth of focus. We'll explain why this problem is computationally hard, and how using NVIDIA GPUs allows us to overcome it. Recent advances in hologram generation procedures will be explored, including benefits from newer GPU models and the application of multiple GPUs to computing for a binocular holographic system. We'll also discuss results from VividQ's latest holographic augmented reality head-mounted display prototype.","Tom Durrant, Chief Development Officer, VividQ",Software / Cloud Services
A Faster Radix Sort Implementation [S21572],"We'll present a faster implementation of the least-significant-digit radix sort. Decoupled look-back is used to reduce the number of memory operations per partition pass from 3N to 2N. A faster partition implementation inside the thread block is used. For 32-bit keys, we use four partition passes, with 8 bits per digit. On V100 sorting 64 million random UInt32 keys, our implementation achieves the speed of 16 Gkey/s, which is more than 2x faster than cub::DeviceRadixSort.","Andrey Adinets, AI Devtech, NVIDIA",Software / Cloud Services
AIaaS: Scaling AI Infrastructure for the Enterprise from DGX-1 to SuperPOD [S21996],"Are you getting the most out of your GPU-accelerated hardware? AIaaS (AI-as-a-Service) deployments provide powerful new ways for enterprises to stand up AI infrastructure on top of GPU-accelerated servers (such as NVIDIA DGX Servers). Data scientist users can interact with the cluster via GUI or a simplified command-line interface, where knowledge of underlying container, orchestration, and scheduling technologies is obfuscated. Such systems allow administrators to maximize use of resources and users to get more work done. We'll focus on the variety of AIaaS stacks, considerations, and how to deploy them. We'll demonstrate scaling common AI workloads from a single GPU on an NVIDIA DGX-1 to a SuperPOD cluster (64x DGX-2). We'll also discuss methods to integrate storage and manage datasets, tie in authentication and authorize users, train on NGC containers, and deploy models to production.","Michael Balint, Senior Product Manager, NVIDIA",Software / Cloud Services
AI Argus: A Unique Insight Into Logistics [P21812],"As of August 2019, AI Argus, the leading domestic intelligent video analytics platform powered by NVIDIA TeslaT4 and Xavier servers, has been deployed and applied in more than 200 distribution centers and sorting centers in China. We'll focus on algorithms such as loading-rate detection and violated-action pattern detection. AI Argus introduced the Package Lifecycle Tracking System (PLTS), which uses the whole SF's 310,000-channel camera video data to match the Operator's barcode data collector for locating the courier package on each operating node. It shows that the detection of damage caused by penetrating damage, moisture, wrinkles/pressure is significant. We'll further develop and research product performance indicators based on T4 and Xavier. We expect that in the coming year, AI Argus will deploy thousands of edge servers and become the first truly large-scale edge computing platform in China's logistics industry.","Neo Song, Chief Engineer, SF Techology",Software / Cloud Services
AI/ML with vGPU on Openstack or RHV Using Kubernetes [S22106],"By sharing GPU resources for AI/ML, you can better utilize on-premises hardware and gain flexibility without moving sensitive workflows into the cloud. Learn how Red Hat Openstack Platform and Red Hat Virtualization are bringing agility to AI/ML accelerated workloads with vGPU. We'll describe how Red Hat contributes to new vGPU capabilities like SR-IOV and live migration support. This makes setting up AI/ML within a Kubernetes system even simpler and more reliable, as the workloads can be migrated and SR-IOV functionality boosts the performance and usability of the vGPU device.","Erwan Gallen, Senior Principal Product Manager, Red Hat",Software / Cloud Services
An Overview of GPU Recurrent Neural Network Performance [S21301],"We'll focus on performance of recurrent neural networks (RNNs) at the sub-framework level. There are several distinct methods one can use to implement an RNN, each with advantages and disadvantages, and the right choice will depend on both the target GPU and the network hyperparameters. We'll cover these methods in detail and give approximate hyper-parameter ranges for each method.","Jeremy Appleyard, AI DevTech, NVIDIA",Software / Cloud Services
AutoFAQ: Automation of Customer Support for Most Common Questions [s21252],"In most support systems, up to 70% of user questions are very similar to each other. Instead of manually answering each of them, it makes sense to automate it. We'll discuss the business value of such systems, and how to integrate them into processes; how to develop an architecture for automating question-answering; how to set up a training loop, and which algorithms to choose; which models need to be trained, and why; and state-of-the-art language modeling models. Knowledge of Python, NLP, language models, and cloud computing will be helpful, but it's not essential.","Vitaly Davydov, CEO, Poteha Labs",Software / Cloud Services
Building a Smart Language-Understanding System for Conversational AI with HuggingFace Transformers [S22647],"In this session, HuggingFace showcases an example of a natural language understanding pipeline to create an understanding of sentences, which can then be used to craft a simple rule-based system for conversation. They'll leverage the famous HuggingFace transformers and showcase the powerful yet customizable methods to implement tasks such as sequence classification, named-entity recognition, natural language generation, or question answering. These tasks will be joined to create a basic NLU pipeline to get the most out of a sentence or text, using transformer models such as BERT to provide state-of-the-art results. These methods leverage the PyTorch or TensorFlow numerical computation frameworks, which can leverage the power of GPUs to radically speed up the inference.","Lysandre Debut, Machine Learning Engineer, Hugging Face, Inc.",Software / Cloud Services
CLOUDXR: Streaming AR and VR [S22178],"Learn how the NVIDIA CloudXR SDK can help you drive immersive extended reality (XR) experiences from anywhere using NVIDIA Quadro GPUs. NVIDIA CloudXR is the groundbreaking technology that delivers wireless virtual and augmented reality from NVIDIA RTX GPUs across performant networks. By dynamically adjusting to network conditions, CloudXR maximizes image quality and frame rates. Scale XR capabilities throughout your enterprise by combining CloudXR with NVIDIA GPU virtualization software to provide seamless experiences comparable to the most robust tethered configurations. Included in this presentation will be examples of partner ISVs extending the OpenVR applications into XR streaming applications.","Greg Jones, Senior Manager, Global Business Development for Enterprise XR, NVIDIA",Software / Cloud Services
Combined Python/CUDA JIT for Flexible Acceleration in RAPIDS [S21393],"We'll introduce our design and implementation of a framework within RAPIDS/cuDF that enables compiling Python user-defined functions and inlining them into native CUDA kernels. Our framework uses the Numba Python compiler and Jitify CUDA just-in-time (JIT) compilation library to provide cuDF users the flexibility of Python with the performance of CUDA as a compiled language. An essential part of the framework is a parser that parses the CUDA PTX function, which is compiled from the Python UDF, into an equivalent CUDA device function that can be inlined into native CUDA C++ kernels. Learn how our approach makes it possible for non-expert Python users to extend optimized dataframe operations with their own Python UDFs, and enables more flexibility and generality for high-performance computations on dataframes in RAPIDS.","Jiqun Tu, NVIDIA",Software / Cloud Services
CuPy Overview: NumPy Syntax Computation with Advanced CUDA Features [S22471],"We'll introduce CuPy, describing the advantages of the library and how it is cleanly exposing in Python multiple CUDA state-of-the art libraries such as cuTENSOR or cuDNN. Discover the CuPy advantages and how they can use it to experience performance gains in their NumPy codes without any major changes.","Crissman Loomis, Engineer/Business Development, Preferred Networks",Software / Cloud Services
Distributed Deep Learning with Horovod on Spark [s21300],"We'll show how to scale distributed training of TensorFlow, PyTorch, and MXNet models with Horovod, a library designed to make distributed training fast and easy to use. We'll explain the role of Horovod in taking a model designed on a single GPU and training it on a cluster of GPU servers with just a few additional lines of Python code. We'll also explore how Horovod has been used across the industry to scale training to hundreds of GPUs, and the techniques that are used to maximize training performance. Although frameworks like TensorFlow and PyTorch simplify the design and training of deep learning models, difficulties usually arise when scaling models to multiple GPUs in a server or multiple servers in a cluster.","Travis Addair, Senior Software Engineer, Uber Technologies",Software / Cloud Services
Document Understanding Platform: Extracting Structured Information from Financial Documents [S21459],"In today’s highly automated world of financial services, consumers, the self-employed, and small business owners still face the tedious and time-consuming task of entering data manually from paper documents. Document Understanding is a company-wide initiative at Intuit that aims to make data preparation and entry obsolete through the application of computer vision and machine learning. We'll describe the design and modeling methodologies used to build this platform-as-a-service. Intuit’s Document Understanding Platform orchestrates a variety of services and machine-learning capabilities using structured and unstructured documents uploaded by users, regardless of format (smartphone photos, PDFs, forms, etc.), and presents high-confidence results back within the company’s product ecosystem.","Joy Rimchala, Data Scientist, Intuit",Software / Cloud Services
Efficient 3D Convolutional Network Design for Human Instance-Level Video Action Recognition [P22275],"We discuss the design factors of the human instance-level video action recognition, especially on edge computing hardware like a Jetson AGX Xavier embedded module. First, we present our research motivation in human action recognition fields. Second, we talk about our pipeline for human instance-level video action recognition. Third, we explain how to enhance the accuracy using person detectors such as YOLOv3 and Mask RCNN. Finally, we discuss how to utilize action recognition models in terms of accuracy to action inference speed ratio on the Xavier module, which is very useful in practice.","Inwoong Lee, Researcher, Artificial Intelligence Research Institute (AIRI)",Software / Cloud Services
Espresso: A Fast End-to-End Neural Speech Recognition Toolkit [s21239],"We'll introduce Espresso, an open-source, modular, extensible, end-to-end neural automatic speech recognition (ASR) toolkit based on the deep learning library PyTorch and the popular neural machine translation toolkit fairseq. Espresso supports distributed training across GPUs and computing nodes, and features various decoding approaches commonly employed in ASR, including look-ahead word-based language model fusion, for which a fast, parallelized decoder is implemented. Espresso achieves state-of-the-art ASR performance on the WSJ, LibriSpeech, and Switchboard datasets, among other end-to-end systems without data augmentation, and is up to 11x faster for decoding than similar systems, such as ESPnet.","Yiming Wang, Ph.D. Student, Johns Hopkins University",Software / Cloud Services
Evaluation of a Multi-GPU Optimized Non-Hydrostatic Ocean Model with Multigrid Preconditioned Conjugate Gradient Method [P21930],"The conjugate gradient method with multigrid preconditioners (MGCG) is used in scientific applications because of its high performance and scalability with many computational nodes. GPUs are thought to be good candidates for accelerating such applications, with many meshes where an MGCG solver could show high performance. No previous studies have evaluated and discussed the numerical character of an MGCG solver on GPUs. Consequently, we implemented and optimized our “kinaco” numerical ocean model with an MGCG solver on GPUs. We evaluated its performance and discussed inter-GPU communications on a coarse grid on which GPUs could be intrinsically problematic. We achieved 3.9x speedup compared to CPUs, and learned how inter-GPU communications depend on the number of GPUs and the aggregation level of information in a multigrid method.","Takateru Yamagishi, Chief, RIST",Software / Cloud Services
EXtended Particle System (XPS): High-Performance Particle Simulation [P21775],"We'll present a framework for a GPU-based discrete element method solver for real-world particle problems featuring coupling ability with a CPU-based computational fluid dynamics solver provided by one of our partners. Current features include support of non-spherical shapes like multi-sphere particles and true bi-convex tablets, plus heat transfer and liquid transfer. We recently integrated a smoothed-particle hydrodynamics solver, as well as support for polyhedral-shaped solid particles.","Hermann Kureck, Scientist, RCPE GmbH",Software / Cloud Services
Exterminating Buffer Overflows and Other Embarrassing Vulnerabilities with SPARK Ada on Tegra [S21122],"Since 2018, NVIDIA has been actively investigating the SPARK Ada programming language to develop their most sensitive pieces of firmware. We'll explain how users of the NVIDIA hardware can also benefit from this language choice when developing applications for the Tegra SoC. The benefits of the technology, from a cyber security point of view, will be demonstrated through the use of formal methods, allowing trivial proof of properties, such as absence of buffer overflows. We'll describe using this technology on top of ARM processor cores, as well as methodologies for applications leveraging the GPU, either through existing libraries interfaces/CUDA code, or through an experiential port of Ada/OpenACC, which allows applications directly written in Ada or SPARK to offload to the GPU.","Quentin Ochem, Lead of Business Development and Technical Account Management, AdaCore",Software / Cloud Services
Fast Distributed Joins with RAPIDS and UCX [s21482],"There are numerous optimized single-GPU join implementations (such as RAPIDS cuDF), but scaling out to multiple GPUs across multiple nodes is challenging. The repartitioned join approach is one of the most popular distributed join algorithms, featuring all-to-all exchange as the main communication pattern. We'll show how to leverage UCX for efficient all-to-all implementation and demonstrate various optimization strategies, such as reusing communication buffers to speed up GPU-to-GPU transfers and overlapping compute with communications. The implementation is designed to reuse RAPIDS components for single-GPU, and scales to NVLINK and Infiniband systems. Our latest performance results demonstrate that a single DGX-2 can achieve 220 GB/s throughput for joining 8B/8B key-value pairs, while 18 DGX-1V nodes (144 GPUs) connected over IB achieve 503 GB/s, which is comparable with 244 CPU nodes (2K cores) in the best-known distributed CPU implementation.","Nikolay Sakharnykh, Developer Technology Engineer, NVIDIA",Software / Cloud Services
From High to Low Level: A Comparative Study of Programming Approaches for NVIDIA GPUs [S21308],"Learn about various available methods to program NVIDIA GPUs, from using high-level GPU libraries in Python to optimized CUDA C programming. We'll discuss the development and performance of several implementations of software to simulate the 2D Ising model for spin systems, comparing the different programming approaches in terms of development effort and simulation performance. We'll show how Python, in combination with the Numba/CuPy packages, enables users to write programs, with all the productivity benefits of a high-level language, that can still provide competitive performance to lower-level implementations. We'll also highlight some performance pitfalls encountered with these tools and discuss how we addressed them. Finally, we'll compare performance against published results on other hardware platforms and show that even simple programming methods on GPUs can provide competitive performance, while our optimized low-level implementation can rapidly simulate lattices sizes outside the scope of comparable field-programmable gate array solutions.","Joshua Romero, Developer Technology Engineer, NVIDIA",Software / Cloud Services
GPipe: Efficient Training of Giant Neural Networks Using Pipeline Parallelism [S21873],"Scaling up deep neural network capacity is an effective way to improve model quality for several different machine learning tasks. In many cases, increasing model capacity beyond the memory limit of a single accelerator has required developing special algorithms or infrastructure. These solutions are often architecture-specific, and do not transfer to other tasks. To address the need for efficient and task-independent model parallelism, we'll introduce GPipe, a pipeline parallelism library that allows scaling any network that can be expressed as a sequence of layers. By pipelining different sub-sequences of layers on separate accelerators, GPipe provides the flexibility of efficiently scaling a variety of different networks to gigantic sizes. Moreover, GPipe utilizes a novel batch-splitting pipelining algorithm, resulting in almost linear speedup when a model is partitioned across multiple accelerators.","Yanping Huang, Software Engineer, Google",Software / Cloud Services
How to Build a Multi-Camera Media Server for AI Processing on Jetson [S22396],"We'll build a simple multi-camera media server for AI processing on a Jetson Board and demonstrate how, by using GStreamer Daemon, Interpipes, and DeepStream, you can develop a scalable and robust prototype to capture from multiple cameras using GMSL2 Virtual Channels. Besides achieving real-time deep learning inference, the server is completely dynamic. We'll show its flexibility by triggering actions such as taking a snapshot, recording a video, or starting a network streaming when a specific prediction is made. By the end of this session you'll have acquired the framework basics that will allow you to scale to your specific multimedia and AI needs.","Carlos Rodriguez, Embedded Software Team Lead, RidgeRun",Software / Cloud Services
Knowledge Transfer Graph for Deep Collaborative Learning [P22383],"We present a new graph-based approach for more flexible and diverse combinations of knowledge transfer for deep collaborative learning. To achieve the knowledge transfer, we propose a novel graph representation called knowledge transfer graph that provides a unified view of the knowledge transfer and has the potential to represent diverse knowledge transfer patterns. We also propose four gate functions that are introduced into loss functions. The four gates, which control the gradient, can deliver diverse combinations of knowledge transfer. Searching the graph structure enables us to discover more-effective knowledge transfer methods than a manually designed one. Experimental results show that the proposed method achieved significant performance improvements and was able to find remarkable graph structures.","Hironobu Fujiyoshi, Professor, Chubu University",Software / Cloud Services
Learning Human Objectives by Evaluating Hypothetical Behavior [P22312],We present ReQueST: an algorithm for training reinforcement learning agents from human feedback in the presence of unknown unsafe states.,"Siddharth Reddy, Ph.D. Student, University of California, Berkeley",Software / Cloud Services
Modularizing Natural Language Processing [S21560],"Recent success and growth in natural language processing and artificial intelligence have given the world many new applications, techniques, models, and architectures. We'll show how appropriate abstraction and modularization can streamline both development and deployment of NLP technologies. We'll provide a systematic overview of NLP abstractions and breakdown, the insights of machine learning integration, and the designs of NLP systems for fast module development. You'll learn to use off-the-shelf tools to practice the modularized NLP and build practical applications. Our talk is suitable for researchers and practitioners with an intermediate understanding of NLP and ML concepts and applications, and a strong interest in real-world NLP application design and development.","Zhengzhong Liu, Research Assistant, Carnegie Mellon University",Software / Cloud Services
Motion Reasoning for Goal-Based Imitation Learning [P22363],"We address goal-based imitation learning, where the aim is to output the symbolic goal from a third-person video demonstration. This enables the robot to plan for execution and reproduce the same goal in a completely different environment. The key challenge is that the goal of a video demonstration is often ambiguous at the level of semantic actions. The human demonstrators might unintentionally achieve certain subgoals in the demonstrations with their actions. Our main contribution is to propose a motion reasoning framework that combines task and motion planning to disambiguate the true intention of the demonstrator in the video demonstration. This allows us to robustly recognize the goals that cannot be disambiguated by previous action-based approaches.","De-An Huang, Ph.D. Student, Stanford University",Software / Cloud Services
"Named Tensors, Model Quantization, and the Latest PyTorch Features [S22145]","PyTorch, the popular open-source ML framework, has continued to evolve rapidly since the introduction of PyTorch 1.0, which brought an accelerated workflow from research to production. We’ll deep dive on some of the most important new advances, including the ability to name tensors, support for quantization-aware training and post-training quantization, improved distributed training on GPUs, and streamlined mobile deployment. We’ll also cover new developer tools and domain-specific frameworks including Captum for model interpretability, Detectron2 for computer vision, and speech extensions for Fairseq.","Joseph Spisak, Product Manager, Facebook",Software / Cloud Services
Opening Up the Black Box: Model Understanding with Captum and PyTorch [S22147],"PyTorch, the popular open-source ML framework, has continued to evolve rapidly since the introduction of PyTorch 1.0, which brought an accelerated workflow from research to production. We’ll deep dive on some of the most important new advances, including the ability to name tensors, support for quantization-aware training and post-training quantization, improved distributed training on GPUs, and streamlined mobile deployment. We’ll also cover new developer tools and domain-specific frameworks including Captum for model interpretability, Detectron2 for computer vision, and speech extensions for Fairseq.","Narine Kokhlikyan , Research Scientist, Facebook AI",Software / Cloud Services
Optimized Image Classification on the Cheap [s21598],"We'll anchor on building an image classifier trained on the Stanford Cars dataset to evaluate two approaches to transfer learning — fine tuning and feature extraction — and the impact of hyperparameter optimization on these techniques. Once we define the most performant transfer-learning technique for Stanford Cars, we'll explore Bayesian Optimization as a black-box optimization technique to tune image-transformation parameters required to augment the model, using the downstream image classifier’s performance as the guide. Drawing on a rigorous set of experimental results can help us answer the question: How can resource-constrained teams make tradeoffs between efficiency and effectiveness using pre-trained models?","Meghana Ravikumar, Machine Learning Engineer, SigOpt",Software / Cloud Services
Optimizing Work Scheduling and Memory Usage for Complex Database Queries on GPUs [P22017],"The growth of data volumes of online analytical processing systems has made the GPU an attractive platform for executing data analytical queries. Besides the large data sizes, most of the real-world analytical queries are very complex, with multiple joins performed across multiple tables. In this session, we'll show how CUDA Graphs and RAPIDS cuDF components enable a much easier acceleration of complex queries on the GPU. In addition to programmability improvements, our design achieves better utilization of system resources for complex queries, such as optimized data pipeline, maximized task parallelism, and more efficient memory management. Moreover, we compare CUDA Graphs approach with the traditional approach using multiple streams and showcase how we achieve temporary memory reusing across different joins.","Nikolay Sakharnykh, Developer Technology Engineer, NVIDIA",Software / Cloud Services
Optuna: An Eager Hyperparameter Optimization Library [s21291],"We'll present Optuna, an open-source, next-generation hyperparameter optimization framework with three novel design criteria: (1) an eager API that allows users to concisely construct dynamic, nested, or conditional search spaces; (2) efficient implementation of both sampling and early-stopping strategies; and (3) an easy-to-set-up, versatile architecture that can be deployed for various purposes, ranging from simple scaling on distributed GPUs to lightweight experimentation, conducted via interactive interface. Optuna is the first optimization software designed to run in Eager mode. We'll present the basic usage of Optuna, describe the design techniques that we needed to meet the above criteria during development, and share our experiences leveraging Optuna to tune hyperparameters on single and multiple GPUs.","Shotaro Sano, Engineer, Preferred Networks",Software / Cloud Services
Scaling Deep Learning for Automatic Speech Recognition [S21838],"We'll discuss challenges of scaling automatic speech recognition (ASR) workloads with wav2letter++, a fast C++ toolkit for ASR. We'll introduce distributed training techniques used to achieve almost linear scalability and compare wav2letter to other popular ASR toolkits. Constant increase in model and dataset sizes, along with current trends toward unsupervised and semi-supervised learning, require squeezing out every bit of performance. In addition to distributed training, we'll cover other approaches for faster training and for training large models.","Jacob Kahn, Research Engineer, Facebook",Software / Cloud Services
Software-Based Compression for Analytical Workloads [S21597],"Real-world analytical pipelines have very large memory requirements and stress the CPU-GPU and GPU-GPU interconnects. The GPU memory size is limited, and the data is often offloaded to CPU memory for further processing on the GPU later. That can present a significant bottleneck for the end-to-end pipeline. Fast compression and decompression can improve performance by reducing the amount of data to be sent over the interconnect, or even completely eliminate the need to offload data by storing it in GPU memory in compressed format and performing subsequent operations on the compressed data. We'll survey various parallel compression algorithms, from LZ-based to run-length encoding, dictionary, and bit-packing. Efficient GPU implementations of cascaded schemes for analytical workloads will be presented, along with user-friendly interfaces for GPU applications. Our best approaches can achieve up to 77x compression ratio for columns from Fannie Mae's Loan Performance dataset, and maintain 250-350GBps compression/decompression speed on Tesla V100.","Nikolay Sakharnykh, Developer Technology Engineer, NVIDIA",Software / Cloud Services
"The Future of GPU Rendering: Real-Time Raytracing, Holographic Displays, and Light Field Media [S22153]","We'll present our vision for the future of GPU rendering and how it will impact gaming, VFX, media, and design in the 2020s and beyond. We'll detail how the future of media lies in holographics, light field technologies, and real-time rendering, and how OTOY is working to help drive that future through OctaneRender.","Jules Urbach, CEO, OTOY Inc",Software / Cloud Services
The SpeechBrain Project [s21648],"SpeechBrain is an open-source project that aims to develop an all-in-one speech toolkit based on PyTorch. Our goal is to create a single, flexible, user-friendly toolkit that can be used to easily develop state-of-the-art speech technologies. SpeechBrain will be a standalone framework that can significantly speed up research and development of speech and audio processing techniques. Indeed, it's a lot easier to familiarize oneself with a single toolkit than to learn several different frameworks, as one must do today. Moreover, using a single platform makes it easier to build a strong and fruitful community where members can share models, codes, baselines, and suggestions with a possible positive impact in the field of speech technologies. SpeechBrain is currently under development, and a first alpha version will be available in the next months. We'll describe the motivations, goals, and current status of the project.","Mirco Ravanelli, Ph.D., Mila - Université de Motréal",Software / Cloud Services
TNL: Template Numerical Library for Modern Parallel Architectures [P21800],"We'll present a numerical library that's being developed int our department. The library's goal is to offer flexible data structures and algorithms for the high performance computing, and especially for GPUs. Its design benefits strongly from the modern features of C++. In many situations, the user of TNL may write code independently on the hardware platform. The library is available at www.tnl-project.org.","Tomáš Oberhuber, Researcher, Czech Technical University in Prague",Software / Cloud Services
Toward Industrial LES/DNS in Aeronautics: Leveraging OpenACC for Massively Parallel CPU+GPU Simulations [s21958],"We'll describe recent advances toward industrial LES/DNS computational fluid dynamics within the scope of the EU TILDA (Towards Industrial LES/DNS in Aeronautics) project. The TILDA project aims to complete high-fidelity industrial LES/DNS simulations with upwards of 1 billion degrees of freedom, with a turnaround time on the order of one day. Achieving this requires near-linear efficiency on massively parallel, heterogeneous CPU+GPU compute resources. We'll describe the development of FineFR, a high-order CFD solver supporting heterogeneous CPU+GPU architectures. We'll emphasize the highly tuned OpenACC implementation, allowing very efficient data locality with minimal code intrusion. Finally, we'll present benchmark data and demonstration computations from the OLCF Summit Supercomputer showing near-linear scalability on upwards of 50,000 CPU cores and 7,000 NVIDIA GPUs.","David Gutzwiller, Software Engineer, head of HPC, Numeca",Software / Cloud Services
Toward INT8 Inference: Deploying Quantization-Aware Trained Networks using TensorRT [s21664],"We'll describe how TensorRT can optimize the quantization ops and demonstrate an end-to-end workflow for running quantized networks. Accelerating deep neural networks (DNN) is a critical step in realizing the benefits of AI for real-world use cases. The need to improve DNN inference latency has sparked interest in lower precision, such as FP16 and INT8 precision, which offer faster inference time. Two prevalent techniques to convert FP32 DNNs to INT8 precision are post-training quantization and quantization-aware training (QAT). TensorRT, a platform for high-performance deep learning inference, supports post-training quantization by performing calibration on the trained model, which quantizes the weights and activations. However, in some cases post-training quantization can degrade accuracy when converting a FP32 model to its INT8 counterpart. QAT introduces quantization ops to achieve higher accuracy by simulating the process for lower-precision quantization during training.","Dheeraj Peri, Deep Learning Software Engineer, NVIDIA",Software / Cloud Services
Training and Deploying Conversational AI Applications with NeMo and Jarvis [S21211],"Learn how to build speech recognition, natural language processing, and speech synthesis services with Neural Modules. First, we'll cover the basics of the NeMo toolkit for training and fine-tuning conversational AI models on your data. Then, we'll discuss how to use Jarvis to deploy and combine these services into a complete conversational AI solution.","Oleksii Kuchaiev, Senior Applied Scientist, NVIDIA",Software / Cloud Services
XLNet Optimization Using CUDA [S21478],"XLNet, a generalized autoregressive pretraining method, achieved great results on several natural language processing tasks. Compared to the previous language model, XLNET has advantages like being able to process long sentences, and avoids the disadvantage of using special tokens. However, as far as we know, there still isn't proper performance optimization for XLNet using CUDA, which would demand more inference time and hinder XLNET's wide deployment. We first ran the performance analysis of XLNet using its Tensorflow code. Then we optimized XLNet with these aspects:
For relative positional encoding, we optimized its parallelization with the help of cuBlas;
We customized the corresponding self-attention architecture based on the attention code in FastTransformer; and
We used kernel fusion and other CUDA optimization strategies to speedup XLNet.","Christina Zhang, DevTech Engineering, NVIDIA",Software / Cloud Services
