conference ,title,description,Inc
NVIDIA GTC 2020,Accelerating Unsupervised SAR Polarimetric Image Segmentation by Parallel Wishart Classifier [P21999],"Our poster presents the unsupervised segmentation of SAR polarimetric images. These images are multi-band areal images of the Earth captured by satellites with special sensors. The process required to segment these images is massive, and the task of segmentation is crucial, as through it we needed to extract important physical information of the area under observation, such as the geometric orientation, structure, shape, and configuration. We propose a much faster parallel implementation of Wishart Classification Based technique that outperforms the vanilla methods by 240%.",University of Cambridge
NVIDIA GTC 2020,AI  The Network Edge (Presented by HPE) [S22480],"It's at the extreme edge of your network, where most data generation occurs. In this session, we'll cover the unique system, software, and data capabilities required to extend AI to the edge in extreme environments. We will also discuss how to extend the data fabric that AI depends on from the edge to other processing elements within the Core and Cloud.",Hewlett Packard Enterprise
NVIDIA GTC 2020,Anomaly Detection on Aircraft Sensor Data Using Deep Learning [s21454],"A vast amount of aircraft sensor data still remains unexploited due to its volume and shear complexity. In 2019, Airbus released, as part of a four-month long challenge, a 65+ gigabyte dataset recorded from real aircraft systems. The goal was to detect a set of anomalies from an unlabeled multidimension time series dataset. We'll describe our solution, which is based on a two-stage approach using auto encoders and long short-term memory neural networks, and how we reached third place out of 160 teams competing. You'll learn the benefits of using autoencoder to achieve dimension reduction in a clustering problem and how LSTM-based neural networks can be applied to detect anomalies in an unsupervised way. We'll dive into the technical details of the solution and discuss the results obtained, as well as potential next steps. Basic knowledge of deep learning-techniques such as Autoencoder or LSTM will help, but isn't required.",Teradata
NVIDIA GTC 2020,Complex 60 GPU CFD Simulations for Aerospace Gearboxes [s21788],"We'll present a unique new computational fluid dynamics approach to solving the extremely complex multiphase case of an aerospace gearbox model using a purely GPU-based CFD solver, Altair nanoFluidX. The challenge is to better understand the complex flow inside such machinery, as good oil supply to all the key areas inside the gearbox is critical for efficient heat management, lubrication, and fuel consumption. We'll introduce the numerical method used, main challenges of the current state-of-the-art methods, CFD results (flow fields), and performance numbers from the cluster, going all the way up to 60 GPUs.",Altair Engineering
NVIDIA GTC 2020,Deep Learning-Based Anti-Drone System [P21900],This poster is about research on detecting and tracking drones by applying deep learning technology to capture drones that are illegally operated.,KAIST
NVIDIA GTC 2020,Deep-Learning Model for Finding New Superconductors [P21328],"We report the first deep-learning model for finding new superconductors. We represented the periodic table in such a way that deep learning can learn it. Although we used only the chemical composition of materials as information, we obtained an R2 value of 0.92 for predicting Tc for materials in a database of superconductors. We obtained three remarkable results. The deep-learning method can predict superconductivity for a material with a precision of 62\%, which shows the usefulness of the model; it found the recently discovered superconductor CaBi2, which is not in the superconductor database; and it found Fe-based high-temperature superconductors (discovered in 2008) from the training data before 2008. These results open the way for the discovery of new high-temperature superconductor families.",National Institute of Information and Communications Technology
NVIDIA GTC 2020,Landing on Mars: Petascale Unstructured-Grid Computational Fluid Dynamics Simulations on Summit [S21584],"We'll present a campaign to investigate the use of supersonic retropropulsion as a means to land payloads large enough to enable human exploration on Mars. The world’s largest supercomputer, Summit, located at Oak Ridge National Laboratory, performs simulations. We'll review the engineering and computational challenges associated with retropropulsion aerodynamics and the need for large-scale resources like Summit. A GPU implementation of NASA Langley Research Center's FUN3D flow solver is used for these simulations. We'll compare the development history, performance, and scalability with those of contemporary HPC architectures. Using an optimized GPU-accelerated computational fluid dynamics solver on Summit has enabled simulations well beyond conventional computing paradigms.",Nasa Langley Research Center
NVIDIA GTC 2020,Large-Scale Landslides Detection from Satellite Images with Incomplete Labels [P21920],"Earthquakes and tropical cyclones cause suffering for millions of people around the world every year. The resulting landslides exacerbate the effects of these disasters. Landslide detection is, therefore, critical for protecting human life and livelihood in mountainous areas. To tackle this problem, we propose a combination of satellite technology and deep neural networks (DNNs). We evaluate the performance of multiple DNN-based methods for landslide detection on actual satellite images of landslide damage using the capabilities of high-performance GPUs. Our analysis demonstrates the potential for a meaningful social impact on disasters and rescue.",Ridge-i inc.
NVIDIA GTC 2020,LUCID: High-Resolution Ground-Based Observations of LEO Satellites with Multi-Frame Blind Deconvolution [s21270],"High-resolution imaging of objects in space from a ground-based observatory is achievable with a sufficiently large aperture, but atmospheric turbulence causes significant degradation. Computationally expensive algorithms can mitigate the blurring effects of turbulence, and these algorithms have only recently begun to leave the domain of CPU-bound computation. We'll describe space domain awareness, the imaging-through-turbulence problem, and algorithms that attempt to solve it. We'll also describe Likelihood-based Uncertainty Constrained Iterative Deconvolution (LUCID), a new multi-frame blind deconvolution implementation that uses CUDA to extract high-resolution images of low-Earth-orbit (LEO) satellites from a series of short-exposure observations.",The Boeing Company
NVIDIA GTC 2020,Super-Resolution of Digital Terrain Data [P21822],"Given a digital elevation model (DEM) at some fixed-grid post spacing, we'd like to better interpolate between the posts to produce a DEM of higher resolution. Intuitively, single-image super-resolution techniques can do that. A neural network is trained on low-resolution elevation data (for example, 90m post spacing) until it can produce a reasonable approximation of higher-resolution elevation data, such as 30m post spacing. Dictionary-based interpolation is achieved through a statistical mapping from low-resolution (LR) features to high-resolution (HR) features, as learned by the hidden layers in the network. This approach shows promising results in producing pseudo-HR elevation models from LR models, and if applied to HR data, can produce pseudo-VHR models suitable for meshing with lidar scans.",Centauri
NVIDIA GTC 2020,Using Robust Networks to Inform Lightweight Models in Semi-Supervised Learning for Object Detection [P21789],"Learn from industry practitioners as we present a practical, streamlined data curation workflow designed to mitigate burdens associated with hand-labeled data for supervised learning in object detection. We utilize the capacity of robust object detection networks to efficiently train low-latency networks with access to large pools of unlabeled data but only limited hand-labeled data. By fine-tuning a robust model on the limited examples, we create new, larger training datasets via robust inference on unlabeled data. The combined hand-labeled subset and the inferenced dataset are then used to train a lightweight model. This approach results in more accurate lightweight models with minimal cost from hand-labeled data while also providing an efficient way of curating ground-truth datasets.",SURVICE Engineering
NVIDIA GTC 2020,VR Blast Forensics [S21629],We'll present a unique approach of coupling high-fidelity computational physics GPU-based solvers with the software Unity to produce a post-blast forensics virtual reality environment. We'll also cover the benefits of utilizing GPUs to perform both computational physics and VR rendering.,Missouri University of Science and Technology
NVIDIA GTC 2020,WASP: A WeArable SuPercomputing Platform for Lost Person Search-and-Rescue [P22071],"WASP, A WeArable SuPercomputing Platform for Lost Person Search-and-Rescue (SAR), redefines computation at the edge. More than 100,000 people were reported lost in the wilderness and urban settings in the United States in 2017. Use of UAVs for SAR applications shines due to their aerial point of view, enhanced mobility, and wireless connectivity by virtue of unhindered airspace in otherwise-dense foliage. Despite these benefits, the adoption of UAVs in such applications is rendered somewhat infeasible due to their short flight times and limited computation and load-bearing capabilities. WASP, which houses four Jetson Xavier AGXs, aims at offloading computation from the drone to a wearable backpack for the human-in-the-loop searcher, with a high-capacity battery and wireless router. The backpack not only manages massive real-time computational workloads including deep-learning inference, dynamic path planning, and GUI, but also hands-off enhanced control and contextual information to the human searchers.",Virginia Tech
NVIDIA GTC 2020,Accelerating DNN Inference with GraphBLAS and the GPU [S21796],"Our work addresses the 2019 Sparse Deep Neural Network Graph Challenge with an implementation using the GraphBLAS programming model. We'll demonstrate our solution to this challenge with GraphBLAST, a GraphBLAS implementation on the GPU, and compare it to SuiteSparse, a GraphBLAS implementation on the CPU. The GraphBLAST implementation is 1.94x faster than SuiteSparse; the primary opportunity to increase performance on the GPU is a higher-performance sparse-matrix-times-sparse-matrix (SpGEMM) kernel.",Davis
NVIDIA GTC 2020,Accelerating Graph Algorithms on Exascale Systems [S21642],"We'll discuss our current efforts for accelerating graph algorithms on the latest US Department of Energy leadership systems. Graph methods are key kernels for large-scale data analytics, as well as for several exascale application domains, including smart grids, computational biology, computational chemistry, and climate science. We'll present our latest results on distributed implementations employing GPUs and accelerators of graph kernels such as community detection and influence maximization, showing how we can tackle large-scale problems with heterogeneous supercomputers.",Pacific Northwest National Laboratory
NVIDIA GTC 2020,Accelerating Large-Scale GW Calculations in Material Science [s21353],"Learn the balancing act of porting a large-scale HPC code to modern GPUs, where a plethora of architectural characteristics can both accelerate and limit performance. We'll showcase various techniques used to accelerate the material science code BerkeleyGW on NVIDIA GPUs targeting large-scale simulations with thousands of atoms, matrices of up to 1 million by 1 million, and reductions of thousands of billions of numbers. These techniques include the use of cuBLAS and cuFFT, pinned memory, streams, batched operations, shared memory, and the overlapping of message-passing interface communication and GPU computation. Excellent strong scaling and weak scaling are observed on thousands of Volta GPUs, and a 16x improvement is obtained on FLOPs/Watt efficiency compared to the CPU-only implementation.",Lawrence Berkeley National Laboratory
NVIDIA GTC 2020,Accelerating the Singular Value Decomposition: Measuring SVD Performance for Small Matrices [P21776],"Advanced algorithms for spatio-temporal signal processing of data from a phased sensor array often require singular value decompositions (SVD) of a very large number of complex-valued matrices of a relatively small size. Our poster describes a GPU-implemented SVD that we custom-designed for such signal processing applications in order to enable real-time performance. We compare our SVD with tools available in the cuSolver, MKL, and MAGMA libraries, and we document a substantial performance improvement on batches of small square matrices.",Oak Ridge National Laboratory
NVIDIA GTC 2020,A Framework for Measuring Hardware Gather-Scatter Support [P21887],"This poster describes a new benchmark tool, Spatter, for assessing memory system architectures in the context of indexed accesses. This type of memory operation is often used to express sparse and irregular data patterns. Gathers and scatters have widespread utility in modern HPC applications, including traditional scientific simulations, data mining and analysis computations, and graph processing. Spatter specifically measures gather / scatter (G/S) variations for multiple platforms, with several tunable backends, including CUDA and OpenMP, and provides comparison metrics for different sparse access patterns. We show how Spatter can be used to evaluate the recent improvements to G/S hardware on NVIDIA GPUs and CPUs, and show performance numbers for STREAM-like, uniform-stride, and application-derived memory access patterns.",Georgia Tech
NVIDIA GTC 2020,A Partitioned Global Address Space Library for Large GPU Clusters [S22093],"We'll discuss NVSHMEM, a PGAS library that implements the OpenSHMEM specification for communication across NVIDIA GPUs connected by different types of interconnects that include PCI-E, NVLink, and Infiniband. NVSHMEM makes it possible to initiate communication from within a CUDA kernel. As a result, CUDA kernel boundaries are not forced on an application due to its communication requirements. Less synchronization on the CPU helps strong scaling efficiency. Ability to initiate fine-grained communication from inside the CUDA kernel helps achieve better overlap of communication with computation. QUDA is a popular GPU-Enabled QCD library used by several popular packages like Chroma and MILC. NVSHMEM enables better strong scaling in QUDA. NVSHMEM not only benefits latency-bound applications like QUDA, but can also help improve performance and reduce complexity of codes like FFT that are bandwidth-bound, and codes like Breadth First Search that have a dynamic communication pattern.",NVIDIA
NVIDIA GTC 2020,A Performance-Portable Diffusion Solver Based on Axom [P21892],"The sheer size and complexity of large-scale multiphysics codes motivate maintenance of a single-source codebase that is parallel and readily portable across different architectures. This is especially attractive due to recent trends toward heterogeneous architectures. We present Mint, an API that provides a mesh-aware, fine-grain, parallel execution model that underpins the development of computational tools and discretization methods. We build a 2D finite-volume diffusion solver that models separation of chemical species that arise in inertial confinement fusion. We demonstrate portability of the solver across two different architectures, one on a CPU-based architecture and one with CPU+GPU architecture. When compiled with the CUDA backend relative to the OpenMP backend, we find up to 3.5x speedup, demonstrating the viability of the API.",University of Texas at Austin
NVIDIA GTC 2020,Applications of Convolutional Neural Network to the Important Earth Science Problems [P22283],"We applied the convolutional neural network to several Earth science problems that involve (1) discrimination of seismic signals from earthquakes and tectonic tremors; (2) determination of an earthquake hypocenter; and (3) solar radiation estimation from fixed-point camera images. Monitoring tremor activity provides insights into deformation processes of megathrust earthquakes. We could achieve 99.5% accuracy for identifications of signals from tremors, regular earthquakes, and noise. We establish neural networks for the determination of hypocentral parameters. We calculate theoretical seismograms for a realistic 3D Earth model and use these seismograms as learning dataset. We can determine an earthquake hypocenter with this neural network. We develop a new observation method that used deep learning to estimate the amount of solar radiation from images. This new technique can be used to make multifaceted observations.",Japan Agency for Marine-Earth Science and Technology
NVIDIA GTC 2020,Big Lasers for Small Accelerators: Exascale Simulations for Better Cancer Therapy [S21850],"Learn how we leverage PIConGPU, an open source, multi-platform particle-in-cell code scaling to the fastest supercomputers in the TOP500 list (Titan, Piz Daint, Summit), to model advanced plasma physics applications. Advances in compact plasma-based accelerators driven by petawatt-class lasers spark great interest in their applications. Ion beams accelerated by intense laser pulses break new ground for treating cancer. Laser-generated electron beams can drive new compact X-ray sources to create snapshots of ultrafast processes in materials and show a path to reaching the energy frontier for studying fundamental physics. We'll present our strategies to harness the power of future exascale supercomputers, handling extreme data flows from thousands of GPUs for analysis with in-situ data analytics and an open data ecosystem. We'll provide detailed performance analysis and show the benefits of PIConGPU for real-world physics cases.",Helmholtz-Zentrum Dresden-Rossendorf
NVIDIA GTC 2020,CUDA C++ in Jupyter: Adding CUDA Runtime Support to Cling [S21588],"Jupyter Notebooks are omnipresent in the modern scientist's and engineer's toolbox just as CUDA C++ is in accelerated computing. We present the first implementation of a CUDA C++ enabled read-eval-print-loop (REPL) that allows to interactively ""script"" the popular CUDA C++ runtime syntax in Notebooks. With our novel implementation, based on CERN's C++ interpreter Cling, the modern CUDA C++ developer can work as interactively and productively as (I)Python developers while keeping all the benefits of the vast C++ computing and library ecosystem coupled with first-class performance.",Lawrence Berkeley National Laboratory
NVIDIA GTC 2020,Day of the Living Cell: Supercomputers Reveal Molecular Design Principles of Photosynthesis [S21500],"We'll discuss developing the highly parallel molecular dynamics code NAMD for current and upcoming machines, followed by an accessible presentation of results from a decade of work that paves the way to first-principles modeling of whole living cells. One of the many ambitious research projects begun by the late Klaus Schulten was to model, simulate, and ultimately understand the photosynthetic apparatus of bacteria, from the scale of individual atoms to that of the entire cell. The GPU-accelerated supercomputers of Oak Ridge National Laboratory running NAMD have been critical to enabling progress in this continuing work.",University of Illinois
NVIDIA GTC 2020,Deep Learning for Efficient Modeling of High-Dimensional Spatiotemporal Physics [S22094],"Several research problems in physical sciences are exceptionally complex and high-dimensional, exhibiting spatio-temporal dynamics, non-linearity, and chaos. In an era when vast quantities of such scientific data are generated, building practical, physics-driven reduced-order models (ROM) of such phenomena is crucial. While deep neural networks for spatio-temporal data have shown considerable promise, they face severe computational bottlenecks in learning extremely high-dimensional datasets, often with greater than 10^9 degrees of freedom. These application-agnostic networks may also lack physical constraints and interpretability that is desired in scientific ROMs. We'll present our efforts in leveraging the strong mathematical and physical foundations underlying wavelet theory with the learning capacity of deep neural nets. We'll demonstrate computationally efficient, partially interpretable learning with some embedded physics constraints for modeling large scientific datasets.",Los Alamos National Laboratory
NVIDIA GTC 2020,Dramatic Acceleration of Quantum Transport Simulations: Solving Non-Equilibrium Green’s Function with GPU Devices [P21787],"The poster presents in-depth discussion on technical details and strategies for GPU-driven performance enhancement in solving the Non-Equilibrium Green's Function (NEGF), which is critically used to simulate quantum transport behaviors of electronic devices in a nanoscale regime. Although here we focus on NEGF as a main target of performance enhancement, the contents (particularly the strategies of performance improvement with GPU computing) presented in this poster can be still useful for ANY numerical problems that involve the computation of an inverse of block-tridiagonal (dense/sparse, real/complex) matrices.",Korea Institute of Science and Technology Information
NVIDIA GTC 2020,Effective Use of Mixed Precision for HPC [S21725],"We'll discuss how using mixed-precision techniques effectively can significantly speed up current-generation supercomputers, as well as the practical and numerical challenges in adopting such techniques. We'll focus specifically on lattice quantum chromodynamics, which is a regular top-cycle consumer of public supercomputers. Such techniques are an important and sometimes necessary optimization direction for HPC applications.",NVIDIA
NVIDIA GTC 2020,Enabling 800 Projects for GPU-Accelerated Science on Perlmutter at NERSC [S21582],"The National Energy Research Scientific Computing Center (NERSC) is the mission HPC center for the U.S. Department of Energy Office of Science and supports the needs of 800+ projects and 7,000+ scientists with advanced HPC and data capabilities. NERSC’s newest system, Perlmutter, is an upcoming Cray system with heterogeneous nodes including AMD CPUs and NVIDIA Volta-Next GPUs. It will be the first NERSC flagship system with GPUs. Preparing our diverse user base for the new system is a critical part of making the system successful in enabling science at scale. The NERSC Exascale Science Application Program is responsible for preparing the simulation, data, and machine learning workloads to take advantage of the new architecture. We'll outline our strategy to enable our users to take advantage of the new architecture in a performance-portable way and discuss early outcomes. We'll highlight our use of tools and performance models to evaluate application readiness for Perlmutter and how we effectively frame the conversation about GPU optimization with our wide user base. In addition, we'll highlight a number of activities we are undertaking in order to make Perlmutter a more productive system when it arrives through compiler, library, and tool development. We'll also cover outcomes from a series of case studies that demonstrate our strategy to enable users to take advantage of the new architecture. We'll discuss the programming model used to port codes to GPUs, the strategy used to optimize code bottlenecks, and the GPU vs. CPU speedup achieved so far. The codes will include Tomopy (tomographic reconstruction), Exabiome (genomics de novo assembly), and AMReX (Adaptive Mesh Refinement software framework).",NERSC
NVIDIA GTC 2020,Enhancing Intra-Node Multi-GPU Stencil Calculations on DGX-2 Using Concurrent-Addressing with Unified Memory [P21784],"In the “CityLBM” project at the Japan Atomic Energy Agency, a real-time AMR (adaptive mesh refinement)-based urban wind prediction code was developed. The next generation of CityLBM code needs ensemble simulations to improve the reliability of the prediction. To achieve that, memory usage should shrink to a single node, or 4-16 GPUs per simulation. To reduce memory usage and accelerate data communication in the AMR code, we tried an intra-node multi-GPU implementation using Unified Memory in CUDA. This approach enables easy parallel-GPU implementation, because the access to Unified Memory is automatically managed from via HBM2 (self GPU) or NVLink (neighbor GPU). We implemented multi-GPU calculations for a 3D diffusion equation and a lattice Boltzmann equation on uniform mesh, and tested weak/strong scalability and NVLink utilization.",Japan Atomic Energy Agency
NVIDIA GTC 2020,Exploiting Novel GPU Parallelism in the SNAP Interatomic Potential [S21976],"Cutting-edge investigations of material behavior via classical molecular dynamics simulation methods require application-specific, quantum-accurate interatomic potentials (IAPs). The SNAP machine-learning IAP, formulated in terms of general four-body geometric invariants, is trained against quantum electronic structure calculations. This enables the verifiably high-fidelity investigation of diverse material systems at length- and timescales unattainable by purely quantum calculations. Despite the high arithmetic complexity, achieving good SNAP performance with the increasing parallelism provided by modern GPU architectures is challenging. To address this, we have developed a novel parallelization over the geometric structure of the SNAP IAP, prompting memory layout optimizations which facilitate data reuse and reduce memory bandwidth requirements. The new SNAP algorithm will be deployed in the GPU-optimized LAMMPS implementation using the Kokkos templated C++ library.",NVIDIA
NVIDIA GTC 2020,Explosive DGX Performance for Weapon Component Modeling [P21870],"At Sandia National Laboratories, we model explosive weapon components using sophisticated simulations. This poster shows how porting those simulations using CUDA and running them on a DGX Workstation provided great speedups, enabling weapon designers a whole new workflow for simulation.",Sandia National Laboratories
NVIDIA GTC 2020,Extensions of TensorFlow-Based Computational Fluid Dynamics [S21816],"The National Energy Technology Laboratory (NETL) recently developed a completely TensorFlow-based computational fluid dynamics code for single-phase flow as a potential next-generation solver for Multiphase Flow with Interphase eXchanges (MFIX). NETL demonstrated over a 3x speedup using NVIDIA V100 GPUs on a DGX-1 before any domain decomposition was implemented. NETL is currently working on implementing parallel linear solvers in TensorFlow. In addition, NETL has strategically chosen to invest in a TensorFlow-based Multiphase Particle-In-Cell (MP-PIC) methodology. We'll cover the most recent developments to further the TensorFlow-based solver, and discuss plans for AI/ML acceleration.",The National Energy Technology Laboratory
NVIDIA GTC 2020,Fromage Optimizer for Deep Neural Networks [P22300],"We propose a new geometric characterization of neural network loss surfaces, called deep relative trust. This characterization suggests a new optimization algorithm that we call Fromage, short for ""Frobenius matched gradient descent."" Experiments with the optimizer on standard deep learning benchmarks—including Transformer and GAN—are promising.",California Institute of Technology
NVIDIA GTC 2020,"GPU-Accelerated Deep Learning for Weather, Climate, and Space [s21255]","We'll demonstrate how to use deep learning to tackle important challenges in weather forecasting, climate modeling, and the processing of satellite observations. We'll present recent results from ongoing research collaborations with the National Oceanic and Atmospheric Administration, NASA, and various universities, and explain how accurate results were achieved. We'll show how to automate feature detection to identify threats from severe weather, solar storms, and near-earth objects, and we'll discuss how to accelerate weather/climate models and data assimilation techniques to produce more accurate predictions. Finally, we'll illustrate how to better use satellite observations by enhancing, transforming, interpolating, fusing, and repairing multispectral data.",NVIDIA
NVIDIA GTC 2020,GraphDefense: Toward Robust Large-Scale Graph Convolutional Network [P21806],"We study the robustness of graph convolutional networks (GCNs). Despite the good performance of GCNs on graph semi-supervised learning tasks, previous works have shown that the original GCNs are very unstable to adversarial perturbations. Inspired by the previous works on adversarial defense for deep neural networks, and especially adversarial training algorithm, we propose a method called GraphDefense to defend against the adversarial perturbations. In addition, for our defense method, we could still maintain semi-supervised learning settings without a large label rate. We also show that adversarial training in features is equivalent to adversarial training for edges with a small perturbation. Our experiments show that the proposed defense methods successfully increase the robustness of Graph Convolutional Networks. Furthermore, we show that with careful design, our proposed algorithm can scale to large graphs, such as Reddit dataset.",Davis
NVIDIA GTC 2020,Hedgehog: A Performance-Oriented General Purpose Library that Exploits Multi-GPU Systems [s21227],"We'll present Hedgehog, a general-purpose library for taking advantage of powerful compute nodes, multicore CPUs, and multiple GPUs. The novel aspects of Hedgehog are: (1) its explicit representation of a program as a dataflow graph, (2) its pure dataflow-driven scheduling, (3) its maintenance of a computation’s localized state via state managers, and (4) its fine control of memory via memory managers. This dataflow approach results in extremely low overhead for task executions (< 1 microsecond) and no-cost profiling at the task level. This allows us to prototype operations that compare favorably with leading libraries such as cuBLAS-XT.",NIST
NVIDIA GTC 2020,"High-Throughput 3D Image Reconstruction, Visualization, and Segmentation of Large-Scale Data at the Sirius Synchrotron Light Source [s21278]","We'll present highly efficient tools for large-scale 3D image reconstruction, visualization, and segmentation being developed for the Sirius synchrotron light source. Sirius will be the second fourth-generation synchrotron in the world, and will acquire 3D/4D images with resolution up to <50 nm using hard X-rays. With NVIDIA, we're creating integrated pipelines to reconstruct 3D images from modalities such as coherent-diffraction imaging and transmission tomography, visualize the data in streaming mode, and segment the images to provide almost real-time feedback. We rely on multi-GPU/node CUDA programming and machine/deep learning-optimized inference to address the issues, given that each 3D image may be larger than 100 GB and may be acquired down to 1s, resulting in around 50 TB of data of a wide variety of samples (for example, biological and geological) expected to be produced every day.",Brazilian Synchrotron Light Laboratory / CNPEM
NVIDIA GTC 2020,High-Throughput Real-time Data Processing with GPUs at CERN [s21341],"We'll present the design and performance considerations and system optimization of a GPU-based, real-time physics selection system at a Large Hadron Collider experiment. Millions of particles collide every second at the LHCb experiment at CERN in Switzerland. To select interesting particle collisions, data must pass through an acquisition system and be filtered with real-time selection software. The throughput processed in the first stage of this streaming data processing application amounts to 40 terabytes per second, and the efficiency of the selection is crucial toward improving our fundamental understanding of the universe. In order to process this massive data throughput in real-time, we developed GPU physics reconstruction software called Allen. The Allen framework hands the raw data to GPU streams, which perform the decoding, reconstruction, and selection of particle collisions.",NIKHEF and University of Maastricht
NVIDIA GTC 2020,HPL-AI: Benchmarking Half-Precision Hardware with Modern Numerical Linear Algebra [S21470],"We'll present the mixed-precision iterative and direct methods used by the HPL-AI benchmark. These new approaches are instrumental in kernel-based performance evaluation of modern hardware accelerators that offer fast implementation of limited-precision floating-point units. The scope of the benchmark spans a number of important computational patterns that scientific codes, both in the past and on modern GPUs, often rely upon. We had to resolve a number of numerical issues to achieve a robust implementation, and we'll present those results for relevant background on the design process involved in benchmarking and how it can be leveraged for the benefit of the GTC audience.",University of Tennessee
NVIDIA GTC 2020,Improving Geophysical Turbulence Models with Machine Learning [S21574],"In large-scale geophysical flows, the relatively small-scale processes of turbulence and mixing can have a leading-order impact on the prediction of (for instance) ocean circulation and global energy budgets. Such predictions are critical components of weather and climate simulation — geophysical problems where small-scale models help offset the otherwise prohibitively expensive computational cost of simulation. These turbulence closure models attempt to capture dynamics that have complex functional dependence on a potentially broad range of large-scale flow parameters. However, models and frameworks are often phenomenological and heuristic in nature, such that robust model calibration to simulation, observation, and experiment data is a challenge. We'll explain how a nonintrusive supervised GPU-driven machine learning framework, such as Neural ODE, can help improve the state of turbulence models in canonical geophysical flows. We'll also discuss the interpretability of these machine-learning models and provide a roadmap to create more general frameworks for modeling such physics.",Los Alamos National Laboratory
NVIDIA GTC 2020,Improving Ocean Model Framework NEMO with GPU Port and Asynchronous Execution [P21955],"Our work describes the improvement of an operational ocean model framework (NEMO) with routine porting and asynchronous execution in large-scale clusters using CUDA. We show routine GPU optimizations speed-up and implementation of asynchronous execution over large scale workloads. Considering that NEMO is a standard production framework for research activities and forecasting service, we aim to expose the ongoing challenges of accelerating Fortran/MPI long-term applications aligned with the needs of a broad community of developers.",Barcelona Supercomputing Center
NVIDIA GTC 2020,"Integrating NVIDIA Tesla V100 GPUs into a Cray System for a Diverse Simulation, Machine Learning, and Data Workload [S21569]","The HPC system ""Perlmutter"" will the first GPU-accelerated production system at the U.S. Department of Energy’s National Energy Research Scientific Computing Center (NERSC) when it is deployed in 2021. We'll explain how, to enable its users to prepare their applications for Perlmutter, NERSC recently integrated 18 GPU-accelerated compute nodes into its current production system, the Knights Landing-based Cray system ""Cori."" These nodes’ primary purpose is application development and profiling for GPU acceleration, as part of the NERSC Exascale Science Application Program (NESAP). Despite significant differences in hardware from the rest of the Cori system, the GPU nodes have been configured such that, from both the user’s and the administrator’s perspective, they are seamlessly integrated into Cori. This integration has streamlined access for hundreds of NESAP users to these nodes, and facilitates a diverse workload of NESAP applications spanning simulation, data-intensive workloads, and machine learning.",Lawrence Berkeley National Laboratory
NVIDIA GTC 2020,Interpretable Deep Learning for Hurricane Intensity Prediction [S21548],"Hurricanes can experience rapid increases in intensity, in which they can strengthen from a tropical storm to a major hurricane in only a couple of days. These rapid intensification periods are currently difficult to predict, but deep learning may be able to detect spatial patterns in the storms that are precursors to rapid intensification. We'll show how a convolutional neural network trained on output from the Hurricane Weather Research and Forecasting model can produce probabilistic estimates of rapid intensification. We'll also show how deep-learning interpretation techniques can reveal what storm structures are associated with rapid intensification versus rapid weakening.",National Center for Atmospheric Research
NVIDIA GTC 2020,Learning the Hard Parts: Scaling Reacting Flow Simulations with Machine Learning [S21740],"Explore a new technique for overcoming bottlenecks in computational fluid dynamics with machine learning. We'll describe using a neural chemistry solver for an order-of-magnitude speedup, opening the doors to problems that are currently impossible. Simulating chemically reacting flow — fluid flow with a chemical source term (such as a flame) — is computationally prohibitive: an at-scale combustor simulation with a practical hydrocarbon fuel can take years! Solving the chemical source term dominates this process, taking up to 90% of the total compute time. This share increases for more complex reactions, resulting in poor scalability. We'll discuss designing a neural network to model hydrogen combustion; integrating this neural chemistry solver into computational fluid dynamics code; analyzing the performance of this new approach; and discussing how these lessons could be applied to other CFD bottlenecks.",Naval Research Lab
NVIDIA GTC 2020,Matrix-Free Real-time Online Reconstruction of Compressive Focal Plane Array Camera [P21865],"This study proposes a solution for online and real-time reconstruction of compressive focal plane array cameras. These cameras enable high-resolution imaging using low-resolution sensors through compressive sensing reconstruction. Here, we use a spatial light modulator for encoding high-resolution data onto low-resolution sensors. Then, we solve an optimization problem for reconstruction. Previous methods involve reconstruction through algorithms that require many batched matrix multiplications. In this study, we propose using an algorithm that does not involve any matrix multiplication and show its performance against the previous method. Due to the high computational load of the image reconstruction algorithms, we use GPUs for fast computation. Moreover, we show that the camera can work in real time by sampling each data online and reconstruction frames in real time in a pipelined fashion. Finally, we show the reconstructed images through OpenGL interoperability library.",Aselsan Research Center
NVIDIA GTC 2020,Optimizing Stencil Operations in OpenACC [P21723],"Stencil operations are used widely in HPC applications and pose an optimization challenge on both CPUs and GPUs. On GPUs, fine-tuned optimizations can be formulated using low-level APIs such as CUDA, but many large established codes prefer a portable, higher-level API such as OpenACC. Although OpenACC lacks the fine-tuning of CUDA, it does allow for some tuning through a variety of parallelization constructs and loop directives. Here, we optimize a stencil operation within our production solar physics research code Magnetohydrodynamics Around a Sphere. We explore numerous OpenACC directive options (including tile, cache, collapse, etc.) and compare their performance over several problem types and sizes. The optimal result is used to run a full-scale simulation and analyzed with Nsight Systems. An emerging cautionary result is that although many directive options yield a speedup of the operator, using the ""wrong"" directives can result in drastically poor performance.",Predictive Science Inc.
NVIDIA GTC 2020,"Performance Comparison of Search for Neighbor-Particle in MPS on Xeon, Xeon Phi and GPU [P21969]","The MPS method is a particle-based simulation used for computation fluid dynamics. It was originally developed for simulating fluid dynamics such as fragmentation of incompressible fluid. Target fluid or objects are divided into particles and each particle interacts with neighbor-particles. Searching for neighbor-particles is the main bottleneck of MPS. We’re researching and developing the in-house program called P-Flow_lite. We're proposing two optimizations for a search for neighbor-particles and implementing them on Xeon, Xeon Phi, and GPU by using directives. V100 is the fastest among them.",RIKEN Center for Computational Science
NVIDIA GTC 2020,Roofline Performance Model for HPC and Deep-Learning Applications [s21565],"Learn how to use the Roofline model to analyze the performance of GPU-accelerated applications. We'll cover the basics of the model, explain how to use tools such as nvprof and Nsight Systems/Compute to automate the data collection, and demonstrate how to track progress using Roofline for both HPC and deep-learning applications. We'll use examples such as GPP from material science, high-performance geometric multigrid from adaptive mesh refinement, and two kernels from TensorFlow to show how characteristics such as arithmetic intensity, memory access pattern, and thread divergence/prediction can all be captured by Roofline, offering useful insights to performance optimization.",Lawrence Berkeley National Laboratory
NVIDIA GTC 2020,Scalable Workflow System for Whole Slide Microscopy Analyses Using Neural Networks [P21983],"We present a streaming, asynchronous, high-throughput workflow for joining traditional computer vision and artificial intelligence inference across multiple models using the Hybrid Task Graph Scheduler and TensorRT for scalable, single-node, multi-GPU object detection, classification, and regression. Our approach targets very large (100k x 50k pixel) automated whole-slide microscopy imaging, which operates under tight time constraints. The end-to-end workflow starts with a microscope scan and finishes with a populated database containing quality-assurance metrics, object detections, and other salient features. This generalizable workflow applies three independent TensorRT models, with dependency management, concurrently across all available GPUs within a single computer.",NIST
NVIDIA GTC 2020,Scaling Data by 109x and Compute for Deep-Learning Applications [S21390],"We'll explore the scalable applications of artificial intelligence on massive data sets. First, we'll cover how we optimized and developed highly parallelized implementations of DL algorithms and tested them on HPC GPU clusters. Then we'll demonstrate how to develop models that can run over large high-resolution datasets, identifying the spatial and temporal relationships between physical parameters in global-scale high-resolution numerical weather prediction models.",DST/CSIRO
NVIDIA GTC 2020,The Rocky Road to Tasking: Task Queues Reloaded [S21189],"We'll show you how to parallelize your irregular algorithm on GPUs with tasking, starting with an overview of our CUDA C++ tasking framework for fine-grained task parallelism. After touching on persistent threads, synchronization mechanisms, and load balancing, we'll present diverse optimization strategies. First, we'll describe the implementation of task queues based on static memory allocation. Second, we'll show how to implement work sharing on a GPU through hierarchical task queues. Third, we'll present a thread coordination scheme to reduce contention on the task queues, thus keeping all threads busy. We'll analyze each optimization step's performance gains for a prototypic implementation of a task-based fast multipole method for molecular dynamics.",Jlich Supercomputing Centre
NVIDIA GTC 2020,"Tiny, Tiny Tasks: Fine-Grained FMM Tasking on GPUs [P22059]","Learn how fine-grained task parallelism occurring in many HPC applications can be exploited with GPUs. In contrast to available tasking approaches (e.g. CUDA graphs) that rely in some part on the involvement of the host CPU by scheduling new tasks, we present a scheme to fully control the execution of tasks by the GPU without additional synchronization or data movement from the CPU. A fine-grained formulation of tasks can lead to more available parallelism at any given time, and may thereby help to reduce the runtime even further. The poster will show the current concept of our GPU-tasking for the Fast Multipole Method. However, our ideas can be reused for other, especially hierarchical, algorithms. We'll discuss common bottlenecks and pitfalls when applying fine-grained tasking on a GPU.",Jlich Supercomputing Centre
NVIDIA GTC 2020,Toward an Exascale Earth System Model with Machine Learning Components: An Update [S21834],"Many have speculated that combining exascale GPU computational power with machine-learning algorithms could radically improve weather and climate modeling. We'll discuss the status of an ambitious project at the U.S. National Center for Atmospheric Research that's moving in that direction. Having achieved performance portability for a standalone version of the Model for Prediction Across Scales-Atmosphere (MPAS-A) on heterogeneous CPU/GPU architectures across thousands of GPUs using OpenACC, our project has begun looking at two new directions. First, we've launched an effort to port the MOM-6 Ocean Model. Second, machine-learning scientists at NCAR and elsewhere have begun evaluating replacing atmospheric parameterizations with machine-learned emulators, including the atmospheric surface layer, cloud microphysics, and aerosol parameterizations. We'll also discuss related efforts to apply machine-learning emulation to model physics.",National Center for Atmospheric Research
NVIDIA GTC 2020,Turbulence Forecasting via Neural ODEs [P22054],"Fluid turbulence is characterized by strong coupling across a broad range of scales. Furthermore, besides the usual local cascades, such coupling may extend to interactions that are non-local in scale-space. As such, the computational demands associated with explicitly resolving the full set of scales and their interactions, as in the direct numerical simulation of the Navier-Stokes equations, are so high in most problems of practical interest that modeling of scales and interactions must be reduced before further progress can be made. While popular reduced models are typically based on phenomenological modeling of relevant turbulent processes, recent advances in machine-learning techniques have energized efforts to further improve the accuracy of such reduced models. In contrast to such efforts that seek to improve an existing turbulence model, we propose a machine learning methodology that captures, de novo, underlying turbulence phenomenology without a pre-specified model form.",Los Alamos National Laboratory
NVIDIA GTC 2020,Workload Management for Complex Workflows on a GPU-Enabled Heterogeneous System [s21608],"The National Energy Research Scientific Computing Center, the U.S. Department of Energy mission supercomputing center, is integrating experimental science workflows into its existing HPC-centric workload. This introduces new scheduling requirements, including real-time computing demands and complex workflows that depend on external services, data, and devices. These new workflows also need to co-schedule heterogeneous job components, with detailed subtask scheduling on both GPUs and CPUs. It is challenging to maintain scheduler responsiveness to this dynamically changing workload on an extreme-scale HPC system. We'll explain innovations that NERSC introduced to the Slurm scheduler to help meet these challenges, enhancing the scheduling algorithm to enable full-scale jobs and single-core jobs to schedule fairly, and adapting reservation and preemption capabilities to allow real-time computing. We'll also show enhancements to support Linux containers both for user software and system resources, as well as heterogeneous job execution.",Lawrence Berkeley National Laboratory
NVIDIA GTC 2020,"Accelerate ML Lifecycle with Containers, Kubernetes and NVIDIA GPUs (Presented by Red Hat) [S22516]","Data scientists desire a self-service, cloud-like experience to easily access ML modeling tools, data, and GPUs to rapidly build, scale, reproduce, and share ML modeling results with peers and developers. Containers and kubernetes platforms, integrated with NVIDIA GPUs, provide these capabilities to accelerate the training, testing, and deploying the ML models in production quickly. We'll provide an overview of how these technologies are helping solve real-world customer challenges. We'll walk through the various customer use cases and solutions associated with the combination of these technologies. We'll review the key capabilities required in a containers and kubernetes platform to help data scientists easily use technologies (such as Jupyter Notebooks, ML frameworks, etc.) to innovate faster. Finally, we'll share the platform options (for example, Red Hat OpenShift, Kubeflow), and examples of how data scientists are accelerating ML initiatives with containers and Kubernetes.",Red Hat
NVIDIA GTC 2020,AWDF: An Adaptive Weighted Deep Fusion Architecture for Multi-Modality Learning [P21985],"Deep model fusion is getting lots of attention in dealing with multi-modality learning problems. We propose an Adaptive Weighted Deep Fusion scheme (AWDF) to capture potential relationships among various input sources. It integrates the feature-level and decision-level fusion in one framework. To address the limitations of existing fusing models with fixed weights, we also propose a new scheme named Cross Decision Weights Method (CDWM). It can dynamically learn the weight for each input branch during the fusion process instead of utilizing predefined weights. To evaluate the performance of AWDF, we conducted experiments on three different real-world datasets: Wild Business Terms, Iceberg Detection, and CareerCon. Our experiments demonstrate that AWDF outperforms other fusion systems by a large margin. (This work has been accepted by IEEE Big Data 2019 Special Session on Intelligent Data Mining).",IBM
NVIDIA GTC 2020,Container-Based Artificial Intelligence Applications Deployment Platform for GPU High Performance Computing Clusters [P21902],"A container-based AI applications deployment platform was proposed and implemented in our center to solve root privileges and network issues in HPC clusters. A virtual machine is used to simulate the targeted HPC cluster, which has the same or similar runtime environment including compilers, shared libraries, and GPU drivers based on a GPU card attached by PCI pass-through way. Users can have root privileges of virtual machines to set up containers. Further, a user can create containers and deploy AI applications by network tools, because each user has an exclusive virtual machine running on an independent server that connects to internet. The deploy platform and the HPC clusters are connected by a 10Gb/s, or even 100Gb/s network so that users can transfer the container images quickly and easily.",Chinese Academy of Sciences
NVIDIA GTC 2020,CTR Inference Optimization on GPU [s21416],"The CTR (click-through-rate) model is one of the most important models in internet businesses such as search, recommendation, and advertising. The performance of CTR online service has become a critical impact on user experience/enterprise revenue. With the development of deep learning technology, the CTR model began to adopt deeper and more complex DNN structure, and the computation scale and complexity continued to rise, which demanded more computing power. With the evolution of the CTR model in recent years and the promotion of NVIDIA GPU computing platform, more and more companies have begun to use NVIDIA GPU to accelerate the CTR online inference model, and achieved significant acceleration and got commercial benefits. We'll introduce how to profile and locate the issues when doing optimization CTR inference model on NVIDIA GPU, and provide the general methods on how to solve the issues and get satisfying speedup.",NVIDIA
NVIDIA GTC 2020,Deploying your Models to GPU with ONNX Runtime for Inferencing in Cloud and Edge Endpoints [s21379],"Models are mostly trained targeting high-powered data centers for deployment — not low-power, low-bandwidth, compute-constrained edge devices. There is a need to accelerate the execution of the ML algorithm with GPU to speed up performance. GPUs are used in the cloud, and now increasingly on the edge. And the number of edge devices that need ML model execution is exploding, with more than 5 billion total endpoints expected by 2022. ONNX Runtime is the inference engine for accelerating your ONNX models on GPU across cloud and edge. We'll discuss how to build your AI application using AML Notebooks and Visual Studio, use prebuild/custom containers, and, with ONNX Runtime, run the same application code across cloud GPU and edge devices like the Azure Stack Edge with T4 and low-powered device like Jetson Nano. We'll also demonstrate distribution strategies for those models, using hosted services like Azure IoT Edge. You'll take away an understanding of the various tradeoffs for moving ML to the edge, and how to optimize for a variety of specific scenarios.",Microsoft
NVIDIA GTC 2020,Edge Computing for Building Machine Learning Pipelines Using Azure Stack (Presented by Microsoft) [S22473],"Machine learning models need to be built closer to the source due to latency and data sovereignty requirements. Microsoft offers industry-leading hybrid cloud solutions, such as Azure Stack Hub and Azure Stack Edge in partnership with NVIDIA GPUs, to drive innovation and deliver a consistent cloud experience for these edge applications. Learn how to unlock customer-use cases by building production-scale models using data-science pipelines on the edge.",Microsoft
NVIDIA GTC 2020,Faster Transformer [s21417],"Recently, models such as BERT and XLNet, which adopt a stack of transformer layers as key components, show breakthrough performance in various deep learning tasks. Consequently, the inference performance of the transformer layer greatly limits the possibility that such models can be adopted in online services. First, we'll show how Faster Transformer optimizes the inference computation of both the transformer encoder and decoder layers. In addition to optimizations on the standard transformer, we'll get into how to customize Faster Transformer to accelerate a pruned transformer encoder layer together with the CUTLASS library.",NVIDIA
NVIDIA GTC 2020,From Hours to Minutes: The Journey of Optimizing Mask-RCNN and BERT Using MXNet [S22483],"Training large deep learning models like Mask R-CNN and BERT takes lots of time and compute resources. Using MXNet, the Amazon Web Services deep learning framework team has been working with NVIDIA to optimize many different areas to cut the training time from hours to minutes.",Amazon
NVIDIA GTC 2020,Google Cloud AutoML Video and Edge Deployment [S22022],"Google Cloud will be presenting AutoML Edge Video, a solution using NVIDIA GPUs. AutoML Edge Video allows users to train customized models without knowing how to tune parameters, using Google’s AutoML. In this talk we will focus on our end to end solution for Video Classification and Video Object tracking. To train a model, we use multiple NVIDIA GPUs in parallel for hyper-parameter tuning and transfer learning, allowing us to return high-performance models quickly. We will show how to export models to the edge using a frozen Tensorflow graph, and how to optimize for NVIDIA GPUs including Jetson and Tesla T4.",Google
NVIDIA GTC 2020,GradZip: Gradient Compression Using Alternating Matrix Factorization for Large-scale Deep Learning [P22289],"Our poster first explains the need for a dense and homomorphic algorithm for gradient compression in deep learning, in contrast to the conventional sparse-encoding-based approaches. Then, it details the thinking that came up with GradZip step by step, mainly modifying the existing matrix-factorization in the context of deep learning that reuses the latest all-reduce result as a part of alternating matrix-factorization. The final section captures the key performance result against the popular ResNet50 to demonstrate the effectiveness and efficiency of the proposed compression algorithm.",IBM
NVIDIA GTC 2020,Improve ML Training Performance with Amazon SageMaker Debugger (Presented by Amazon Web Services) [S22493],"During ML model training, it’s challenging to ensure that models are progressively learning the correct values for different parameters and to analyze and debug model characteristics without building additional tools, making the process time-consuming and cumbersome. With Amazon SageMaker Debugger, developers can get complete insights into the training process by automating data capture and analysis from training runs without code changes. We'll take a closer look at how you can define rules to monitor and analyze tensors and watch for issues in your model. By monitoring training flow, developers can improve GPU utilization, reduce troubleshooting time during training, and build high-quality models.",Amazon Web Services
NVIDIA GTC 2020,Is the Label Trustworthy? Training Better Deep-Learning Models via Uncertainty Mining Net [P22055],"In this work, we consider a new problem of training deep neural networks on partially labeled data with label noise. To our best knowledge, there have been very few efforts to tackle such problems. We present a novel end-to-end deep generative pipeline for improving classifier performance when dealing with such data problems. We call it Uncertainty Mining Net (UMN). During the training stage, we utilize all the available data (labeled and unlabeled) to train the classifier via a semi-supervised generative framework. During training, UMN estimates the uncertainly of the labels to focus on clean data for learning. More precisely, UMN applies the sample-wise label uncertainty estimation scheme. Extensive experiments and comparisons against state-of-the-art methods on several popular benchmark datasets demonstrate that UMN can reduce the effects of label noise and significantly improve classifier performance.",IBM
NVIDIA GTC 2020,Manifold Regularization For Time Series Embedding [P22063],"The wide use of multivariate time-series data has also brought up challenges for humans to parse the long sequences, in its raw form, to identify patterns, create labeled data for supervised modeling approaches, summarize, and explain cause and effect. Variational AutoEncoder (VAE) is a class of deep-generative models that have been used to learn a concise representation of the input data while minimizing information loss. However, when dealing with time-series data these models do not take into account the continuity in the input time-series data. In this work, we use the VAE model along with manifold regularization on the embeddings to learn a smooth representation for the time-series data that can be used to effectively visualize different patterns.",IBM
NVIDIA GTC 2020,Performance Optimization on Quantized Deep-Learning Models [S22484],"Quantization (8-bit) has been broadly adapted in computer vision-related deep-learning models for better inference performance. We present a set of techniques to speed up inference performance on a quantized model (8-bit). At graph level, we proposed a quantization-aware global layout transformation and graph optimization to minimize the data-layout transformation between 32-bit float and 8-bit integer. At the kernel level, we proposed an algorithm to fused the IM2COL with GEMM to save both the GPU memory usage and CUDA launch time caused by the process to generate IM2COL matrix. In addition, we proposed a double-buffering technique to improve the concurrency and reduce the data dependency. By comparing with cuDNN 7.1, our proposed method got up to 5x performance improvement.",Alibaba Group
NVIDIA GTC 2020,PyTorch from Research to Production [S21928],"Learn how to get your neural network from the PyTorch framework into production. Explore ways to handle complex neural network architectures during deployment. We'll show how to transform a neural network developed in PyTorch into a model ready for a production environment and exemplify the workflow on a conversational AI system. For full understanding, you should be familiar with PyTorch framework and have some interest in model deployment for inference. We’ll demonstrate the neural network system on TensorRT Inference Server (TRTIS).",NVIDIA
NVIDIA GTC 2020,qCUDA-ARM: Virtualization for Embedded GPU Architectures [P21851],"The emergence of the internet of things (IoT) is changing the ways to compute resource acquisition, from centralized cloud data centers to distributed pervasive edge nodes. We investigated two research trends to cope with the small amount of diversity problem for IoT devices and applications for the system design of edge nodes: heterogeneity and virtualization. Our poster presents the integration of those two important trends, and a virtualization system for embedded GPU architectures, called qCUDA-ARM. We evaluated the performance of qCUDA-ARM with three CUDA benchmarks and two-real world applications. For computationally intensive jobs, qCUDA-ARM can perform similar to the native system; for memory-bound programs, qCUDA-ARM can reach up to 90% of native performance.",National Tsing Hua University
NVIDIA GTC 2020,Recommendation Systems Using Distributed Graph Convolutional Networks (GCN) on GPUs (Presented by Amazon Web Services) [S22491],"Deep learning so far has been mostly applied to simple and structured data, such as images, or sequences, such as time-series and language. Most of the information in the world is, however, non-Euclidean and complex and can be presented as graph. Molecules, social network relatedness, and relationships between products and consumers are among the more popular examples. We'll represent an intuitive theory of Graph Convolutional Networks and provide a walkthrough on an implementation for recommender systems using Deep Graph Library and Apache MXNet.",Amazon Web Services
NVIDIA GTC 2020,SPTAG and GPU Acceleration for Approximate Nearest Neighborhood Search [s21561],"The Approximate Nearest Neighborhood (ANN) search algorithm is essential to many machine-learning applications. For instance, vector similarity search, multimedia search, and duplicate entry search all employ ANN for handling very large datasets efficiently. There are two main approaches to implementing ANN: space partitioning trees and locality sensitive hashing. Although hashing methods are accelerated on the GPUs (RAPIDS and FAISS), to our knowledge there is no GPU-accelerated space partitioning ANN algorithm in the literature. We'll present the GPU acceleration of the SPTAG library, where both space partition tree and neighborhood graph construction are accelerated on GPUs. We'll discuss the data structures and algorithms developed for efficient GPU implementation. Finally, we'll discuss the performance characteristics and compare the execution times against a single-socket GPU.",Principal Software Engineer Manager
NVIDIA GTC 2020,Visual Anomaly Detection using NVIDIA DeepStream with Azure IoT [S22675],"In this workshop, you'll discover how to build a solution that can process up to 8 real-time video streams with an AI model on a $100 device, how to remotely operate your device, and demonstrate how you can deploy custom AI models to it. With this solution, you can transform pixels from a camera into insights to know when there is an available parking spot, a missing product on a retail store shelf, an anomaly on a solar panel, a worker approaching a hazardous zone., etc. We'll build this solution using NVIDIA DeepStream on a NVIDIA Jetson Nano device connected to Azure via Azure IoT Edge. DeepStream is a highly-optimized video processing pipeline, capable of running deep neural networks. It is a must-have tool whenever you have complex video analytics requirements like real-time object detection or when employing cascading AI models. IoT Edge gives you the possibility to run this pipeline next to your cameras, where the video data is being generated, thus lowering your bandwidth costs and enabling scenarios with poor internet connectivity or privacy concerns. We'll operate this solution with an aesthetic UI provided by IoT Central and customize the objects detected in video streams using Custom Vision, a service that automatically generates computer vision AI models from pictures.",Microsoft
NVIDIA GTC 2020,Wide and Deep Recommender Inference on GPU [S21559],"We'll discuss using GPUs to accelerate so-called ""wide and deep"" models in the recommendation inference setting. Machine learning-powered recommender systems permeate modern online platforms. Wide and deep models have become a popular choice for recommendation problems due to their high expressiveness compared to more traditional machine learning models, and the ease with which they can be trained and deployed using Tensorflow. We'll demonstrate simple APIs to convert trained canned Tensorflow estimators to TensorRT executable engines and deploy them for inference using NVIDIA’s TensorRT Inference Server. The generated TensorRT engines can also be configured to enable reduced-precision computation that leverages tensor cores in NVIDIA GPUs. Finally, we'll show how to integrate these served models into an optimized inference pipeline, exploiting shared request-level features across batches of items to minimize network traffic and fully leverage GPU acceleration.",NVIDIA
NVIDIA GTC 2020,Accelerated Analytics Fit for Purpose: Scaling Out and Up (Presented by OmniSci) [S22556],"OmniSci has demonstrated the massive scaling possible using GPUs for computation and visualization. However, not every analytics problem or user persona requires massive scale; rather, our customers say they want proper-sized tools for the various problems they encounter across the enterprise. Our talk will outline the vision for scaling the OmniSci platform from trillions of records in a giant data store to hundreds of millions of records on a laptop, and every form factor in between. Whether you have a massive cluster of servers, a data science workstation, a GPU-enabled laptop, or even a CPU-only laptop, OmniSci can provide the same accelerated analytics experience appropriate for the problem at hand.",OmniSci
NVIDIA GTC 2020,Accelerating AI Workflows with NGC [S22421],"AI has moved beyond research into mission-critical production. AI is now solving real-world problems for organizations around the globe, who are looking to move faster and do more with their data. NVIDIA provides a range of SDKs that simplify training, inference, and deployment of AI for industries including health care, smart cities, robotics, and telecommunications. We'll cover how NGC, through containers, pre-trained models, helm charts, and SDKs, allows data scientists and developers to build AI solutions faster, DevOps to streamline the development-to-production process, and IT teams to quickly provide compute platforms that the users need. We'll demo how you can take advantage of an SDK to easily build and deploy your AI solution on-premises, at the edge, or in the cloud.",NVIDIA
NVIDIA GTC 2020,Accelerating Applications for the NERSC Perlmutter Supercomputer Using OpenMP [s21387],"Learn about the NERSC/NVIDIA effort to support OpenMP target offload on the forthcoming NERSC-9 Perlmutter supercomputer with next-generation NVIDIA GPUs. NVIDIA's HPC compilers for C, C++, and Fortran will support a subset of OpenMP 4.5/5.0 for GPU offload on Perlmutter. We'll review the primary features included in this subset, walk through the reasons some features were included and others were left out, and highlight the characteristics that make some OpenMP applications better candidates for GPU acceleration than others. Selected teams from the NERSC Exascale Science Applications Program (NESAP) are porting their applications to NVIDIA GPUs using this subset of OpenMP for target offload. We'll share their early experiences and those of other NERSC developers working with early releases of NVIDIA's OpenMP-offload-enabled compilers.",Lawrence Berkeley National Laboratory
NVIDIA GTC 2020,Accelerating Chemistry Modules in Atmospheric Models Using GPUs [S22005],"We’ll explore a novel solution to speed up chemistry modules in atmospheric models. The Multiscale Online Nonhydrostatic AtmospheRe CHemistry model (MONARCH), a chemical weather prediction system developed by the Barcelona Supercomputing Center, is used as our test bed. The model implements a new flexible treatment for gas- and aerosol-phase chemical processes, Chemistry Across Multiple Phases (CAMP), that allows multiple chemical processes (such as gas- and aerosol-phase chemical reactions, emissions, deposition, photolysis, and mass-transfer) to be solved simultaneously as a single system. We’ll discuss innovative ways to speed up the CAMP module, including reducing memory accesses, multiple-cell chemistry solving, adaptation of the most time-consuming functions to GPUs, and heterogeneous computation approaches for CPU/GPU execution. We'll compare the optimized model to state-of-the art chemistry solvers commonly used in the earth sciences community (for example, EBI and KPP).",Barcelona Supercomputing Center
NVIDIA GTC 2020,Accelerating Hyperparameter Tuning with Container-Level GPU Virtualization [S21463],"Many people think that hyperparameter tuning requires a large number of GPUs to get optimal results quickly. That's generally true, but to what extent? We'll present what we've learned about finding a sweet spot to balance both costs and accuracy by exploiting partitioned GPUs with Backend.AI's container-level GPU virtualization. Our benchmark includes distributed mnist, cifar10 transfer learning and TGS salt identification cases using AutoML with network morphism and ENAS tuner with NNI running on NGC-optimized containers on Backend.AI. You'll get a tangible guide to plan and deploy your GPU infrastructure capacity in a more cost-effective way.",Lablup Inc.
NVIDIA GTC 2020,Acceleration of Test Data Quality Assurance Technology Using Neuron Coverage [P21809],"“Neuron coverage test” is one way to test quality in deep-learning systems. We've implemented “DeepXplore,” which is a quality test method for DL systems that applies our unique high-speed inference library. DeepXplore is a framework that can systematically test DL systems and automatically generate misrecognition images from the coverage rate of the trained model and the output difference of the comparison model. However, a large amount of inference would be necessary to automatically generate these misrecognition images, and that could require a lot of processing time. We sped up processing by using our unique high-speed inference library to implement DeepXplore.",Computermind Corp.
NVIDIA GTC 2020,AceCAST GPU-Enabled Weather Research and Forecasting Model Development and Applications [P22064],"The Weather Research and Forecasting (WRF) model is an open-source, mesoscale numerical weather prediction system designed to serve both operational forecasting and atmospheric research needs. It is the most widely used regional weather forecasting model and a top 5 HPC application worldwide. TQI has implemented an OpenACC/CUDA-based version of WRF to take advantage of NVIDIA GPUs. By utilizing GPUs, the measured performance benefits enable better forecasting through higher resolution, temporal/geographical extents, and so on. Our poster discusses the GPU implementation of the model as well as performance benchmarks demonstrating the model’s practical performance benefits. TQI has also developed a cloud-based solution for running end-to-end AceCAST GPU-WRF workflows on AWS. This provides a solution for a wide range of users who would otherwise not have access to GPU-based compute resources, and automates a highly complex process that is a significant barrier for researchers and operational weather forecasters.",TempoQuest Inc.
NVIDIA GTC 2020,Advanced Scientific Visualization with NVIDIA Omniverse [S21973],"Historically, scientific visualization has served two main purposes: on one hand, for the scientist to gain insight into the data and the processes under investigation; on the other hand, for outreach and education, to communicate scientific results to the public and to funding bodies. The tools associated with these workflows are hugely diverse, often preventing scientists from getting access to the highest quality visuals for telling their stories. Especially in times where the general public has been spoiled with computer-generated content in movies, the gap in visual quality between scientific content and entertainment content has never been bigger. Tools that seamlessly fit into the scientific workflow while being capable of producing the highest quality visuals are therefore needed. We'll outline how Omniverse, NVIDIA’s collaboration platform for 3D content creation, can be leveraged to greatly simplify, accelerate, and enhance scientific visualization. We'll introduce the overall Omniverse architecture and then focus on the scientific workflow. We cover how content can be ingested into Omniverse, as well as how content from different sources can be fused and how to produce rich, high-quality visualizations. We'll then show how to augment these visualizations with the embedded real-time physics engine PhysX. Further, we'll demonstrate how, through Omniverse, virtual reality setups that incorporate scientific visualizations can be defined quickly and easily.",NVIDIA
NVIDIA GTC 2020,Advances in Real-Time 3D Hologram Generation [S21248],"We'll introduce VividQ's software pipeline, which facilitates the extraction of 3D point cloud data and conversion to a hologram to be displayed using diffractive optics. This allows for the real-time projection of scenes as truly 3D holographic images. For example, video games can now be viewed in true 3D with full depth of focus. We'll explain why this problem is computationally hard, and how using NVIDIA GPUs allows us to overcome it. Recent advances in hologram generation procedures will be explored, including benefits from newer GPU models and the application of multiple GPUs to computing for a binocular holographic system. We'll also discuss results from VividQ's latest holographic augmented reality head-mounted display prototype.",VividQ
NVIDIA GTC 2020,A Faster Radix Sort Implementation [S21572],"We'll present a faster implementation of the least-significant-digit radix sort. Decoupled look-back is used to reduce the number of memory operations per partition pass from 3N to 2N. A faster partition implementation inside the thread block is used. For 32-bit keys, we use four partition passes, with 8 bits per digit. On V100 sorting 64 million random UInt32 keys, our implementation achieves the speed of 16 Gkey/s, which is more than 2x faster than cub::DeviceRadixSort.",NVIDIA
NVIDIA GTC 2020,AIaaS: Scaling AI Infrastructure for the Enterprise from DGX-1 to SuperPOD [S21996],"Are you getting the most out of your GPU-accelerated hardware? AIaaS (AI-as-a-Service) deployments provide powerful new ways for enterprises to stand up AI infrastructure on top of GPU-accelerated servers (such as NVIDIA DGX Servers). Data scientist users can interact with the cluster via GUI or a simplified command-line interface, where knowledge of underlying container, orchestration, and scheduling technologies is obfuscated. Such systems allow administrators to maximize use of resources and users to get more work done. We'll focus on the variety of AIaaS stacks, considerations, and how to deploy them. We'll demonstrate scaling common AI workloads from a single GPU on an NVIDIA DGX-1 to a SuperPOD cluster (64x DGX-2). We'll also discuss methods to integrate storage and manage datasets, tie in authentication and authorize users, train on NGC containers, and deploy models to production.",NVIDIA
NVIDIA GTC 2020,AI Argus: A Unique Insight Into Logistics [P21812],"As of August 2019, AI Argus, the leading domestic intelligent video analytics platform powered by NVIDIA TeslaT4 and Xavier servers, has been deployed and applied in more than 200 distribution centers and sorting centers in China. We'll focus on algorithms such as loading-rate detection and violated-action pattern detection. AI Argus introduced the Package Lifecycle Tracking System (PLTS), which uses the whole SF's 310,000-channel camera video data to match the Operator's barcode data collector for locating the courier package on each operating node. It shows that the detection of damage caused by penetrating damage, moisture, wrinkles/pressure is significant. We'll further develop and research product performance indicators based on T4 and Xavier. We expect that in the coming year, AI Argus will deploy thousands of edge servers and become the first truly large-scale edge computing platform in China's logistics industry.",SF Techology
NVIDIA GTC 2020,AI/ML with vGPU on Openstack or RHV Using Kubernetes [S22106],"By sharing GPU resources for AI/ML, you can better utilize on-premises hardware and gain flexibility without moving sensitive workflows into the cloud. Learn how Red Hat Openstack Platform and Red Hat Virtualization are bringing agility to AI/ML accelerated workloads with vGPU. We'll describe how Red Hat contributes to new vGPU capabilities like SR-IOV and live migration support. This makes setting up AI/ML within a Kubernetes system even simpler and more reliable, as the workloads can be migrated and SR-IOV functionality boosts the performance and usability of the vGPU device.",Red Hat
NVIDIA GTC 2020,An Overview of GPU Recurrent Neural Network Performance [S21301],"We'll focus on performance of recurrent neural networks (RNNs) at the sub-framework level. There are several distinct methods one can use to implement an RNN, each with advantages and disadvantages, and the right choice will depend on both the target GPU and the network hyperparameters. We'll cover these methods in detail and give approximate hyper-parameter ranges for each method.",NVIDIA
NVIDIA GTC 2020,AutoFAQ: Automation of Customer Support for Most Common Questions [s21252],"In most support systems, up to 70% of user questions are very similar to each other. Instead of manually answering each of them, it makes sense to automate it. We'll discuss the business value of such systems, and how to integrate them into processes; how to develop an architecture for automating question-answering; how to set up a training loop, and which algorithms to choose; which models need to be trained, and why; and state-of-the-art language modeling models. Knowledge of Python, NLP, language models, and cloud computing will be helpful, but it's not essential.",Poteha Labs
NVIDIA GTC 2020,Building a Smart Language-Understanding System for Conversational AI with HuggingFace Transformers [S22647],"In this session, HuggingFace showcases an example of a natural language understanding pipeline to create an understanding of sentences, which can then be used to craft a simple rule-based system for conversation. They'll leverage the famous HuggingFace transformers and showcase the powerful yet customizable methods to implement tasks such as sequence classification, named-entity recognition, natural language generation, or question answering. These tasks will be joined to create a basic NLU pipeline to get the most out of a sentence or text, using transformer models such as BERT to provide state-of-the-art results. These methods leverage the PyTorch or TensorFlow numerical computation frameworks, which can leverage the power of GPUs to radically speed up the inference.",Inc.
NVIDIA GTC 2020,CLOUDXR: Streaming AR and VR [S22178],"Learn how the NVIDIA CloudXR SDK can help you drive immersive extended reality (XR) experiences from anywhere using NVIDIA Quadro GPUs. NVIDIA CloudXR is the groundbreaking technology that delivers wireless virtual and augmented reality from NVIDIA RTX GPUs across performant networks. By dynamically adjusting to network conditions, CloudXR maximizes image quality and frame rates. Scale XR capabilities throughout your enterprise by combining CloudXR with NVIDIA GPU virtualization software to provide seamless experiences comparable to the most robust tethered configurations. Included in this presentation will be examples of partner ISVs extending the OpenVR applications into XR streaming applications.",NVIDIA
NVIDIA GTC 2020,Combined Python/CUDA JIT for Flexible Acceleration in RAPIDS [S21393],"We'll introduce our design and implementation of a framework within RAPIDS/cuDF that enables compiling Python user-defined functions and inlining them into native CUDA kernels. Our framework uses the Numba Python compiler and Jitify CUDA just-in-time (JIT) compilation library to provide cuDF users the flexibility of Python with the performance of CUDA as a compiled language. An essential part of the framework is a parser that parses the CUDA PTX function, which is compiled from the Python UDF, into an equivalent CUDA device function that can be inlined into native CUDA C++ kernels. Learn how our approach makes it possible for non-expert Python users to extend optimized dataframe operations with their own Python UDFs, and enables more flexibility and generality for high-performance computations on dataframes in RAPIDS.",NVIDIA
NVIDIA GTC 2020,CuPy Overview: NumPy Syntax Computation with Advanced CUDA Features [S22471],"We'll introduce CuPy, describing the advantages of the library and how it is cleanly exposing in Python multiple CUDA state-of-the art libraries such as cuTENSOR or cuDNN. Discover the CuPy advantages and how they can use it to experience performance gains in their NumPy codes without any major changes.",Preferred Networks
NVIDIA GTC 2020,Distributed Deep Learning with Horovod on Spark [s21300],"We'll show how to scale distributed training of TensorFlow, PyTorch, and MXNet models with Horovod, a library designed to make distributed training fast and easy to use. We'll explain the role of Horovod in taking a model designed on a single GPU and training it on a cluster of GPU servers with just a few additional lines of Python code. We'll also explore how Horovod has been used across the industry to scale training to hundreds of GPUs, and the techniques that are used to maximize training performance. Although frameworks like TensorFlow and PyTorch simplify the design and training of deep learning models, difficulties usually arise when scaling models to multiple GPUs in a server or multiple servers in a cluster.",Uber Technologies
NVIDIA GTC 2020,Document Understanding Platform: Extracting Structured Information from Financial Documents [S21459],"In today’s highly automated world of financial services, consumers, the self-employed, and small business owners still face the tedious and time-consuming task of entering data manually from paper documents. Document Understanding is a company-wide initiative at Intuit that aims to make data preparation and entry obsolete through the application of computer vision and machine learning. We'll describe the design and modeling methodologies used to build this platform-as-a-service. Intuit’s Document Understanding Platform orchestrates a variety of services and machine-learning capabilities using structured and unstructured documents uploaded by users, regardless of format (smartphone photos, PDFs, forms, etc.), and presents high-confidence results back within the company’s product ecosystem.",Intuit
NVIDIA GTC 2020,Efficient 3D Convolutional Network Design for Human Instance-Level Video Action Recognition [P22275],"We discuss the design factors of the human instance-level video action recognition, especially on edge computing hardware like a Jetson AGX Xavier embedded module. First, we present our research motivation in human action recognition fields. Second, we talk about our pipeline for human instance-level video action recognition. Third, we explain how to enhance the accuracy using person detectors such as YOLOv3 and Mask RCNN. Finally, we discuss how to utilize action recognition models in terms of accuracy to action inference speed ratio on the Xavier module, which is very useful in practice.",Artificial Intelligence Research Institute (AIRI)
NVIDIA GTC 2020,Espresso: A Fast End-to-End Neural Speech Recognition Toolkit [s21239],"We'll introduce Espresso, an open-source, modular, extensible, end-to-end neural automatic speech recognition (ASR) toolkit based on the deep learning library PyTorch and the popular neural machine translation toolkit fairseq. Espresso supports distributed training across GPUs and computing nodes, and features various decoding approaches commonly employed in ASR, including look-ahead word-based language model fusion, for which a fast, parallelized decoder is implemented. Espresso achieves state-of-the-art ASR performance on the WSJ, LibriSpeech, and Switchboard datasets, among other end-to-end systems without data augmentation, and is up to 11x faster for decoding than similar systems, such as ESPnet.",Johns Hopkins University
NVIDIA GTC 2020,Evaluation of a Multi-GPU Optimized Non-Hydrostatic Ocean Model with Multigrid Preconditioned Conjugate Gradient Method [P21930],"The conjugate gradient method with multigrid preconditioners (MGCG) is used in scientific applications because of its high performance and scalability with many computational nodes. GPUs are thought to be good candidates for accelerating such applications, with many meshes where an MGCG solver could show high performance. No previous studies have evaluated and discussed the numerical character of an MGCG solver on GPUs. Consequently, we implemented and optimized our “kinaco” numerical ocean model with an MGCG solver on GPUs. We evaluated its performance and discussed inter-GPU communications on a coarse grid on which GPUs could be intrinsically problematic. We achieved 3.9x speedup compared to CPUs, and learned how inter-GPU communications depend on the number of GPUs and the aggregation level of information in a multigrid method.",RIST
NVIDIA GTC 2020,EXtended Particle System (XPS): High-Performance Particle Simulation [P21775],"We'll present a framework for a GPU-based discrete element method solver for real-world particle problems featuring coupling ability with a CPU-based computational fluid dynamics solver provided by one of our partners. Current features include support of non-spherical shapes like multi-sphere particles and true bi-convex tablets, plus heat transfer and liquid transfer. We recently integrated a smoothed-particle hydrodynamics solver, as well as support for polyhedral-shaped solid particles.",RCPE GmbH
NVIDIA GTC 2020,Exterminating Buffer Overflows and Other Embarrassing Vulnerabilities with SPARK Ada on Tegra [S21122],"Since 2018, NVIDIA has been actively investigating the SPARK Ada programming language to develop their most sensitive pieces of firmware. We'll explain how users of the NVIDIA hardware can also benefit from this language choice when developing applications for the Tegra SoC. The benefits of the technology, from a cyber security point of view, will be demonstrated through the use of formal methods, allowing trivial proof of properties, such as absence of buffer overflows. We'll describe using this technology on top of ARM processor cores, as well as methodologies for applications leveraging the GPU, either through existing libraries interfaces/CUDA code, or through an experiential port of Ada/OpenACC, which allows applications directly written in Ada or SPARK to offload to the GPU.",AdaCore
NVIDIA GTC 2020,Fast Distributed Joins with RAPIDS and UCX [s21482],"There are numerous optimized single-GPU join implementations (such as RAPIDS cuDF), but scaling out to multiple GPUs across multiple nodes is challenging. The repartitioned join approach is one of the most popular distributed join algorithms, featuring all-to-all exchange as the main communication pattern. We'll show how to leverage UCX for efficient all-to-all implementation and demonstrate various optimization strategies, such as reusing communication buffers to speed up GPU-to-GPU transfers and overlapping compute with communications. The implementation is designed to reuse RAPIDS components for single-GPU, and scales to NVLINK and Infiniband systems. Our latest performance results demonstrate that a single DGX-2 can achieve 220 GB/s throughput for joining 8B/8B key-value pairs, while 18 DGX-1V nodes (144 GPUs) connected over IB achieve 503 GB/s, which is comparable with 244 CPU nodes (2K cores) in the best-known distributed CPU implementation.",NVIDIA
NVIDIA GTC 2020,From High to Low Level: A Comparative Study of Programming Approaches for NVIDIA GPUs [S21308],"Learn about various available methods to program NVIDIA GPUs, from using high-level GPU libraries in Python to optimized CUDA C programming. We'll discuss the development and performance of several implementations of software to simulate the 2D Ising model for spin systems, comparing the different programming approaches in terms of development effort and simulation performance. We'll show how Python, in combination with the Numba/CuPy packages, enables users to write programs, with all the productivity benefits of a high-level language, that can still provide competitive performance to lower-level implementations. We'll also highlight some performance pitfalls encountered with these tools and discuss how we addressed them. Finally, we'll compare performance against published results on other hardware platforms and show that even simple programming methods on GPUs can provide competitive performance, while our optimized low-level implementation can rapidly simulate lattices sizes outside the scope of comparable field-programmable gate array solutions.",NVIDIA
NVIDIA GTC 2020,GPipe: Efficient Training of Giant Neural Networks Using Pipeline Parallelism [S21873],"Scaling up deep neural network capacity is an effective way to improve model quality for several different machine learning tasks. In many cases, increasing model capacity beyond the memory limit of a single accelerator has required developing special algorithms or infrastructure. These solutions are often architecture-specific, and do not transfer to other tasks. To address the need for efficient and task-independent model parallelism, we'll introduce GPipe, a pipeline parallelism library that allows scaling any network that can be expressed as a sequence of layers. By pipelining different sub-sequences of layers on separate accelerators, GPipe provides the flexibility of efficiently scaling a variety of different networks to gigantic sizes. Moreover, GPipe utilizes a novel batch-splitting pipelining algorithm, resulting in almost linear speedup when a model is partitioned across multiple accelerators.",Google
NVIDIA GTC 2020,How to Build a Multi-Camera Media Server for AI Processing on Jetson [S22396],"We'll build a simple multi-camera media server for AI processing on a Jetson Board and demonstrate how, by using GStreamer Daemon, Interpipes, and DeepStream, you can develop a scalable and robust prototype to capture from multiple cameras using GMSL2 Virtual Channels. Besides achieving real-time deep learning inference, the server is completely dynamic. We'll show its flexibility by triggering actions such as taking a snapshot, recording a video, or starting a network streaming when a specific prediction is made. By the end of this session you'll have acquired the framework basics that will allow you to scale to your specific multimedia and AI needs.",RidgeRun
NVIDIA GTC 2020,Knowledge Transfer Graph for Deep Collaborative Learning [P22383],"We present a new graph-based approach for more flexible and diverse combinations of knowledge transfer for deep collaborative learning. To achieve the knowledge transfer, we propose a novel graph representation called knowledge transfer graph that provides a unified view of the knowledge transfer and has the potential to represent diverse knowledge transfer patterns. We also propose four gate functions that are introduced into loss functions. The four gates, which control the gradient, can deliver diverse combinations of knowledge transfer. Searching the graph structure enables us to discover more-effective knowledge transfer methods than a manually designed one. Experimental results show that the proposed method achieved significant performance improvements and was able to find remarkable graph structures.",Chubu University
NVIDIA GTC 2020,Learning Human Objectives by Evaluating Hypothetical Behavior [P22312],We present ReQueST: an algorithm for training reinforcement learning agents from human feedback in the presence of unknown unsafe states.,Berkeley
NVIDIA GTC 2020,Modularizing Natural Language Processing [S21560],"Recent success and growth in natural language processing and artificial intelligence have given the world many new applications, techniques, models, and architectures. We'll show how appropriate abstraction and modularization can streamline both development and deployment of NLP technologies. We'll provide a systematic overview of NLP abstractions and breakdown, the insights of machine learning integration, and the designs of NLP systems for fast module development. You'll learn to use off-the-shelf tools to practice the modularized NLP and build practical applications. Our talk is suitable for researchers and practitioners with an intermediate understanding of NLP and ML concepts and applications, and a strong interest in real-world NLP application design and development.",Carnegie Mellon University
NVIDIA GTC 2020,Motion Reasoning for Goal-Based Imitation Learning [P22363],"We address goal-based imitation learning, where the aim is to output the symbolic goal from a third-person video demonstration. This enables the robot to plan for execution and reproduce the same goal in a completely different environment. The key challenge is that the goal of a video demonstration is often ambiguous at the level of semantic actions. The human demonstrators might unintentionally achieve certain subgoals in the demonstrations with their actions. Our main contribution is to propose a motion reasoning framework that combines task and motion planning to disambiguate the true intention of the demonstrator in the video demonstration. This allows us to robustly recognize the goals that cannot be disambiguated by previous action-based approaches.",Stanford University
NVIDIA GTC 2020,"Named Tensors, Model Quantization, and the Latest PyTorch Features [S22145]","PyTorch, the popular open-source ML framework, has continued to evolve rapidly since the introduction of PyTorch 1.0, which brought an accelerated workflow from research to production. We’ll deep dive on some of the most important new advances, including the ability to name tensors, support for quantization-aware training and post-training quantization, improved distributed training on GPUs, and streamlined mobile deployment. We’ll also cover new developer tools and domain-specific frameworks including Captum for model interpretability, Detectron2 for computer vision, and speech extensions for Fairseq.",Facebook
NVIDIA GTC 2020,Opening Up the Black Box: Model Understanding with Captum and PyTorch [S22147],"PyTorch, the popular open-source ML framework, has continued to evolve rapidly since the introduction of PyTorch 1.0, which brought an accelerated workflow from research to production. We’ll deep dive on some of the most important new advances, including the ability to name tensors, support for quantization-aware training and post-training quantization, improved distributed training on GPUs, and streamlined mobile deployment. We’ll also cover new developer tools and domain-specific frameworks including Captum for model interpretability, Detectron2 for computer vision, and speech extensions for Fairseq.",Facebook AI
NVIDIA GTC 2020,Optimized Image Classification on the Cheap [s21598],"We'll anchor on building an image classifier trained on the Stanford Cars dataset to evaluate two approaches to transfer learning — fine tuning and feature extraction — and the impact of hyperparameter optimization on these techniques. Once we define the most performant transfer-learning technique for Stanford Cars, we'll explore Bayesian Optimization as a black-box optimization technique to tune image-transformation parameters required to augment the model, using the downstream image classifier’s performance as the guide. Drawing on a rigorous set of experimental results can help us answer the question: How can resource-constrained teams make tradeoffs between efficiency and effectiveness using pre-trained models?",SigOpt
NVIDIA GTC 2020,Optimizing Work Scheduling and Memory Usage for Complex Database Queries on GPUs [P22017],"The growth of data volumes of online analytical processing systems has made the GPU an attractive platform for executing data analytical queries. Besides the large data sizes, most of the real-world analytical queries are very complex, with multiple joins performed across multiple tables. In this session, we'll show how CUDA Graphs and RAPIDS cuDF components enable a much easier acceleration of complex queries on the GPU. In addition to programmability improvements, our design achieves better utilization of system resources for complex queries, such as optimized data pipeline, maximized task parallelism, and more efficient memory management. Moreover, we compare CUDA Graphs approach with the traditional approach using multiple streams and showcase how we achieve temporary memory reusing across different joins.",NVIDIA
NVIDIA GTC 2020,Optuna: An Eager Hyperparameter Optimization Library [s21291],"We'll present Optuna, an open-source, next-generation hyperparameter optimization framework with three novel design criteria: (1) an eager API that allows users to concisely construct dynamic, nested, or conditional search spaces; (2) efficient implementation of both sampling and early-stopping strategies; and (3) an easy-to-set-up, versatile architecture that can be deployed for various purposes, ranging from simple scaling on distributed GPUs to lightweight experimentation, conducted via interactive interface. Optuna is the first optimization software designed to run in Eager mode. We'll present the basic usage of Optuna, describe the design techniques that we needed to meet the above criteria during development, and share our experiences leveraging Optuna to tune hyperparameters on single and multiple GPUs.",Preferred Networks
NVIDIA GTC 2020,Scaling Deep Learning for Automatic Speech Recognition [S21838],"We'll discuss challenges of scaling automatic speech recognition (ASR) workloads with wav2letter++, a fast C++ toolkit for ASR. We'll introduce distributed training techniques used to achieve almost linear scalability and compare wav2letter to other popular ASR toolkits. Constant increase in model and dataset sizes, along with current trends toward unsupervised and semi-supervised learning, require squeezing out every bit of performance. In addition to distributed training, we'll cover other approaches for faster training and for training large models.",Facebook
NVIDIA GTC 2020,Software-Based Compression for Analytical Workloads [S21597],"Real-world analytical pipelines have very large memory requirements and stress the CPU-GPU and GPU-GPU interconnects. The GPU memory size is limited, and the data is often offloaded to CPU memory for further processing on the GPU later. That can present a significant bottleneck for the end-to-end pipeline. Fast compression and decompression can improve performance by reducing the amount of data to be sent over the interconnect, or even completely eliminate the need to offload data by storing it in GPU memory in compressed format and performing subsequent operations on the compressed data. We'll survey various parallel compression algorithms, from LZ-based to run-length encoding, dictionary, and bit-packing. Efficient GPU implementations of cascaded schemes for analytical workloads will be presented, along with user-friendly interfaces for GPU applications. Our best approaches can achieve up to 77x compression ratio for columns from Fannie Mae's Loan Performance dataset, and maintain 250-350GBps compression/decompression speed on Tesla V100.",NVIDIA
NVIDIA GTC 2020,"The Future of GPU Rendering: Real-Time Raytracing, Holographic Displays, and Light Field Media [S22153]","We'll present our vision for the future of GPU rendering and how it will impact gaming, VFX, media, and design in the 2020s and beyond. We'll detail how the future of media lies in holographics, light field technologies, and real-time rendering, and how OTOY is working to help drive that future through OctaneRender.",OTOY Inc
NVIDIA GTC 2020,The SpeechBrain Project [s21648],"SpeechBrain is an open-source project that aims to develop an all-in-one speech toolkit based on PyTorch. Our goal is to create a single, flexible, user-friendly toolkit that can be used to easily develop state-of-the-art speech technologies. SpeechBrain will be a standalone framework that can significantly speed up research and development of speech and audio processing techniques. Indeed, it's a lot easier to familiarize oneself with a single toolkit than to learn several different frameworks, as one must do today. Moreover, using a single platform makes it easier to build a strong and fruitful community where members can share models, codes, baselines, and suggestions with a possible positive impact in the field of speech technologies. SpeechBrain is currently under development, and a first alpha version will be available in the next months. We'll describe the motivations, goals, and current status of the project.",Mila - Universit de Motral
NVIDIA GTC 2020,TNL: Template Numerical Library for Modern Parallel Architectures [P21800],"We'll present a numerical library that's being developed int our department. The library's goal is to offer flexible data structures and algorithms for the high performance computing, and especially for GPUs. Its design benefits strongly from the modern features of C++. In many situations, the user of TNL may write code independently on the hardware platform. The library is available at www.tnl-project.org.",Czech Technical University in Prague
NVIDIA GTC 2020,Toward Industrial LES/DNS in Aeronautics: Leveraging OpenACC for Massively Parallel CPU+GPU Simulations [s21958],"We'll describe recent advances toward industrial LES/DNS computational fluid dynamics within the scope of the EU TILDA (Towards Industrial LES/DNS in Aeronautics) project. The TILDA project aims to complete high-fidelity industrial LES/DNS simulations with upwards of 1 billion degrees of freedom, with a turnaround time on the order of one day. Achieving this requires near-linear efficiency on massively parallel, heterogeneous CPU+GPU compute resources. We'll describe the development of FineFR, a high-order CFD solver supporting heterogeneous CPU+GPU architectures. We'll emphasize the highly tuned OpenACC implementation, allowing very efficient data locality with minimal code intrusion. Finally, we'll present benchmark data and demonstration computations from the OLCF Summit Supercomputer showing near-linear scalability on upwards of 50,000 CPU cores and 7,000 NVIDIA GPUs.",Numeca
NVIDIA GTC 2020,Toward INT8 Inference: Deploying Quantization-Aware Trained Networks using TensorRT [s21664],"We'll describe how TensorRT can optimize the quantization ops and demonstrate an end-to-end workflow for running quantized networks. Accelerating deep neural networks (DNN) is a critical step in realizing the benefits of AI for real-world use cases. The need to improve DNN inference latency has sparked interest in lower precision, such as FP16 and INT8 precision, which offer faster inference time. Two prevalent techniques to convert FP32 DNNs to INT8 precision are post-training quantization and quantization-aware training (QAT). TensorRT, a platform for high-performance deep learning inference, supports post-training quantization by performing calibration on the trained model, which quantizes the weights and activations. However, in some cases post-training quantization can degrade accuracy when converting a FP32 model to its INT8 counterpart. QAT introduces quantization ops to achieve higher accuracy by simulating the process for lower-precision quantization during training.",NVIDIA
NVIDIA GTC 2020,Training and Deploying Conversational AI Applications with NeMo and Jarvis [S21211],"Learn how to build speech recognition, natural language processing, and speech synthesis services with Neural Modules. First, we'll cover the basics of the NeMo toolkit for training and fine-tuning conversational AI models on your data. Then, we'll discuss how to use Jarvis to deploy and combine these services into a complete conversational AI solution.",NVIDIA
NVIDIA GTC 2020,XLNet Optimization Using CUDA [S21478],"XLNet, a generalized autoregressive pretraining method, achieved great results on several natural language processing tasks. Compared to the previous language model, XLNET has advantages like being able to process long sentences, and avoids the disadvantage of using special tokens. However, as far as we know, there still isn't proper performance optimization for XLNet using CUDA, which would demand more inference time and hinder XLNET's wide deployment. We first ran the performance analysis of XLNet using its Tensorflow code. Then we optimized XLNet with these aspects:
For relative positional encoding, we optimized its parallelization with the help of cuBlas;
We customized the corresponding self-attention architecture based on the attention code in FastTransformer; and
We used kernel fusion and other CUDA optimization strategies to speedup XLNet.",NVIDIA
NVIDIA GTC 2020,3D Analysis Data Generation from X-ray Data for HealthTech by DGXs [P22304],"In Japan, research on regenerative medicine is advancing, various levels of treatment are advancing, and laboratories dealing with nearly 1,000 regenerative medicine cells are starting to appear every week. Research in these regenerative treatments requires new medical imaging—to increase the resolution of two-dimensional image data, for example. Rather than discovering new indicators that map annotations, it is easier to use existing indicators or to generate images for use. Currently, a 3D image is generated from a 2D image by X-ray, and a 3D curved surface learned by a Tesla chip is developed to generate a realistic 3D image. By generating even sliced images, we provide digital data materials that can be used as reference for proceeding to MRI after X-ray imaging.",Inc.
NVIDIA GTC 2020,Accelerate Quantitative Clinical Neuroimaging Analysis with AI: Steps Toward Personalized Disease Progression Monitoring in Clinical Neurology [S21421],"Quantitative neuroimaging analysis can further extract critical information from diagnostic imaging. For instance, brain-volume change has been considered as a critical biomarker in neurodegenerative disease progression. Instead of describing the brain shrinking process as ""moderate"" or ""severe"" through visual inspection, now we can, with computing, precisely measure the brain-tissue loss at the scale of 0.1% with conventional MRI scans. However, precision and accuracy of the analysis often require an expert level of quality control, usually carried out by certified imaging analysts at dedicated imaging reading centers. Deep learning and GPU acceleration make it possible to transfer complex and sophisticated analysis into fully automated assessment in daily clinics. We'll share views from both imaging scientists and clinicians on how AI is accelerating clinical imaging quantitative biomarker research.",Sydney Neuroimaging Analysis Centre
NVIDIA GTC 2020,"Accelerating Cancer Research: VDI by Day, Compute by Night [S21845]","The Netherlands Cancer Institute–Antoni van Leeuwenhoek Hospital (NKI-AVL) is one of the top 10 comprehensive cancer centers in the world. By combining cancer care, research, and by exchanging knowledge internationally, they make a significant contribution to solving the cancer problem in the 21st century. To meet that challenge, NKI-AVL built a software-defined infrastructure to accelerate research and enhance efficiency for clinicians. During daytime, the VDI infrastructure will give health care professionals fast, remote, and secure access to patient data. At night, the same VDI platform is utilized by researchers to execute computational GPU workloads. As a result, the high-performance and flexible IT infrastructure enables physicians and nurses to spend more focused time on patient care, and researchers to advance new discoveries in cancer treatment.",Antoni van Leeuwenhoek Hospital and Netherlands Cancer Institute
NVIDIA GTC 2020,Accelerating Quantum Chemistry Simulations with AI [S21273],"We'll discuss computational chemistry applications of machine learning covering three topics. First, we'll examine the use of neural networks and other machined-learned methods for describing a quantum-accurate potential energy surface. Second, we'll cover graph convolution neural networks and graph message-passing networks for predicting molecular properties at a fraction of the cost of traditional electronic structure calculations. Third, we'll discuss variational autoencoders for molecule discovery and illustrate their application to drug discovery.",NVIDIA
NVIDIA GTC 2020,Acceleration of Wavefront Analysis Using Zernike Polynomial Fitting [P21849],"Zernike polynomial fitting can analyze the wavefront map of biological cells with uniform contents. Aberrations not only indicate changes in the wavefront after passing through cells, but also provide angular position for tomogram reconstruction. However, Zernike polynomial fitting for wavefront analysis is time-consuming. To raise efficiency, we implemented Zernike polynomial fitting on a GPU card. We simulate a sequence of red blood cell phase maps with various rotation angles, and estimate the angles using the coefficients of defocus, vertical, and horizontal tilts from Zernike analysis. The error of angle estimation is under 0.7%, and we achieve video-rate processing at 128×128 pixels. This shows the possibility for implementing wavefront analysis on a GPU card and its benefit on tomogram reconstruction.",National Taiwan University
NVIDIA GTC 2020,Accurate Multiple Sclerosis Lesion Segmentation Using Deep Learning [P21889],"We present results of a comparison of multiple deep neural networks (DNNs) performing image segmentation for multiple sclerosis (MS) brain lesions. MS is an autoimmune disease that leads to demyelinating lesions in the central nervous system. Measuring disease progression via brain magnetic resonance imaging (MRI) is an important part of managing MS. Manual segmentation of MRI brain lesions by radiologists is time-consuming and subject to high user variability. Our poster shows the results of using three different DNN architectures, trained using NVIDIA GPU. You'll learn about using neural network for medical image segmentation, particularly applied to the measurement of MS brain lesions. You'll also learn about different DNN architectures, including Inception-based convolutional networks and U-Net based networks, as well as techniques used to compare automated segmentation results to each other and to human expert segmentation.",Girls Computing League
NVIDIA GTC 2020,"AI Methods to Transfer Natural Language into Actionable Knowledge in Medicine: From Radiology, Pathology Reports to Social Media Posts [S22150]","Learn about the key types of use cases for AI-driven natural language processing (NLP) that will ultimately improve medical and public health practice by mining humongous amounts of free text from clinical notes and social media posts. First, we'll describe methods to leverage narrative reports associated with radiological scans to automatically generate labels for creating large annotated image datasets, and we'll highlight its application to different domains of radiology (CT, MR, US). Second, we'll discuss the challenge of clinical prediction and present innovative longitudinal XAI approaches (AI with explainability) to improve clinician acceptance. Finally, we'll discuss the emerging use of publicly available social media data in medicine and public health. While it is challenging to execute data mining from this resource, we'll present successful NLP pipelines that can convert this noisy language data into valuable and actionable knowledge.",Emory University
NVIDIA GTC 2020,A New Era of Medical Imaging [S22554],"Deep Learning has revolutionized medical imaging research and has become an essential element for every single step of the imaging pipeline. In this session, we will discuss some recent trends and share key learnings from related conferences. We will go through the general imaging pipeline - from signal, to image, to image understanding, to actionable insights - and provide examples how Deep Learning can accelerate, augment and improve various steps of the pipeline. And even enable the previously impossible.",NVIDIA
NVIDIA GTC 2020,"Animation, Segmentation, and Statistical Modeling of Biological Cells Using Microscopy Imaging and GPU Compute [s21944]","This presentation describes the use of GPU compute in the Allen Institute for Cell Science. The mission of the Allen Institute for Cell Science is to create dynamic and multi-scale visual models of cell organization, dynamics, and activities that capture experimental observation, theory, and prediction to understand and predict cellular behavior in its normal, regenerative, and pathological contexts. Our first project is to understand how the parts of the cells integrate to determine diverse cellular behaviors as revealed through 3D live-cell imaging, creating a dynamic and animated virtual model of the cell. We will describe three applications where we use GPU-compute solutions for our research; these include (1) animation, (2) segmentation, and (3) statistical modeling of biological cell images.",Allen Institute for Cell Science
NVIDIA GTC 2020,Building a Medical Imaging AI Ecosystem Using Clara SDKs [s22295],"This product talk will give an overview of Clara Imaging Application Framework and will focus on the latest feature updates for accelerating data annotation, domain-optimized training, Iterative & high performance experimentation for training AI models and AI inference workflows for deployment of multi-domain, multi-AI applications in smart hospitals. We will walk through the details of Clara Application Platform capabilities and our ecosystem partners. Using standardized tools for data management and annotation, model creation, validation, and deployment - Clara Imaging tools along with its ecosystem collaborators lower the barrier to adoption of AI in the medical imaging ecosystem.",NVIDIA
NVIDIA GTC 2020,Building Blocks for Machine Learning Integration into Clinical Workflow [S22189],"Applications of machine learning in radiology image analysis continue to grow at an increasing pace. For these tools to make an impact in diagnostics, they need to be well integrated into a clinical workflow. We'll review the radiology diagnostic interpretation process and the role of several machine-learning algorithms that support radiologists in this effort. We'll then focus on seven generalizable building blocks that are needed to integrate algorithm results into clinical workflow, with roles ranging from quality control and results presentation to error correction and active learning. We'll discuss current standards and the need for new standards, and highlight our experience in applying these algorithms and building blocks in a large cancer center.",Memorial Sloan Kettering Cancer Center
NVIDIA GTC 2020,"Building Optimized, Low-Cost, Scalable Health-Care Enterprise Deep Learning Services Platform with NVIDIA TensorRT Inference Server and Kubernetes [s21570]","We'll illustrate how NVIDIA TensorRT Inference Server and Kubernetes helped us resolve several issues and build a scalable, low-cost and high-performance solution. Deep learning inference services are fundamentally different from web services and traditional machine learning services. DL services often require GPUs, and one must understand demand and GPU utilization so that load can be efficiently balanced across available inference resources. DL models may require significant compute and memory resources, and the need for serving multiple models and model versions compounds the complexity of a balanced inference deployment. Often, pre- and post-processing run on CPUs, while model inference runs on GPUs, so it is important to decouple them in order to be able to scale GPU and CPU resources independently. Health-care services also have rigorous security requirements and often require on-premises deployment.",Optum
NVIDIA GTC 2020,Clara Developer Day: Clara Train SDK Performance Walkthrough and Deep Dive [S22717],"In this session, developers and data scientists will learn about the latest performance feature in Clara Train v3.0. There will be a deep dive session walk-through for how to enable different features on a dummy example as well as real segmentation task.",NVIDIA
NVIDIA GTC 2020,Clara Developer Day: Federated Learning using Clara Train SDK [S22564],"Federated Learning techniques enable training robust AI models in a de-centralized manner – meaning that the models can learn from diverse data but that data doesn't leave the local site and always stays secure. This is achieved by sharing model-weights or partial model weights from each local client and aggregating these on a server that never accesses the source data. In this session we will deep dive into the federated learning architecture of latest Clara Train SDK. We will cover the core concepts of Federated Learning and the different collaborative learning techniques. Afterward, we will dive deeper into how using the Clara Train SDK enables privacy-preserving Federated Learning. This session will also cover the ease of bringing up Federated Learning clients and establishing communication between various clients and a server for model aggregation.",NVIDIA
NVIDIA GTC 2020,Clara Developer Day: Getting Started with Clara Train for High Performance & Iterative Experimentation with AutoML [S22563],"In this session, developers and data scientists will learn how using Clara Train SDK accelerates and standardizes model development for medical imaging. We will cover the SDKs core concepts and capabilities to define a training workflow with the option to “bring your own components”. The session will also include a hands-on deep dive on how optimize hyper-parameter using AutoML.",NVIDIA
NVIDIA GTC 2020,Clara Developer Day: Scalable and Modular Deployment Powered by Clara Deploy SDK [S22565],"Clara Deploy SDK provides a reference framework for developers, data scientists and engineers to make seamless the process to turn trained AI models into operators. These operators can be stitched together to define an AI deployment pipeline, using reference DICOM adapters and sample pipelines that can interface with a medical imaging environment, like a PACS or VNA. In this session, we will do a walk-through of platform features that enable scalable deployment of multiple AI based pipelines in a hospital IT-like infrastructure. We will also do a hands-on session enabling end-end deployment of a Clara Train model. The users will interact with the SDK to deploy reference pipelines and learn how they can use the modular nature of the overall framework to power deployment of AI models in medical imaging workflows.",NVIDIA
NVIDIA GTC 2020,Computer Vision in Agriculture: Racing with Pests and Diseases [P22001],"Modern technologies of GPU computing and machine learning allow us to create an autonomous system of greenhouse plant condition control. We combined multiple sensor systems with a photo/video surveillance system, which allows us to create a comprehensive control system helpful for early detection of pests and diseases. We strive to create a stable and reliable solution for AI-based early plant disease detection, disease classification, pest detection, and plant development monitoring. We present our working solutions for vertical farming and a prototype of a more general system.",Fermata
NVIDIA GTC 2020,Data-Driven Approach of Coronary Vessel Reconstruction Using X-ray Angiography [P21799],"Coronary artery disease is typically diagnosed using X-ray angiography. The percent stenosis, or narrowing of the blood vessel, is estimated via visual inspection of 2D angiography images; however, image artifacts and non-ideal projection angles can lead to miscalculation of disease severity. To address this problem, we propose a data-driven method to reconstruct the 3D geometry of the coronary vessels. A convolutional neural network was trained to segment vessels from angiography images. The segmented binary images were subsequently used as input images for several 3D reconstruction algorithms. The reconstructed 3D geometry can be used as input data for computational fluid dynamics analysis to characterize the hemodynamics of the diseased vessel and thus further improve diagnostic accuracy.",University of Michigan
NVIDIA GTC 2020,Data-Efficient Weakly Supervised Histology Classification Using Contrastive Predictive Coding [P22025],"Neural network classification models can be trained on weakly annotated medical imaging data using multiple instance learning (MIL). However, due to weak supervisory signals, direct application of MIL suffers from overfitting when faced with limited labeled data and the network is unable to learn rich feature representations. To overcome such limitations, we propose a two-stage semi-supervised approach that combines data-efficient self-supervised feature learning via contrastive predictive coding (CPC) and attention-based MIL. Across five random splits, we report state-of-the-art performance with a mean validation accuracy of 95% and an area under the ROC curve of 0.968 on a breast cancer classification dataset. We evaluate the quality of features learned via CPC relative to simple transfer learning and show that strong classification performance using CPC features can be efficiently leveraged under the MIL framework using limited labels and with the feature encoder frozen.",Harvard Medical School
NVIDIA GTC 2020,"Deep Learning-Aided Label-Free, Real-Time and Time-Lapse “Cell Visualization” Technology that Enables Live/Dead Cell Discrimination and Counting [P21952]","""Cell visualization"" is a new technology that can predict cell properties, such as cell life and death, from bright-field cell images. It's necessary to build a deep-learning model in which the relationship between a teacher cell image, such as fluorescence labeling indicating cell properties, and the corresponding bright-field cell image is finely learned. Inputting unknown bright-field cell images to the constructed learning model generates a pseudo-fluorescent labeling image showing the characteristics of the cells, enabling ""visualization of cells."" The present technology may replace many cell assays in drug discovery by solving the invasiveness of fluorescence-based cell assays. This technology also realizes real-time monitoring of cell quality. This innovative cell digitization technology is expected to become essential in the fields of drug discovery and regenerative medicine.",Nagahama Institute of Bio-Science and Technology
NVIDIA GTC 2020,Deep Learning in Health Care: from Voicebots to Disease Prediction [S21128],"We will focus on a few use cases where deep learning and NVIDIA GPUs help transform health care operations, automate prior authorization processes to reduce operational costs, suggest the next-best action to patients in order to help them manage their chronic conditions, and control call center volumes while increasing customer NPS scores. United Health Group receives over 30 million provider calls each year, out of which over 7 million result in call transfers. We've used a combination of deep learning models to identify the predictors of future calls and the providers likely to make them, enabling us to proactively reach out to them to prevent the incoming calls. If an incoming call is, however, detected, we use NVIDIA’s Nemo ASR / NLP framework to analyze the purpose of the call in real time and optimally route it to the appropriate agent. We'll also discuss our early successes in applying NVIDIA’s OpenSeq2Seq and Nemo frameworks to create voicebots — systems that users can interact with using voice. The quality of audio (smartphone versus phone line), domain (general versus clinical), speaker accents, and so on matter. We'll include an end-to-end overview of the tooling that we created for incremental data collection and model fine tuning on that data, along with metrics of quality and performance benchmarks. Finally, we'll cover our deep learning work on Next Best Action, where we use patients' history to predict the poly-chronic medical conditions and identify high-risk individuals. We then recommend the preventative next-best action, such as an ER visit or a hospitalization.",Optum Technology
NVIDIA GTC 2020,"Deep, Self-Supervised Learning for Patient-Specific Anomaly Detection in Stereoelectroencephalography [S21962]","We'll discuss methods for accurate, real-time detection of anomalous events in medical time-series data, with particular application to stereoelectroencephalographic (SEEG) interpretation and event localization for patients with epilepsy. Previous deep learning-based approaches to detecting EEG anomalies have been plagued by high false-positive rates and inability to detect anomalous events that fall below a statically-set threshold. You'll learn the benefits of a nonparametric approach that utilizes a dynamic thresholding method for event detection, producing significant performance improvements. We'll discuss basic approaches and challenges in anomaly detection in medical time series data, limitations encountered by previous deep-learning approaches to anomaly detection, and how to apply a nonparametric dynamic thresholding procedure to improve performance and mitigate false positive results.",Icahn School of Medicine at Mount Sinai
NVIDIA GTC 2020,De Novo Protein Design of Epitope-Directed Inhibitors [s21348],"Protein drugs have revolutionized modern cancer therapeutics, as protein-based binders can effectively target molecules that are otherwise undruggable by small molecules. Presently, such binders are based on monoclonal antibodies, which are developed and engineered through lengthy and resource-intensive processes of iterative optimization that are empirically guided. Conversely, de novo protein design can offer a rational means for generating new binders with bespoke scaffolds, guided by physics-based computations. Here, we'll present our work on developing a purpose-built, GPU-accelerated computational pipeline for designing protein-based binders de novo. As a proof of principle, we designed proteins that target a key modulator of cancer metastasis. Our experimental characterization of only a few design candidates resulted in binders with strong binding affinities. Solving the structure of one design showed atomic-level agreement between the design model and the determined structure.",Friedrich Miescher Laboratory of the Max Planck Society
NVIDIA GTC 2020,"Deploying FDA-Cleared, TRT-Powered AI Radiology Products to Improve Quality and Productivity in Clinical Radiology [S22400]","SubtlePET and SubtleMR are FDA-cleared AI software products developed by Subtle Medical that use deep learning to significantly improve the image quality and efficiency for PET-CT and MRI exams. We'll introduce how Subtle develops AI that is generalizable and seamlessly deployed in clinical settings. Join our session to:
Learn how to clinically evaluate and deploy AI software solutions at multiple hospitals and imaging centers, through Subtle’s experience of working with Stanford University, the University of California at San Francisco, Hoag hospital, U-C San Diego, Middlesex, RadNet, and others;
Understand how attention models and improved deep-learning architectures improve model performance;
Learn benefits of training algorithms on NVIDIA DGX Station and DGX-1 systems, and how it provides flexibility and efficiency from prototyping to product;
Learn how integrating with NVIDIA TensorRT can accelerate inference; SubtleMR will share there experience where turnaround time is critical, TRT provided additional speed of up to about 8.8x; and
Learn techniques to show how to show evidence of significant clinical values and financial ROI to customer hospitals and imaging centers.",Subtle Medical
NVIDIA GTC 2020,Distributed Deep Learning for Automatic Disease Detection Systems [P22078],"Deep neural networks and other deep-learning approaches are providing diagnostic capabilities on a par with medical specialists. If deployed in the field, these DL applications and devices can help to detect health conditions early and develop preventive approaches. However, deploying such systems requires large compute infrastructure. To address the infrastructure issues, I developed a distributed cost-effective approach using edge devices and a cloud-based central hub for better predictions. To test its effectiveness, I applied it to diagnosing eye conditions using fundus images.",Princeton International School of Mathematics and Science
NVIDIA GTC 2020,Empowering Virtual Reality and Machine Learning in the Hospital [S21472],"A patient's journey through the hospital system involves long-established steps that, collectively, aim to define and provide the best treatment strategy. Two critical actors in this journey are the radiologist and the surgeon. They communicate via the patient's medical image. We'll discuss how we bring new GPU-based technologies, such as virtual reality and machine learning, to this very traditional place of work. We'll outline the degree to which these technologies have been useful in our medical collaborations, as well as how they can optimize outcomes for the patient.",Institut Pasteur
NVIDIA GTC 2020,Federated Learning for Medical Imaging: Collaborative AI without Sharing Patient Data [S21536],"While deep neural networks have shown promising results in various medical applications, they highly depend on the amount and diversity of the training data. In the context of medical imaging, this poses a major challenge because patient data needs to be protected and cannot easily be shared. The training data that is required to train a reliable and robust algorithm may not be available in a single institution due to the low incidence rate of some pathologies and limited numbers of patients. At the same time, it is often not feasible to collect and share patient data in a centralized data lake due to patient privacy concerns and regulations. Federated learning — as a collaborative machine learning paradigm — combined with an advanced privacy-preserving mechanism has the potential of solving this issue: models can be trained across several institutions without explicitly sharing patient data. When implementing and deploying a federated learning system into the real-world medical imaging ecosystem, participants can authenticate and communicate securely, and exchange model weights efficiently, enabling model training to be successful. In this talk, we present an introduction to the core concepts of federated learning and discuss the benefits as well as the unique considerations and challenges of implementing a federated-learning system in the context of health care.",NVIDIA
NVIDIA GTC 2020,Fostering a Strong Ecosystem for AI in Medical Imaging [S22631],"In order to fully leverage the possibilities that AI offers to drive higher-value health care, we need to create an effective collaboration between physicians and developers, policymakers and payers, and most importantly the patients we will serve. We'll explore ways in which those collaborations are already happening and highlight gaps and opportunities. The speaker, a practicing radiologist specializing in breast imaging, will discuss the ways in which she sees AI impacting her practice now and in the future.",Weill Cornell Medicine
NVIDIA GTC 2020,Fully Automated Blood Analyzer Driving Early Detection for Leukemia Based on Cytomorphology [P22070],"The chances of leukemia survival depend on a variety of factors, including early detection and response to treatment. Cytomorphology introduces the methodology to characterize blood cell morphology to detect leukemia in the early stage. We developed a fully automated blood analyzer based on cytomorphology to count all types of different white blood cells, red blood cells, and blood platelets. The whole workflow of this system includes sample delivery, screening, segmentation, and classification. Compared with the other leading manufacturer, such as CellVision, the segmentation and classification in our system is driven by deep learning instead of pattern recognition, which lets our results achieve more than 95% accuracy based on the validation of a real dataset counted by doctors from a top hospital. It means that our system can be the first real-world health-care AI product in hematology. We'll introduce more detail about our workflow and AI technology accelerated by NVIDIA advanced GPU.",Shanghai Jiao Tong University
NVIDIA GTC 2020,Generating Microscopic Phase Images with Deep Learning [P22394],"This poster will present a deep-learning approach for generating microscopic phase images from brightfield images, contrasting it with current approaches. We'll highlight examples of novel techniques developed at PerkinElmer using supervised and self-supervised approaches to tackle common problems faced by classical approaches. We'll also discuss the challenges of building and testing these generative models for medical imaging and drug discovery applications.",PerkinElmer
NVIDIA GTC 2020,GPU-Accelerated Animated Volume Rendering of Isogeometric Analysis Results [P22069],"Development of isogeometric analysis (IGA) has enabled tighter integration of engineering design and computational analysis. The core idea of IGA is to use same NURBS (Non-uniform rational B-splines) basis functions for representation of geometry in CAD and the approximation of solution fields in FEA. However, visualizing the results of volumetric IGA is compute-intensive; hence, current methods are not interactive. We developed a modified ray-casting and voxelization method for visualizing volumetric NURBS, which provides better interactive performance. This process has been highly parallelized using the GPU to produce interactive animated results in real time. We present an example of the utility of the approach in visualizing results of cardiac simulations.",Iowa State University
NVIDIA GTC 2020,GPU-Accelerated Genome Assembly: A Deep Dive into Clara Genomics Analysis SDK [S21968],"We'll present de novo genome assembly for long DNA reads using the Clara Genomics Analysis SDK and dig deep into the SDK implementation. We'll discuss the suitability of GPUs for genomics workloads and present algorithms suitable for massively parallel systems. We'll also review the challenges we faced while implementing the algorithms in CUDA, and present the solutions used in cudaAligner, cudaPoa, and cudamapper.",NVIDIA
NVIDIA GTC 2020,"High Throughput Cryo-Electron Microscopy and Cryo-Electron Tomography Powered by GPU at the University of California, San Francisco [S21696]","As early as in 2009, GPU-based computing was introduced at the University of California, San Francisco (UCSF) to reconstruct electron tomographic volumes. Today, 10 years later, high-resolution cryo-electron microscopy (CryoEM) and cryo-electron tomography (CryoET) powered by state-of-the-art GPU technology are routinely used worldwide in structural biology. We'll present the development of GPU-based applications at UCSF to solve critical challenges in CryoEM and CryoET — namely, beam-induced motion, cryoET alignment, and deep-learning based de-noising of cryoEM low-dose images. You should be familiar with back- and forward-projections, Fourier Transforms, C++, and CUDA programming.",University of California San Francisco
NVIDIA GTC 2020,Holistic AI-Enhanced Workflows in Radiology [S21440],"We'll highlight the advantages of a holistic workflow in radiology, the opportunities and challenges of AI implementation in diagnostic workflows, and their impact on treatment decisions and patient outcomes. AI will have a tremendous impact on the way we think and work in medicine. Diseases are complex pathological processes and more than one algorithm is often needed to answer relevant medical questions. This requires a well-managed orchestration of the reporting workflow. We'll introduce the Smart Reporting platform as an innovative tool for synoptic AI-enhanced reporting, which massively improves interdisciplinary communication. Leveraging on NVIDIA Clara and GPU computing, we'll also present a novel pipeline for prostate cancer imaging, tumor detection, automatic report generation, communication of results, and improved treatment decisions.",Smart Reporting GmbH
NVIDIA GTC 2020,Improving Classification of Lymph Node Histopathology Patches Using Semi-Supervised Classification-GAN (SSC-GAN) [P21984],"The goal of our Semi-Supervised Classification-GAN (SSC-GAN) on histopathology images is to use generated synthetic images to aid in data augmentation and improving classification accuracy, beyond the regular classifiers such as CNNs, where access to annotated data is limited.",Onward Health
NVIDIA GTC 2020,Improving CNN Performance with Spatial Context [S21388],"Deep learning with convolutional neural networks (CNN) is a powerful technique with wide-ranging applications. It has largely replaced traditional computer vision as the go-to method for solving image-analysis and classification problems. At its essence, however, training a CNN is an enormous global optimization problem which, like all optimizations, can fall victim to local extrema. We'll discuss ways of mitigating this issue using computer vision to add spatial context information to restrict the domain of optimization. These techniques not only speed up the training, but also improve the overall performance of the networks. We'll demonstrate results on real-world classification and segmentation problems.",Voxeleron
NVIDIA GTC 2020,Insight-Driven Machine Learning Design with Human Expert Collaborations [S21636],"The fundamental breakthroughs in machine learning, and the rapid advancements of the underlying deep neural network models have enabled the potential use of these systems in specialized, high stakes domains such as healthcare. However, despite this remarkable progress, the design of machine learning systems remains laborious, computationally expensive and opaque, sometimes resulting in catastrophic failures and significantly hindering their ability to work with human experts, who play critical roles in these settings. In this talk, I overview steps towards a more informed, intuitive design of machine learning systems, and methods to facilitate collaboration with human experts. I develop tools that enable the quantitative analysis of the complex hidden layers of deep neural networks, which in turn provides both fundamental insights and informs algorithms for efficiently training these systems. I demonstrate how these trained systems can be adapted to work effectively with human experts, resulting in better outcomes than either entity alone.",Google Brain
NVIDIA GTC 2020,Machine Learning Cell Phenotypes with Immuno-Fluorescent Cancer Tissue [P22105],"Multiplexed immuno-fluorescent tissue imaging enables precise spatial assessment of protein expression in medical resection specimens. However, tissue sections are stained with a mixture of antibodies, DNA, and RNA markers. Detecting weak or broken edges due to fluorescent membrane-staining artifacts between touching or overlapping cells is a long-studied problem, and is an active research topic in biomedical image analysis. Sometimes, even humans can't visually detect these kinds of edges. We've built a GPU client-server and developed a hybrid system combining the stochastic random-reaction-seed (RRS) method and deep neural learning U-net to identify cell membranes accurately and automatically. Furthermore, we've designed a high-performance AI pipeline in quantifying spatial distribution of cell phenotypes from tissue images with various complexities.",Fred Hutchinson Cancer Research Center
NVIDIA GTC 2020,Making Radiology AI Models More Robust: Federated Learning and Other Approaches [S22037],"Learn about the key types of clinical use cases for AI methods in medical imaging and the critical challenges and progress in applying AI in these applications. We describe current challenges to creating robust AI models, such as insufficient quality labeled data and access to data. Next, we'll describe recent AI projects that tackle the challenges, including weak/observational learning and federated learning.",Stanford University
NVIDIA GTC 2020,Manipulating the StyleGAN Latent-Space: H&E Image Synthesis for Prostate Cancer Research [P22077],"The open-source contributions of NVIDIA research projects have been accelerating the effectiveness of generative adversarial networks since their popularization in 2014. The StyleGAN (Dec 2018) enables detailed control over the generator’s captured learnings. We explore the generator’s learned assignment of image features in the medical domain. Today, prostate cancer is one of the most common types in men. Diagnosticians examine slices of prostate tissue to analyze cancer growth and determine treatment strategies. Staining techniques provide unique lenses for diagnosis, highlighting stroma patterns, lumen areas, epithelium groupings, and nuclear areas. In our experiment, we input a dataset of H&E stained prostate tissue covering the entire spectrum of the disease. During model training, the network creates a mapping of all image features into a latent space. We aim to discover direction vectors between cancer-relevant morphological features.",Milwaukee School of Engineering
NVIDIA GTC 2020,Medical Volume Raytracing in Virtual Reality [S22030],"Raytracing voxels in medical images is a relatively new technique that lets us see medical images in a new light. It allows for creating realistic-looking light effects, like soft shadows. Exploring medical images in virtual reality benefits because these advances make objects look more real, making it easier for a physician to interpret what they are seeing. We'll discuss the challenges with doing volume raytracing in VR, and will demonstrate a solution in CUDA involving rendering to an eye-tracked foveated/warped space. We'll discuss how we can stream this warped space from a server to a head-mounted display, and how to effectively de-noise the results for VR. We'll also show how to use the extra GPU budget that we created by foveated/warped rendering to improve visuals by including better material choices based on DL-generated label maps and define less stair-stepped implicit surfaces based on tricubic interpolation.",NVIDIA
NVIDIA GTC 2020,Multi-Task Learning for Sparse Sensor Body Tracking [P22340],Assistance and rehabilitation technologies built on inertial measurement unit-based human motion trackers often use a full-body sensor set. This is necessitated by the bio-mechanical model inspired by the skeleton structure. Reducing the number of sensors will make such technologies more affordable and easier to use for non-expert and unmonitored uses. We explore solutions and demonstrate that it's viable to reduce the number of sensors required for credible body segment acceleration and joint angle tracking. This is achieved by statistical learning on the reduced sensor set by developing inferences based on neural network and signal processing pipelines.,Xsens Technologies
NVIDIA GTC 2020,NVIDIA Quadro RTX for Healthcare - Improving Patient Outcomes [S22422],This talk covers how Quadro RTX features like real-time ray tracing and AI can reshape healthcare and the life sciences disciplines that support basic research leading to new treatment options and better patient outcomes,PNY Technologies
NVIDIA GTC 2020,PDGAN: An End-to-End Parkinson's Disease Data Analysis and Diagnosis System Using Deep Learning [P21110],"PDGAN is a deep-learning-powered tool capable of detecting early signs of Parkinson's disease — a prevalent disease that affects millions of people worldwide. Using MRIs, PDGAN can classify patients in the earliest stages of Parkinson's Disease with 96% accuracy. PDGAN uses a unique technique to solve the diagnosis problem, using generative adversarial networks to synthetically create MRI scans to augment the training dataset. Our poster describes the technical aspects of the dataset, the pipeline of deep-learning tools used to solve the problem, additional features that PDGAN uses that traditional deep-learning solutions don't utilize, and the benefits that each of them bring. It discusses using NVIDIA tools to solve problems that plague many machine-learning problems, including a slow training process and the lack of adequately large data using novel GAN-powered techniques.",Thomas Jefferson High School for Science and Technology
NVIDIA GTC 2020,Practical Guidelines for Optimizing and Accurate Sizing of Medical Imaging Workflows [S21997],"Deep dive into the various considerations that could be critical to substantially improve existing medical imaging workflows. We'll motivate our discussions through a detailed analysis of two common medical imaging workflows: 2D classification and 3D segmentation. Our results suggest that controlling for aspects such as i/o format, selection of hyper-parameters, and mixed-precision training could all be key to maximizing hardware performance and reducing turnaround time for experiments. Furthermore, we aim to discuss other strategies, such as learning-rate warmup and scaling, effect of optimizers, scaling to multiple GPUs, and their potential effects on training throughput. We were able to show a 5x improvement for the 2D classification task through our ablation experiments. We'll use these results suggest best practices learned, and also build a sizing calculator for providing quantitative insights for hardware investments.",NVIDIA
NVIDIA GTC 2020,Reconstruction of Under-Sampled Cartesian Data Using Deep Learning [P21927],"Magnetic resonance imaging (MRI) is a non-ionizing medical imaging modality that provides excellent contrast information about human tissues. One of the limitations of MRI is its long data-acquisition time. One way to reduce the MRI scan time is to collect less data that introduces aliasing in the resulting MR image. Dedicated MRI reconstruction algorithms can be used to remove the aliasing effects. Under-sampled Cartesian data, in conjunction with an advanced reconstruction algorithm, has been used in literature to get the aliasing-free image. We propose a convolution neural network (CNN)-based solution to reconstruct the MR image from under-sampled MRI data, thus helping to reduce the scan time. We use artifact power and signal-to-noise ratio to quantify the reconstruction quality. We compare the results to the state-of-the-art SENSE reconstruction algorithm.",Islamabad Pakistan
NVIDIA GTC 2020,Rethinking Impact Factor: an NLP-Driven Metric and Pipeline Using Generalized Autoregressive Pretraining on Medical Journals for Granular Knowledge [S21552],"Dramatic advances in NLP have reinvented performance on public leaderboards, such as the Stanford Question Answering Dataset and decathlon superGLUE. Nominally, approaches follow transformers-based architectures in a pretrain-finetune paradigm, with the bulk of compute in the pretrain phase. Where previous studies have elucidated finetune paradigms for recurrent neural network architectures, ours examines a multi-phase NLP paradigm for realizing expert-level domain-specific performance, specifically for the clinical task of unplanned 30-day hospital readmission. With exhaustive GPU studies and Bayesian optimization, in part with the NVIDIA Clara Train platform, we'll show that systems are only as good as what they read. From 20 top medical practice journals over the past 90 years, we determined a novel AI-impact factor for the clinical task that guides to state-of-the-art AUC of 0.74. We'll review best practices on training modern transformer-based architectures for medicine.",NVIDIA
NVIDIA GTC 2020,Segmentation-Guided Pelvis Fracture Detection in Pelvis X-rays [P21979],"Pelvis fractures represent approximately 3% percent of all skeletal injuries and can be observed in any group of patients. Like much trauma, there is a bimodal distribution, with younger male patients involved in high-energy trauma and older female patients in minor trauma. In this work, we utilize GPU computing to process high-resolution pelvis X-ray images and segment into pelvis anatomical regions. Then, we train a binary classifier on segmented anatomical region to detect fracture.",imera.ai
NVIDIA GTC 2020,Segmentation of Organs at Risk in Chest Cavity Using 3D Deep Neural Network [P21943],"Our work considers the problem of segmentation of organs-at-risk in planning radiotherapy of lung and esophageal cancer. To solve this problem, we developed a system based on a 3D implementation of the U-Net neural network architecture with ResNet-50 encoder. One of the main problems of segmentation of 3D images is the limitation of computing power. We propose a multi-dimensional approach that effectively uses the available infrastructure for processing large image data. Overfitting due to the small size of the dataset is another important problem. We solve this using input mixup for regularization of neural networks. To train the deep-learning models, the NVIDIA TITAN X Pascal and NVIDIA Tesla V100 were compared, with and without Apex Optimization. The results demonstrate that the best performance was obtained using NVIDIA Tesla V100 16Gb with Apex Optimization.",Ciklum
NVIDIA GTC 2020,*Top 5 Poster Nominee - MolecularRNN: Generating Realistic Molecular Graphs with Optimized Properties [P22377],"We'll introduce MolecularRNN, the graph-generative model for molecular structures. Designing new molecules with a set of predefined properties is a core problem in drug discovery. There is a growing need for de-novo design methods. Our model generates diverse realistic graphs after likelihood pretraining on a big database of molecules. We perform an analysis of pretrained models on large-scale generated datasets of 1 million samples, then tune the model with a policy-gradient algorithm provided with a critic that estimates the reward for the property of interest. We see a significant distribution shift to the desired range for lipophilicity, drug-likeness, and melting point, outperforming state-of-the-art works. With the use of rejection sampling based on valency constraints, our model yields 100% validity. We'll show how invalid molecules provide a rich signal to the model through the use of structure penalties in our reinforcement learning pipeline.",Carnegie Mellon University
NVIDIA GTC 2020,3D Deep Learning in Function Space [s21764],"Recent advances in GPU technology and scalable algorithms have led to breakthroughs in deep learning. In particular, convolutional neural networks (CNNs) achieve state-of-the-art results in longstanding vision problems, such as image classification or object detection. However, autonomous agents that navigate and interact in our world need to reason in 3D. Unlike images in the 2D case, it is not clear how to represent 3D geometry and how to make it amenable for deep-learning techniques. We'll introduce our approach to learning 3D representations in function space. First, we'll show how this approach can represent arbitrary topologies without discretization at fixed memory cost. Then we'll extend this framework to learning to predict not only the shape of an object, but also its texture and motion. Finally, we'll show how we can scale our method to real-world scenarios using state-of-the art NVIDIA GPU technology.",MPI-IS and University of Tabingen
NVIDIA GTC 2020,Accelerated Computing Teaching Kit for University Educators: Introduction and Use Cases [S22414],"As performance and functionality requirements for computing applications rise, industry demand for new graduates familiar with accelerated computing with GPUs grows. This session introduces the newest version of the Accelerated Computing Teaching Kit: a comprehensive set of academic labs, university teaching material, and e-book for use in introductory and advanced parallel programming courses. The teaching materials start with the basics and focus on programming GPUs, and include advanced topics such as optimization, advanced architectural enhancements, and integration of a variety of programming languages. We'll present a successful course-adoption case at Iowa State University — one of many worldwide — along with student feedback. The course at Iowa State covers an introduction to parallel computing using GPUs and its application to solid modeling and CAD. As part of the course outcomes, students were able to demonstrate the use of GPU-accelerated computing in their research by working on an individual course project that uses some of the techniques taught in the class. Finally, we'll discuss brand-new teaching kit modules covering CUDA 11, multi-GPU, and the latest libraries.",NVIDIA
NVIDIA GTC 2020,Accelerated Data Science in the Classroom: Teaching Analytics and Machine Learning with RAPIDS [S22417],"The demand for accelerated data-science skill sets among new graduate students grows rapidly as the computational demands for data analytics applications soar. This session introduces a novel yet reproducible approach to teaching data-science topics in a graduate data science course at the Georgia Institute of Technology, taught by Professor Polo Chau. Haekyu Park, a computer science Ph.D. student and teaching assistant of the course, will co-present key pedagogical considerations and solutions that help students learn GPU-accelerated data science and analytics using the open-source RAPIDS framework. For example, we present a hybrid, flexible approach for students to learn, where they can choose to experiment with RAPIDS using a local NVIDIA DGX-1 system, or the cloud, or both.",The Georgia Institute of Technology
NVIDIA GTC 2020,Accelerated Light-Transport Simulation using Neural Networks [S21852],"Neural network-based techniques have taken many fields by storm, but until recently have seen relatively little use in the field of physically-based rendering. This has begun to change. We'll present techniques for accelerating Monte Carlo integration of light transport without introducing bias by utilizing functions learned by neural networks for variance reduction. Our techniques yield on-par or higher performance than competing machine learning-based techniques at equal sample counts and generalize beyond physically-based rendering, being applicable to other high-dimensional integration problems such as Bayesian inference and reinforcement learning.",NVIDIA
NVIDIA GTC 2020,Accelerating Large Seismic Simulation Code with PACC Framework [P21901],"The pipelined accelerator (PACC) helps lower the hurdle for implementing out-of-core stencil computation, such as large seismic simulations. However, the out-of-core applications themselves are facing data-movement bottlenecks because improvement of accelerators dwarfs that of interconnects. In this poster, we introduce temporal blocking techniques to reuse on-chip data, and propose a data-mapping scheme to eliminate data movement on the host side. The data-mapping scheme accelerates the program by 2.5x compared to previous work and by 35x compared to an OpenMP-based program. However, performance is still bound by data movement between the host and device, so we need to come up with further data-centric strategies. Moreover, the degradation in execution time of PACC code is about 25% compared to an in-core OpenACC code, which should be reduced if we can design further data-centric strategies.",Osaka University
NVIDIA GTC 2020,ALFRED: Action Learning From Realistic Environments and Directives [P22310],"ALFRED is a benchmark for learning a mapping from natural language instructions and egocentric vision to sequences of actions for household tasks. Long composition rollouts with non-reversible state changes are among the phenomena we include to shrink the gap between research benchmarks and real-world applications. ALFRED consists of expert demonstrations in interactive visual environments for 25,000 natural language directives. These directives contain both high-level goals like ""Rinse off a mug and place it in the coffee maker"" and low-level language instructions like ""Walk to the coffee maker on the right."" ALFRED tasks are more complex in terms of sequence length, action space, and language than existing vision-and-language task datasets. We show that a baseline model designed for recent embodied vision-and-language tasks performs poorly on ALFRED, suggesting that there is significant room for developing innovative grounded visual language understanding models with this benchmark.",University of Washington
NVIDIA GTC 2020,Analytic Spherical Harmonic Gradients for Real-Time Rendering with Many Polygonal Area Lights [P22297],"We develop a novel analytic formula for the spatial gradients of the spherical harmonic coefficients for uniform polygonal area lights. The result is a significant generalization, involving the Reynolds transport theorem to reduce the problem to a boundary integral, for which we derive a new analytic formula, showing how to reduce a key term to an earlier recurrence for SH coefficients. The implementation requires only minor additions to existing code for SH coefficients. The results also hold implications for recent efforts on differentiable rendering. We show that SH gradients enable very sparse spatial sampling, followed by accurate Hermite interpolation. This enables scaling PRT to hundreds of area lights with minimal overhead and real-time frame rates. Moreover, the SH gradient formula is a new mathematical result that potentially enables many other graphics applications.",San Diego
NVIDIA GTC 2020,An Improved Immersed Boundary-Lattice Boltzmann Method for Incompressible Fluid-Flow Simulations on GPU [P21931],"We introduce the immersed boundary and the lattice Boltzmann method for fluid-solid interaction. The advantage of the lattice Boltzmann method is that it can be easily parallelized. We use a computational study to demonstrate how the immersed body discretization affects the numerical results. We introduce a modification that improves stability, and compare the performance of both methods using several benchmarks problems. These problems show how the immersed body discretization affects the numerical results, mainly the overall drag force and the permeability of the discretized body boundary. Finally, we discuss a performance analysis of different solvers for linear systems.",Prague
NVIDIA GTC 2020,A Reinforcement Learning Approach for Sequential Spatial Transformer Networks [s21941],"In this session, we discuss the applications of reinforcement learning (RL) for computer vision applications such as classification. We describe how we combine the idea of spatial transformer networks with RL to improve the classifiers' robustness against noise and clutter.",DFKI
NVIDIA GTC 2020,Astaroth: an API for Three-dimensional Stencil Computations on GPUs [S21332],"We'll present the Astaroth library that uses multiple GPUs efficiently in 3D stencil computations. Astaroth provides a C99 API, a domain-specific language, and a compiler for translating programs written in that domain-specific language into efficient CUDA kernels. We'll show that it is possible to achieve near hand-tuned performance while still being able to describe problems using a high-level language. Astaroth meets the demands in computational sciences, where large stencils are often used to attain sufficient accuracy. Maximizing the utilization of caches is challenging, especially in computational physics, where multiple simulated fields interact with each other and should be held in caches simultaneously. Astaroth takes inspiration from graphics and image-processing pipelines and generates a pipeline optimized for processing large 3D stencils.",Aalto University School of Science
NVIDIA GTC 2020,Benanza: Automatic μBenchmark Generation to Compute ”Lower-bound” Latency and Inform Optimizations of Deep Learning Models on GPUs [P21858],"Current profiling tools lack the highly desired abilities to characterize ideal performance, identify sources of inefficiency, and quantify the benefits of potential optimizations. Such deficiencies have led to slow characterization/optimization cycles that can't keep up with the fast pace at which new DL models are introduced. We propose Benanza, a sustainable and extensible benchmarking and analysis design that speeds up the characterization/optimization cycle of DL models on GPUs.",University of Illinois Urbana-Champaign
NVIDIA GTC 2020,Bringing AI to the Classroom: NVIDIA's Deep Learning Teaching Kit [S22357],"The call for AI and deep-learning skills is soaring, and university classrooms are on the front lines of feeding the demand. NVIDIA Teaching Kits lower the barrier of incorporating AI and GPU computing in coursework. NVIDIA’s higher-education leadership and Pawel Morkisz, assistant professor of mathematics at AGH University of Science and Technology in Krakow, will discuss the Deep Learning Teaching Kit, co-developed with Professor Yann LeCun and his team at New York University. The kit was a starting point for preparing materials for the course dedicated for postgraduate students of mathematics. Using NVIDIA’s materials saved many days of work preparing lecture slides and source-level coding projects/solutions.",NVIDIA
NVIDIA GTC 2020,Building MSOE’s GPU-Powered Infrastructure for AI Instruction and Research [s21423],"Last September, Milwaukee School of Engineering, with NVIDIA, deployed a new hybrid cluster to serve multiple AI and HPC computing demands including instruction, student and faculty research, and industry collaborations. We'll describe the optimized accelerated computing architecture of this cluster and the software stack enabling these uses. We'll explain the lessons learned through the design, build, and deploy processes. We'll provide a blueprint for other institutions that seek to build GPU-accelerated computing infrastructure that supports similar diverse requirements.",NVIDIA
NVIDIA GTC 2020,CapsNet-Lite: A Lightweight and High Performance CapsNet Architecture [P21854],"The capsule network (CapsNet) includes a new type of layer composed by ""capsules"". A capsule is a vector that stores in each position the amount of a certain object feature that's present in the image. Then, those capsules are combined to output the label of the object. To learn how to combine the capsules, this network also needs a routing algorithm. This type of network achieves very high accuracy in classification problems. We propose a modified CapsNet architecture called CapsNet-Lite that outperforms the original proposal in accuracy, with a much faster training procedure.",Universidad Rey Juan Carlos
NVIDIA GTC 2020,Capsule Networks for 3D Pose Estimation in Computer Graphics [P21924],"Pose estimation is an important task for novel engineering applications, such as virtual and augmented reality (VR/AR), pose-based video games, object reconstruction, target tracking, driving assistance, and recent sports analytics. Commonly, an efficient pose estimation system depends on the pose visualization given by a 3D configuration of location, orientation, and scaling parameters of the target. In this work, we implement Capsule Networks to solve 3D pose estimation in computer graphics of rigid objects using a multi-GPU architecture.",CETYS Universidad
NVIDIA GTC 2020,CheetahDB: A System for High-Throughput Database Processing on GPUs [P22073],"GPU database is an active topic in academic research and industrial practice. However, existing systems have not shown significant performance advantages over CPU-based in-memory database management systems (DBMS). Two main factors contributed to such difficulties: First, the CUDA programming model, by focusing on HPC-type workloads, requires non-trivial basic research to address the many technical challenges in developing a DBMS system software; and second, I/O bottleneck between host and GPU offsets the performance gain of onboard query processing. CheetahDB is a high-performance in-memory DBMS generated from NSF-supported research at the database group in the University of South Florida and commercialized by Cheetah Data Systems, Inc. CheetahDB addresses the above challenges via a complete rethinking of the software architecture of a DBMS under today’s multi-core hardware environment.",University of South Florida
NVIDIA GTC 2020,Data Mining Pipeline for Predictive Synthesis of Advanced Materials [P22007],"Materials discovery is significantly facilitated and accelerated by high-throughput ab-initio computations. Being able to rapidly design advanced compounds has displaced the materials-innovation bottleneck to developing synthesis routes for the desired material. As there isn't a fundamental theory for materials synthesis, one might attempt a data-driven approach for predicting inorganic materials synthesis, but this is impeded by the lack of a comprehensive database of synthesis parameters. We've generated a dataset of “codified recipes” for solid-state synthesis automatically extracted from scientific publications. The dataset consists of about 20,000 synthesis entries retrieved from over 50,000 solid-state synthesis paragraphs by using text mining and natural language processing approaches. The dataset is publicly available and can be used for data mining of various aspects of inorganic materials synthesis.",Berkeley
NVIDIA GTC 2020,Decoding Texture Information from Rat Somatosensory Cortical Neurons Using CNN [P21855],"Neurons in the neocortex often exhibit feature selectivity against external stimuli. Simple stimuli can be decoded from activity of a single neuron; however, it has been poorly understood whether more complicated features can be decoded from the neuronal activity. To address this question, we recorded local field potentials (LFPs), which reflect the activity of thousands of neurons, of rats given complex sensory stimuli and took advantage of a convolutional neural network (CNN) for decoding. To this end, we developed a novel electrode array that enables wide-range recording from the superficial layers of the cortex. We recorded LFPs from the primary somatosensory cortex (S1) of a rat exploring on either rough or smooth floor. Our CNN model yielded 78% accuracy on average for decoding the surface texture. This suggests that somatosensory LFPs contain enough information to decode a surface texture.",Graduate School of Pharmaceutical Sciences
NVIDIA GTC 2020,Deep Learning-Based Subcellular Phenotyping of Cell Edge Dynamics Reveals Fine-Grained Drug Responses [S21242],"We'll highlight how unsupervised deep learning can reveal subcellular drug responses hidden in the heterogeneity in live cell images. We'll present a feature-learning method for time-series data, which combines long short-term memory autoencoder and the prior information from traditional machine-learning analysis. We'll discuss how feature learning can be used to identify drug-related rare phenotypes and accelerate drug discovery. You need to have basic knowledge about autoencoder, clustering, microscopy, and cell biology.",Worcester Polytechnic Institute
NVIDIA GTC 2020,Dive into Deep Learning [T22537],"Nowadays, deep learning is transforming the world. However, realizing deep learning presents unique challenges, because any single application brings together various disciplines. To fulfill the strong wishes of simpler but more practical deep learning materials, Dive into Deep Learning (https://d2l.ai/), a unified resource of deep learning, was born to achieve these goals: • offer depth theory and runnable code, showing readers how to solve problems in practice; • be complemented by a forum for interactive discussions of technical details and to answer questions; and • be freely available for everyone. We're going to provide an overview of the in-depth convolutional neural networks (CNN) theory and handy Python code. More importantly, you'll be able to train a simple CNN model on our pre-setup cloud-computing instances for free. Here are the detailed schedule and materials: https://github.com/goldmermaid/gtc2020",Amazon Web Services
NVIDIA GTC 2020,Efficient Simulations of Patient-Specific Electrical Heart Activity on the DGX-2 [P22031],"Patients who have suffered a heart attack have an elevated risk of developing arrhythmia. The use of computer simulations of the electrical activity in the hearts of these patients is emerging as an alternative to traditional, more invasive examinations performed by doctors today, and could provide not only safer but also more accurate results. One of the principal barriers to clinical use of such simulations is the tremendous amount of computational power they require. In this poster, we demonstrate a highly efficient code capable of running electrical heart activity simulations at 1/30 of real-time on the NVIDIA DGX-2, and we show that the achieved performance is close to optimal. Topics discussed include optimisations for sparse matrix-vector multiplications, strategies for handling inter-device communication for unstructured meshes, and lessons we learnt while programming the DGX-2.",Simula Research Laboratory
NVIDIA GTC 2020,Exploring the Impact of Functional Package Manager Over a Non-Traditional High Performance Computing System [P21980],"Nowadays, academia and industry are using high performance computing in a search for better ways to improve the performance of the systems, and to make constructing and installing the programs less complex. Our project focuses on two problems: First, we analyze embedded systems capable of HPC as a way to improve energy consumption, and Nix as a tool to manage programs more easily. Then, we analyze whether using these systems is valid in an HPC context in terms of energy efficiency and performance.",Universidad Industrial de Santander
NVIDIA GTC 2020,Few-Shot Adaptive Video-to-Video Synthesis [S21142],"Video-to-video synthesis (vid2vid) aims to convert an input semantic video, such as human poses or segmentation masks, to an output photorealistic video. However, existing approaches have limited generalization capability. For example, to generalize a trained human synthesis model to a new subject previously unseen in the training set requires collecting a dataset of the new subject, as well as retraining a new model. To address these limitations, we propose an adaptive vid2vid framework to synthesize previously unseen subjects or scenes by leveraging few example images of the target at test time. Our model achieves this few-shot generalization capability via a novel network weight-generation module utilizing an attention mechanism. We conduct extensive experimental validations with comparisons to strong baselines on different datasets.",NVIDIA
NVIDIA GTC 2020,Fireiron: A Scheduling Language for High-Performance Linear Algebra on GPUs [P22298],"Our poster introduces Fireiron, a DSL and compiler that allows the specification of high-performance GPU implementations as compositions of simple and reusable building blocks. We show how to use Fireiron to optimize matrix multiplication implementations, achieving performance matching hand-coded CUDA kernels, even when using specialized hardware such as NIVIDA tensor cores, and outperforming state-of-the-art implementations provided by cuBLAS by more than 2x.",University of Mnster
NVIDIA GTC 2020,GPU-Accelerated Next-Generation Sequencing Bioinformatic Pipeline [S21163],"Gold-standard next-generation sequencing (NGS) bioinformatic pipelines (BWA, PICARD, GATK, Samtools and variant annotators) utilize conventional CPUs. However, running pipelines for thousands of whole exome or whole genome sequencing (WES and WGS) samples requires several months, even on high performance computing CPU clusters. Graphical processing units have not been efficiently employed in pipeline acceleration, but have reduced computational runtime in biomedicine. Thus, we created a Kubernetes pipeline where we adapted and GPU-accelerated the code of key NGS software. We also tested the applicability and performance of GPU frameworks, and show through multiple experiments the extent of pipeline acceleration with varying combinations of multithreaded CPUs and GPUs. We show GPU’s effectiveness for bioinformatic analyses and how the mixed use of CPU and GPU will become the gold standard.",Icahn School of Medicine at Mount Sinai
NVIDIA GTC 2020,Heterocomputing and Transprecision Computing in Unstructured Low-Order Finite-Element Analyses on Volta GPUs [P21857],"We show the effect of heterocomputing and transprecision computing on performance using Volta GPUs. Here, we target low-order finite-element analysis. It's a core application in manufacturing, and attaining good performance on GPUs is challenging due to such bottlenecks as memory-bound computations and random memory accesses. We describe detailed algorithms to relieve these bottlenecks by utilizing heterogeneous computing and transprecision computing. As an application example, we conduct an earthquake city simulation.",University of Tokyo
NVIDIA GTC 2020,High-Performance Deep-Learning Operators on NVIDIA GPUs via Multi-Dimensional Homomorphisms [P21790],"We present a holistic approach to CUDA code generation that provides performance, portability, and productivity for deep-learning operators (such as Matrix Multiplication and Convolutions) on NVIDIA GPUs. Our approach is based on our algebraic formalism of Multi-Dimensional Homomorphisms (MDHs). We show that important deep-learning operators can be conveniently expressed as MDHs, and that we can automatically generate CUDA code for MDHs that is specifically optimized for a particular GPU and input size. Our experimental results on real-world, deep-learning input data demonstrate that our automatically generated and optimized CUDA code achieves better performance than well-performing competitors — up to 2.91x better than NVIDIA’s hand-optimized libraries cuBLAS and cuBLASLt for deep-learning operator GEMM, and up to 3x better than NVIDIA’s cuDNN library and the popular deep-learning framework TVM for operator Convolution.",University of Mnster
NVIDIA GTC 2020,High Performance Distributed Deep Learning: A Beginner's Guide [S21546],"Learn the current wave of advances in AI and HPC technologies to improve the performance of deep neural network training on NVIDIA GPUs. We'll discuss many exciting challenges and opportunities for HPC and AI researchers. Several modern DL frameworks (Caffe, TensorFlow, CNTK, PyTorch, and others) that offer ease-of-use and flexibility to describe, train, and deploy various types of DNN architectures have emerged. We'll provide an overview of interesting trends in DL frameworks from an architectural/performance standpoint. Most DL frameworks have utilized a single GPU to accelerate the performance of DNN training/inference. However, approaches to parallelize training are being actively explored. We'll highlight new challenges for message-passing interface runtimes to efficiently support DNN training, and how efficient communication primitives in MVAPICH2-GDR can support scalable DNN training. Finally, we'll discuss how we scale training of ResNet-50 using TensorFlow to 1,536 GPUs for MVAPICH2-GDR.",Ohio State University
NVIDIA GTC 2020,Hybrid Molecular Mechanics: Artificial Intelligence Simulation Methods to Study Molecular Systems [S22090],"The advent of modern GPU architectures, and the development of molecular dynamics (MD) engines to exploit them efficiently, substantially impacted the performance and timescale achieved by MD. We'll present recent enhancements of MD capabilities by coupling to artificial intelligence methods. We'll present the interfaces enabling the coupling of the MD engine NAMD with the deep learning-based molecular descriptor Accurate NeurAl networK engINe (ANI). By using ANI to describe unparameterized molecules in the system — a drug bound to an enzyme, for example — instead of using a more expensive method such as quantum mechanical calculations, one can achieve performances of (nearly) classical MD simulations while maintaining quantum mechanical levels of accuracy. Moreover, the new NAMD platform lets you use AI-based enhanced sampling methods, such as reinforcement learning-based adaptive sampling, to achieve timescales prohibited by pure MD simulations.",University of Illinois at Urbana-Champaign
NVIDIA GTC 2020,Implementation of Artificial Intelligence/Deep Learning Disruption Predictor into a Plasma Control System [P22517],"As described in NATURE (April 2019), Princeton's AI/deep-Learning software uses convolutional and recurrent neural network components to integrate complex information from both spatial and temporal big data to predict dangerous disruptive events in magnetic fusion plasmas with unprecedented accuracy and speed on top supercomputers featuring NVIDIA'S powerful Volta GPUs. Here, we report on improved capability of the software to output not only the “disruption score,” as an indicator of the probable onset of a disruption, but also a “sensitivity score” in real time to indicate the underlying reasons for the imminent disruption. As an indicator of possible causes for future disruptions, the “sensitivity score” can provide valuable physics-based interpretability for the deep-learning model results, and more importantly, provide targeted guidance for the control actuators when implemented into any modern plasma control system. This is a significant step in moving from modern deep-learning disruption prediction to real-time control—a major advance that brings novel AI-enabled capabilities needed for the future burning plasma ITER system.",Princeton Plasma Physics Laboratory
NVIDIA GTC 2020,Inference Path Optimization for Deep Reinforcement Learning on NVIDIA DGX-2 [P22339],"Our institution, the Shanghai Jiao Tong University High Performance Computing Center, is a school-level computing platform, supplying teachers and students in the whole school with computing power. In 2019, we bought eight DGX-2 for our new machine to support the school's research requirements. This poster presents one optimization case we did for our user to unleash the full power of the marvel machine. NVIDIA DGX is the most powerful computation system for artificial intelligence. However, there are still many challenges for a deep reinforcement learning system to take full advantage of the computing power of the DGX-2 platform, because of the limited PCI-e bandwidth, imbalance computation ability between CPU and GPU, and under-utilization of the GPU. We use Volta Multi-Process Service to increase GPU utilization. Introducing NUMA aware and workload balance further improves the computation efficiency. To alleviate the bottleneck of PCI-e bandwidth, we propose a residual compression method for states.",Shanghai Jiao Tong University
NVIDIA GTC 2020,MAGMA: Accelerating Linear Algebra Through Mixed-Precision and Tensor Cores [S21557],"The MAGMA library provides several GPU-accelerated linear algebra algorithms. We'll cover the mixed-precision (MP) algorithms for solving different linear algebra problems, such as linear systems of equations (Ax=b). Classic MP algorithms use two precisions to accelerate the solution of systems in double or double-complex precisions. Thanks to the introduction of half-precision in NVIDIA GPUs and the incredible performance of tensor cores, dual-precision MP algorithms can now accelerate systems in single precision as well as single-complex precision. Triple-precision MP algorithms can now solve systems in double and double-complex precisions. We'll show how to accelerate complex precisions using “half-complex” linear algebra kernels, which are not natively supported by the tensor core units.",University of Tennessee
NVIDIA GTC 2020,MemEAPF for Mobile Robot Path Planning on Jetson Platform [P21986],"We present a parallel implementation on the NVIDIA Jetson TX2 of the membrane evolutionary artificial potential field (memEAPF) algorithm for mobile robot path planning. The memEAPF algorithm is employed to achieve feasible paths, considering minimum path length, safety, and smoothness. The memEAPF algorithm combines membrane computing with a genetic algorithm — specifically, a membrane-inspired evolutionary algorithm with a one-level membrane structure and the artificial potential field method to find the parameters to solve efficiently the robot path-planning problem. The path-planning problem in mobile robots is one of the most computationally intensive tasks, so heterogeneous computing helps to gain performance. Employing GPUs processes data-intensive tasks efficiently — in this work, the evaluation of solution paths.",CETYS Universidad
NVIDIA GTC 2020,Mobile Manipulation Toward Cleaning Dining Tables [P22364],"Service robots are robots designed to work in human environments. Human environments naturally imply unstructured environments, and hence require advanced computer-vision algorithms to reliably operate. Historically, various attempts have been made at developing general-purpose robots capable of performing human tasks in cluttered environments; however, most fall short due to problems in perception and task/motion planning. With the rise of modern RGB-D sensors, deep-learning algorithms, and GPU-based hardware for deep learning, the circumstances are ripe for a new foray into developing a general-purpose robot with artificial general intelligence. Leveraging on these technologies, we’ll develop a service robot for use in a fast-food center to clear and clean tables. We'll present our experiences and findings during its development.",National University of Singapore
NVIDIA GTC 2020,Multiphysics Software Development in the Age of AI [P21687],"We'll explain the main ideas of physics-informed neural networks (PINNs) in the context of an industrial multi-physics problem—namely, designing a heat sink for the next generation of NVIDIA DGX systems. PINNs leverage the underlying laws of physics, often described in the form of partial differential equations (PDEs), to solve forward, inverse/data-assimilation, and model discovery problems. Our work addresses several major drawbacks encountered with the traditional methods of solving PDEs in terms of usability (not requiring arduous meshing), speed (ability to solve multiple geometries simultaneously), scalability (being embarrassingly parallelizable across clusters of GPUs), and expertise (leveraging past experience). PINNs are also agnostic to the physics type (whether fluids, thermal, or solid) and are capable of composing and linking multiple physics representations directly from a description of the physical phenomena (that is, equations, initial/boundary conditions/data, and geometry).",NVIDIA
NVIDIA GTC 2020,N-Body Simulation of Binary Star Mass Transfer [P21883],"Observations suggest that over 50% of the stars in our galaxy are part of a multiple-star system. Of these, binary systems are the most intensely studied. Their abundance and unique characteristics make binary systems invaluable sources of astrophysical data. Our study's concerned with contact-binary systems — a pair of stars in physical contact sharing a common envelope. Due to mass transfer between the stars, the structure and evolution of these systems differ greatly from single-star systems. Here, we develop an N-body model that simulates evolving contact-binary star systems. With it, we study the evolution of contact binaries, in particular the role mass transfer between stars plays in this process.",Tarleton State University
NVIDIA GTC 2020,Overcoming Latency Barriers: Strong Scaling HPC Applications with NVSHMEM [s21673],"For scientific advancement through HPC, ever-increasing simulation capabilities are not the only key to success. Obtaining timely results is often even more important. Reducing the time-to-solution generally requires the application to be strong-scalable. However, scaling up improved single-GPU performance faces many obstacles. We'll show you how to improve the strong-scaling on systems equipped with NVIDIA GPUs. Avoid or hide latencies by exploiting GPU-centric communication with NVSHMEM, an implementation of OpenSHMEM for GPUs. After introducing NVSHMEM, we'll share best practices gathered from using NVSHMEM for QUDA, a library for Lattice QCD on GPUs used by codes as MILC and Chroma. We show results obtained on fat-GPU nodes like DGX-1/2, as well as scaling them to 1,000 GPUs in InfiniBand-connected systems, including Summit.",NVIDIA
NVIDIA GTC 2020,Parallel Index-Based Search Operation for Database Systems via GPUs [P21987],"Handling a large number of queries concurrently is a crucial characteristic of today’s in-memory database system. By supporting such characteristics, index structure has a vital role in a database system. In recent years, GPUs have become the leading hardware for parallel computing. The unique architecture of high-performance computation, however, provides abundant opportunities for optimizing the algorithm toward better performance and achieving high utilization of GPU resources. We present our recent study in designing and optimizing parallel algorithms for index-search on GPUs. We also present techniques to optimize the search operation on both equality and range searches by using a novel clustering technique that can maximize the utilization of an on-chip GPU cache system. To evaluate our index structure, we compare the searching time with the best CPU SIMI index-based searching.",University Of South Florida
NVIDIA GTC 2020,Rapid Pathogen Genomics using Nanopore Sequencing and GPUs [P22462],This project employs a combination of advanced genomics methods in the context of pathogen surveillance. The MinION device is used for nanopore or third-generation DNA sequencing. The MinION is small and portable and can be used to monitor microbial communities directly in the field. Recently released tools using Clara Genomics Analysis SDK enabled rapid genome analysis with GPU-enabled hardware. We envision that our methods will be further developed for field-forward applications of public health.,University of Alaska Fairbanks
NVIDIA GTC 2020,Rebalancing the Load: Profile-Guided Optimization of the NAMD Molecular Dynamics Program for Modern GPUs using Nsight Systems [s21547],"We'll show how we more than doubled the performance of the Nanoscale Molecular Dynamics program on modern GPUs through the use of profile-guided optimization with Nsight Systems and Nsight Compute. NAMD has historically offloaded most computation to GPUs, leaving a much smaller amount of work and device management to the CPU. However, the tremendous performance available on modern GPUs has led to an imbalance, in which the CPU work becomes a bottleneck while the GPU is left idling. We'll show how we improve GPU utilization and overall application performance by moving the remaining workload to the GPU while reducing CPU overhead due to communication and synchronization, all discovered with the help of Nsight Systems. We'll also discuss ongoing efforts in tuning the performance of our most compute-intensive kernels with Nsight Compute.",University Of Illinois at Urbana-Champaign
NVIDIA GTC 2020,Running Multi-Messenger Astrophysics with IceCube Across All Available GPUs in the Cloud [S22206],"We'll report on a computational experiment that marshaled all globally available for-sale NVIDIA GPUs across AWS, Azure, and GCP. The net result was a peak of about 51,000 GPUs of eight different kinds, with an aggregate peak of about 380 PFLOPS fp32. The experiment used simulations for the IceCube Neutrino Observatory, an array of some 5,000 optical sensors buried deep within a cubic kilometer of ice at the South Pole. The sensors detect the signatures of shock waves created by particles from neutrino interactions passing through the ancient ice sheets. Simulation is needed to properly account for natural ice imperfections, and photon propagation codes are a natural fit for GPGPU computing. We'll provide both a summary overview and technical details of the infrastructure needed to create a supercomputer-like environment across multiple cloud providers, as well as an overview of the science behind IceCube and how GPU compute helps in advancing the scientific goals.",UC San Diego - San Diego Supercomputer Center
NVIDIA GTC 2020,Runtime Analysis of Spatial Structure: A CUDA Implementation of Minkowski Functionals [P21829],"Interested in characterizing spatial structure inherent in 3D scalar fields from simulated or imaged data? This poster presents an accelerated solution of the widely-used Minkowski functionals, using both OpenACC and CUDA on commodity GPUs. We'll present methods to minimize the memory footprint, and hence reduce data-transfer costs. Based on measurement frequency, an OpenACC rather than a CUDA solution might be appropriate. Next steps highlight the additional methods to further enhance and fine-tune the performance of the CUDA solution. Minkowski functionals have been widely applied in cosmology, material science, engineering, microbial ecology, and health care.",Abertay University
NVIDIA GTC 2020,Scaling Deep Learning Interpretability by Visualizing Activation and Attribution Summarizations [S21808],"We'll present Summit, an interactive system that scalably and systematically summarizes and visualizes the features that a deep learning model has learned, and how those features interact to make predictions. Summit introduces two new scalable summarization techniques: activation aggregation discovers important neurons, and neuron-influence aggregation identifies relationships among such neurons. Summit combines these techniques to create the novel attribution graph that reveals and summarizes crucial neuron associations and substructures that contribute to a model’s outcomes. Summit scales to large data, such as the ImageNet dataset with 1.2 million images, and leverages neural network feature visualization and dataset examples to help users distill large, complex neural network models into compact, interactive visualizations. We'll present neural network exploration scenarios where Summit helps us discover multiple surprising insights into a prevalent, large-scale image classifier’s learned representations and informs future neural network architecture design. The Summit visualization runs in modern web browsers and is open sourced.",The Georgia Institute of Technology
NVIDIA GTC 2020,Self-Supervised Robot Learning from Pixels [s21921],"How can robots learn manipulation skills from raw sensory input without external supervision? We'll propose an algorithm that acquires such general-purpose skills by combining unsupervised representation learning and reinforcement learning of goal-conditioned policies. Since the particular goals aren't known in advance, the agent performs a self-supervised ""practice"" phase where it imagines goals and tries to achieve them. We learn a visual representation with three distinct purposes: sampling goals for self-supervised practice, providing a structured transformation of raw sensory inputs, and computing a reward signal for goal-reaching. A retroactive goal-relabeling scheme further improves the sample efficiency of our method. Our off-policy algorithm is efficient enough to learn policies that operate on raw image observations and goals for a real-world robotic system, and substantially outperforms prior techniques.",Berkeley
NVIDIA GTC 2020,Sim-to-Real: Virtual Guidance for Robot Navigation [P22224],"We present an effective, easy-to-implement, and low-cost modular framework for completing complex navigation tasks. Our proposed method is based on a single monocular camera to localize, plan, and navigate. A localization module in our framework first localizes and acquires the robot’s pose, which is then forwarded to our planner module to generate a global path and its intermediate waypoints. This information, along with the pose of the robot, is then reinterpreted by our framework to form the “virtual guide”, which serves as a virtual lure for enticing the robot to move toward a specific direction. We evaluate our framework on a Husky robot in a number of virtual and real-world environments, and validate that our framework is able to adapt to unfamiliar environments and demonstrate robustness to various environmental conditions.",National Tsing Hua University
NVIDIA GTC 2020,*Top 5 Poster Nominee - Photometric Mesh Optimization for Video-Aligned 3D Object Reconstruction [P22276],"We address the problem of 3D object mesh reconstruction from RGB videos. Our approach combines the best of multi-view geometric and data-driven methods for 3D reconstruction by optimizing object meshes for multi-view photometric consistency while constraining mesh deformations with a shape prior. We pose this as a piecewise image alignment problem for each mesh face projection. Our approach allows us to update shape parameters from the photometric error without any depth or mask information. Moreover, we show how to avoid a degeneracy of zero photometric gradients via rasterizing from a virtual viewpoint. We demonstrate 3D object mesh reconstruction results from both synthetic and real-world videos with our photometric mesh optimization, which is unachievable with either naive mesh generation networks or traditional pipelines of surface reconstruction without heavy manual post-processing.",Carnegie Mellon University
NVIDIA GTC 2020,Toward Detailed Organ-Scale Simulations in Cardiac Electrophysiology [P21982],"The poster presents previous and ongoing work in the area of cardiac electrophysiology. It introduces the problem and describes the previous state of the art, as well as the improvement obtained through the use of GPUs. Furthermore, it addresses the challenges of organ-scale simulations, including the fact that whole-heart simulation with realistic calcium handling constitutes an exascale problem.",Simula Research Laboratory
NVIDIA GTC 2020,Toward Optimal Implementation of Lattice Boltzmann CFD Simulator for Multi-GPU Clusters [P21922],"We present our progress toward a general and efficient multi-GPU solver for single-phase fluid flow based on the lattice Boltzmann method (LBM). LBM is suitable for parallelization on GPUs. However, the amount of global memory available on a single GPU is the limiting factor. Recently, we've extended our LBM solver using the message-passing interface library in order to utilize many GPUs available on HPC clusters. Our poster describes the optimization strategies used in the solver, analyzes the weak-scaling efficiency, and shows the results of preliminary high-resolution simulations for problems related to our research projects.",Prague
NVIDIA GTC 2020,Uniform Tasking on GPUs and CPUs: Task Queues Reloaded [P21938],"GPU programming with a difference! The poster covers our insights into the development and optimization of a tasking framework for GPUs. We provide an overview of our CUDA C++ tasking framework for fine-grained task parallelism on GPUs. After a short trip into the world of persistent threads, synchronization mechanisms, and load balancing, we dig a bit deeper by finding and analyzing performance bottlenecks. On that point, we present diverse optimization strategies for task queues. First, we describe the implementation of task queues based on static memory allocation. Second, we show how to implement work sharing on a GPU through hierarchical task queues. Third, we present a thread coordination scheme to reduce contention on the task queues, thus keeping all threads busy. Finally, we analyze the performance gains reached through the described optimizations.",Jlich Supercomputing Centre
NVIDIA GTC 2020,Virtual GPU Computing for HPC: Improving System Utilization Through GPU Virtualization [S21827],"We'll discuss Clemson University's early experience of applying the NVIDIA vComputeServer in the high performance computing infrastructure to maximize their investment in GPU resources. We'll present two use cases: one for running the Metamoto simulation workload for the Open Connected Autonomous Vehicle project, and the other for general GPU-accelerated applications in a production HPC cluster.",NVIDIA
NVIDIA GTC 2020,Affordable Modeling of Complex Extreme Events in the Built Environment Using GPU-accelerated CFD [S21373],"We'll explain how Arup and Zenotech are making progress in resolving speed, accuracy, and cost challenges with CFD algorithmic advances, here exemplified by Zenotech’s zCFD code, and GPU acceleration. Using a particularly challenging turbulent flow phenomenon exhibiting fleeting but extreme pressure peaks as a test case, we'll show the outcome of validation tests carried out on a DGX-1 system and compared with comprehensive, high-quality wind-tunnel test data. We'll also demonstrate how in-situ visualization of 3D flows, using NVIDIA IndeX and ParaView Catalyst, opens powerful new possibilities for finding insights into complex fluid dynamical behaviors.",Arup
NVIDIA GTC 2020,Anomaly Detection of Diesel Engine Using Deep Learning with Xavier [s21394],"We've developed an anomaly-detection machine-learning algorithm that can be easily applied to a real machine using NVIDIA's Xavier. Many studies using machine-learning algorithms have been conducted recently, but an embedded board is essential in the field, where PC-based computing power such as construction equipment cannot be used. Using embedded boards like NVIDIA's TX2 and Xavier, it was easy to apply the trained model to real equipment.",Yonsei University
NVIDIA GTC 2020,A Software Systolic Array on GPUs [P22277],"We propose a versatile high-performance execution model, inspired by systolic arrays, for memory-bound regular kernels running on CUDA-enabled GPUs. More specifically, we build a virtual systolic array on the top of CUDA architecture. We formulate a systolic model that shifts partial sums by CUDA warp primitives for the computation. We also employ register files as a cache resource in order to operate the entire model efficiently. We demonstrate the effectiveness and the versatility of the proposed model for a wide variety of stencil kernels that appear commonly in HPC, and also convolution kernels (increasingly important in deep-learning workloads). Our algorithm outperforms the top reported state-of-the-art stencil implementations, including implementations with sophisticated temporal and spatial blocking techniques. For 2D convolution of general filter sizes and shapes, our algorithm is on average 2.5 times faster than NVIDIA’s NPP on Tesla V100 and P100 GPUs.",National Institute of Advanced Industrial Science and Technology
NVIDIA GTC 2020,Designing Better Cities with Deep Learning and Performance Simulation [S21493],"We'll present the use of conditional generative adversarial networks to augment the architectural design process with real-time performance analytics, including wind flow and internal daylight. Building-performance simulation tools have been used for decades in architecture and urban design, but their time and computational expense make them ineffective during the early stages when they are most relevant. We'll describe the challenges in building the dataset, encoding multi-variate parameters, and translating the model's output into the most useful format. We'll also present our exploration of different neural network architectures, selection of the appropriate loss function, and optimization of the training process to fully utilize the RTX 8000 GPU. Finally, we'll touch on our considerations in deploying the trained model with the goal of the real-time inference, and how we applied it on projects in practice.",Kohn Pedersen Fox Associates
NVIDIA GTC 2020,"Drones, Machetes, Fire, and VR: 21st-Century Tools for Social and Sustainable Impact [s21515]","We'll describe and compare workflows developed for projects on the islands of Kosrae, Micronesia and Vorovoro, Fiji. These projects—collaboratively executed with indigenous locals, universities, social-impact organizations, and technology corporations—built immersive experiences to support grant proposals, feasibility studies, and mitigation plans. Our climate is changing, and our global population is increasing. Remote, indigenous communities are developing to accommodate growth and responsible tourism in the face of catastrophic environmental and political forces. Technology is now available to support and promote development of remote communities in an environmentally and culturally sustainable manner. Specifically, photogrammetry and virtual reality enable effective decisions about mitigating the effects of climate change and developing sustainably by empowering stakeholders with intuitive, immersive experience of site conditions from new points of view.",McKinstry
NVIDIA GTC 2020,GPU Rendering for Architectural Visualization [S21657],"At Neoscape, we've created architectural visualization for over two decades. We've also waited days and weeks for our work to render in our CPU render farm — one of the biggest pain points for us and our clients. Now, NVIDIA’s RTX GPU technology rendering has turned days into hours. We'll explain how Neoscape has been experimenting and implementing some of these new technologies with different goals in mind for our diverse clientele, from Chaos group GPU rendering to Epic Games real-time ray tracing.",Neoscape
NVIDIA GTC 2020,High-Resolution Image Reconstruction on Supercomputers [P21724],"Computed tomography (CT) is a widely used technology that requires compute-intense algorithms for image reconstruction. We propose an efficient implementation that takes advantage of the heterogeneity of GPU-accelerated systems by overlapping the filtering and back-projection stages on CPUs and GPUs, respectively. We also propose a distributed framework for high-resolution image reconstruction on state-of-the-art GPU accelerated supercomputers. The framework relies on an elaborate interleave of MPI collective communication steps to achieve scalable communication. We also demonstrate the scalability and instantaneous CT capability of the distributed framework by using up to 2,048 V100 GPUs to solve 4K and 8K problems within 30 seconds and 2 minutes, respectively.",Tokyo Institute of Technology
NVIDIA GTC 2020,Immersive Reality Improving the Construction Industry [S22168],"The health care industry is continually looking to improve how doctors, nurses, and patients view hospitals and care centers throughout the country. With a primary focus on making hospitals and immediate care centers more welcoming, building teams are embracing new technology and using virtual mock-ups to revolutionize how patients and customers see and interact with spaces. Use of virtual reality mock-ups will bring notable changes in planning, acquiring new equipment, combining old and new standards, and incorporating feedback from doctors into ROI results.",Mortenson Construction
NVIDIA GTC 2020,Latest Advancements for Production Rendering with V-Ray GPU and Real-Time Raytracing with Project Lavina [S22197],"We'll cover the latest improvements in V-Ray GPU, including out-of-core rendering and RTX support through OptiX 7, as well as real-time raytracing with DXR and Project Lavina.",Chaos Group
NVIDIA GTC 2020,Leveraging OptiX 7 for High-Performance Multi-GPU Raytracing on Head-Mounted Displays [S21425],"NVIDIA recently introduced OptiX 7, which provides low-level access to the RTX technology and raytracing (RT) cores of the Turing architecture. Its CUDA-centric nature enables direct control over GPU resources — particularly in a multi-GPU context — which allows for a much more efficient scalability compared to previous OptiX versions. We'll show how to utilize OptiX 7 for full-frame raytracing on professional head-mounted displays (HMDs). Specifically, we'll demonstrate how OptiX is used in a dual-GPU setup to rapidly generate stereoscopic output, tailored to the specific optical characteristics of an HMD. Finally, we'll provide an overview of how RTX and OptiX 7 are integrated into ESI’s in-house rendering engine Helios, and will demonstrate the potential of real-time raytracing with practical use cases.",ESI Group
NVIDIA GTC 2020,Low-Cost OpenACC Porting of Matrix Solver with FP21-FP32-FP64 Computing: an Earthquake Application [P21886],"We show that OpenACC is useful for GPU porting of a practical application with FP21-FP32-FP64 mixed-precision computing. FP21 is our custom 21-bit floating-point data type for scientific computing. Here, we ported a soil liquefaction analysis solver that was developed for manycore CPU-based computers. The OpenACC-ported solver achieved a 10.7-fold speedup over the CPU-based solver on a system where the ratio of peak FP64 FLOPS was CPU:GPU = 1:10.2. It took only three weeks for a beginning GPU programmer to port the solver to GPU.",University of Tokyo
NVIDIA GTC 2020,N-body Adaptive Optimization of Lattice Towers [P21881],"There are tens of millions of power transmission towers In the United States alone. The dilemma central to their cost optimization is that a beam’s cross-sectional area is proportional to both its strength and cost — that is, a thicker beam, while able to support a given load with less strain, will cost more and weigh more heavily on the beams supporting it. By varying the cross section of each beam, we want to make towers as light and inexpensive as possible without sacrificing their structural integrity. Known as truss-sizing optimization, this problem is differential in nature and heavily dependent on tower geometry, lending it to a computational approach. Drawing inspiration from the atrophy and hypertrophy of muscles, we developed and evaluated an optimization algorithm that adaptively resizes beams based on their stress — a process that produces rapid results and allows the application of both static and dynamic loads, setting it apart from popular algorithms in this intensely studied field.",Tarleton State University
NVIDIA GTC 2020,PBR Material Creation from a Single Picture in Substance Alchemist [S22194],"After presenting the first AI-powered Delighter at GTC 2019, and after Substance Alchemist was released during Adobe Max in Nov 2019, we'll present the next steps of the technology and the work engaged to recover a full high-resolution tileable PBR material and how the RTX Tensor Cores boost the performance for the Substance Alchemist release to come in 2020.",Adobe
NVIDIA GTC 2020,Ray-Traced Virtual Reality in Omniverse [S22029],"Ominverse is a new platform developed by NVIDIA to share scenes and models between different editors and viewers. Ray tracing is used to accurately visualize content within the Omniverse Kit viewer. As quality ray-tracing effects (such as reflections, soft shadows, and ambient occlusion) are expensive to compute, we'll discuss how we were able to use eye-tracked foveation and warped-space rendering to achieve sufficient performance and quality gains for a virtual reality viewer. We'll also show how adding multi-frame explicit history reprojection to our de-noising strategy better handles the motion of VR interactions. To further improve performance, we'll discuss our strategies for dividing work between multiple GPUs. Streaming allows us to decouple the multi-GPU rendering server from the headset. Finally, we'll demonstrate our application to allow you to experience first-hand the benefits of eye-tracked foveation and ray tracing.",NVIDIA
NVIDIA GTC 2020,Real-Time Ray-Traced Ambient Occlusion of Complex Scenes using Spatial Hashing [S22170],"Ambient occlusion is an effective way to approximate global illumination: in essence, the closer a point is to its surroundings, the darker it gets. For real-time rendering, this effect is often approximated using screen-space techniques, leading to visible artifacts. Ray tracing provides a unique way to increase the rendering fidelity by accurately computing the distance to the surrounding objects, but it introduces sampling noise. Using the NVIDIA RTX technology available with Vulkan, we propose a real-time ray-traced ambient occlusion technique in which noise is removed in world space. Using spatial hashing for efficient storage, we'll cover all the technical challenges to make ambient occlusion a production feature usable in CAD viewports with scenes comprising thousands of instances and hundreds of millions of polygons.",NVIDIA
NVIDIA GTC 2020,Semi-Supervised Three-Dimensional Reconstruction with Generative Adversarial Networks [P21908],"Existing methods for 3D reconstruction often produce holes, distortions, and obscured parts in the reconstructed 3D models, or they can only reconstruct voxelized 3D models for simple isolated objects. So they are not adequate for real usage. This poster's focus is to achieve high-quality 3D reconstruction performance by adopting the generative adversarial network (GAN) approach. We'll propose a novel, semi-supervised 3D reconstruction framework (SS-3D-GAN), which can iteratively improve any raw 3D reconstruction models by training the GAN models to converge. This new model only takes real-time 2D observation images as the weak supervision and doesn't rely on prior knowledge of shape models or any referenced observations. Through the qualitative, quantitative experiments and analysis, SS-3D-GAN shows compelling advantages over the current state-of-the-art methods on the benchmark datasets. We'll also prove the acceleration effect of NVIDIA GPUs in the 3D reconstruction research and applications.",NVIDIA
NVIDIA GTC 2020,Tensor Core Accelerated Sparse GEMM [P21866],"Sparse matrix multiplication is an important tool in many scientific applications, like graph analytics and deformable object modeling. It's also valuable in deep-learning networks with sparse inputs. We'll present a novel way to utilize the tensor cores of the latest NVIDIA GPUs to accelerate sparse matrix-matrix multiplication. Our approach splits the operand matrices into tiles and schedules them to tensor cores for fast matrix multiplication, outperforming two state-of-the-art libraries, CUSP and cuSPARSE, by an average of 2x.",University of Cordoba
NVIDIA GTC 2020,Virtual Reality's Continued Impact in the World of AEC [s22129],"Not too long ago, it seemed that every enterprise company had an R&D team trying to decide if ""video game technology"" could be used for their visualization needs. The days of pilot projects have come and gone, and virtual reality is definitely here to stay. In this talk, Theia Interactive will introduce you to a variety of AEC customers using virtual reality and Unreal Engine on a daily basis to give them an edge in this new era of graphics.",Theia Interactive
NVIDIA GTC 2020,A Study of Pedestrian Protection CAE Using GAN [S21438],"According to the World Health Organization, there are over 270,000 pedestrians involved in traffic fatal accidents. That is 22% of all traffic fatalities verified in the world in 2013. To reduce pedestrian fatalities, third-party organizations in many countries have held New Car Assessment Program (NCAP) tests to evaluate pedestrian-protection performance. For that reason, an efficient method to design pedestrian-protection performance is required for automobile development for all over the world. Computer-aided engineering (CAE) is often used to verify pedestrian-protection performance. But as the necessity for pedestrian protection expands globally, expectations to improve efficiency have recently risen. We decided to reduce CAE verification time and improve the accuracy of CAE using deep learning. We also studied visualizing the basis for judgment in the trained models.",Honda R&D
NVIDIA GTC 2020,Automating DNN Design for DRIVE AGX: Platform-Aware Neural Architecture Search [S21666],"In the past few years, wide applications of deep neural networks (DNN) have contributed to significant progress in various fields such as image classification, object detection, and segmentation. Most of the successful DNNs, such as VGG and ResNet, are designed by humans, which requires in-depth domain expertise and effort. While DNNs have become deeper and wider, the need for fast inference is increasing on (edge) computing devices, while accuracy must be maintained. Therefore, developing state-of-the-art neural networks for resource-constrained applications has become challenging. We'll present our progress on the automated design of neural networks using hardware-aware neural architecture search (NAS) techniques. We show concrete end-to-end examples from differentiable and latency-reflected search of optimal network architectures to their deployment on NVIDIA’s DRIVE AGX platforms using TensorRT for autonomous-driving-related applications.",NVIDIA
NVIDIA GTC 2020,Beyond-Line-of-Sight (BLOS) Perception System for Autonomous Vehicles [P21842],"Recently, autonomous vehicle perception systems show great performance based on deep-learning technology. Cameras and lidar are commonly used for autonomous driving. However, onboard sensors naturally have limited line-of-sight. Our work introduces a BLOS (beyond-line-of-sight) perception system that mutually complements communicated perception with V2X technology and local perception with onboard sensors. The proposed system is integrated into a full-scale vehicle, and we validated the performance and potential by experiments in the real world.",KAIST (Korea Advanced Institute of Science and Technology)
NVIDIA GTC 2020,Bike-Share Demand Prediction from Partial Future data Using Conditional Variational AutoEncoders [P21739],"Recently, bike-sharing services are working worldwide. One important aspect of bike-share management is to periodically rebalance the positions of the available bikes. Because the bike demand varies by and over time, the number of bikes at each bike-port tends to become unbalanced. To efficiently rebalance a bike-share system, it is essential to predict the number of bikes in each bike-port. In this poster, we propose a method to predict the bike demand at each bike-port every hour, up to 24 hours ahead. This method is based on Variational AutoEncoders and Sequence-to-Sequence Neural Networks. We called this method “Conditional Variational AutoEncoders considering Partial Future data”. In the experiment, our proposal method showed higher prediction accuracy than the other methods.",INC.
NVIDIA GTC 2020,Bringing the Autodesk VRED Raytracer to the GPU [s21344],"Autodesk VRED helps designers and engineers create product presentations, design reviews, and virtual prototypes using interactive raytracing and analytic render modes. We'll show how we brought the VRED raytracer to the GPU using NVIDIA Optix. We'll also talk about specific requirements and challenges we faced, and demonstrate the results we achieved utilizing NVIDIA RTX.",Autodesk
NVIDIA GTC 2020,Coverage-Driven Verification for Ensuring AV and ADAS Safety [S22183],"One of the main evolutions the autonomous vehicle industry has seen over the last couple of years is that many companies have reached the point where they can make the fundamentals of AVs work just fine — sensing, planning routes, controlling the vehicle. The really hard part, though, is the validation and verification of all of the hazardous edge cases. We'll address these tough questions about how to enable sufficient verification to ensure AV safety:
How do you handle the infinite space of all the possible driving scenarios that need to be tested?
How can you control the various simulators and other execution platforms used for running the testing scenarios?
How can functional coverage help to systematically go over all the known and unknown hazardous edge cases?
How can you provide metrics that show that the remaining hazardous test cases result in a risk that is acceptable?",Foretellix
NVIDIA GTC 2020,Deep Learning for 3D Vision (Point Clouds) [S21112],"Learning on 3D point clouds is vital for a broad range of emerging applications such as autonomous driving, robot perception, VR/AR, gaming, and security. Such needs have increased recently due to the prevalence of 3D sensors such as lidar, 3D cameras, and RGB-D depth sensors. Point clouds consist of thousands to millions of points and are complementary to the traditional 2D cameras in the vision (or multimedia) community. 3D learning algorithms on point cloud data are new, and exciting, for numerous core problems such as 3D classification, detection, semantic segmentation, and face recognition. The tutorial covers the 3D sensors, 3D representations, emerging applications, core problems, state-of-the-art learning algorithms (for example, voxel-based and point-based), and future research opportunities. We'll also showcase our leading work in several 3D benchmarks such as ScanNet, KITTI, etc., and efficient neural network training (with data parallelism) by NVIDIA GPU platforms (for example, DGX-1).",National Taiwan University
NVIDIA GTC 2020,Drones as a Tool for Development and Safety Validation of Automated Driving Functions [P21913],"Automated driving heavily relies on data-driven methods. Large datasets of real-world measurement data, in the form of road-user trajectories, are crucial for several tasks, such as road-user prediction models or scenario-based safety validation. Using a drone has the major advantage of recording naturalistic behavior. Due to the ideal viewing angle, an entire scenario can be measured with significantly less occlusion than with sensors at ground level. Both the class and the trajectory of each road user can be extracted from the video recordings with high precision using state-of-the-art deep neural networks. Therefore, we have created a large-scale urban intersection dataset with naturalistic road-user behavior using camera-equipped drones. The resulting dataset contains road users including vehicles, bicyclists, and pedestrians at intersections in Germany, and is called inD. The dataset is available online for non-commercial research at: http://www.inD-dataset.com.",fka GmbH
NVIDIA GTC 2020,Eliminating Hidden Bias in Autonomy and Beyond [S22701],"Speaker: Deepti Mahajan, Machine Learning Engineer, Ford Greenfield Lab We tend to trust that the systems that govern our day-to-day life—standards followed by industry, regulations passed by governments—are based on thorough research. However, in many cases, the data used to design these systems fails to represent us all equally and can even reinforce or amplify existing biases. As designers and engineers of the next generation of mobility, how can we learn to recognize hidden bias in the data we utilize and information we take for granted, and thus work towards creating systems that serve everyone?",Ford Motor Company
NVIDIA GTC 2020,"End-to-End Learning with Combined ""Real-World"" and ""Sim-World"" Datasets [P22060]","Training end-to-end learning for self driving requires large and diverse data. We can deal with that if we collect the data not just from the real world, but also simulations. Therefore, we built a customized map based on the real-world proving ground, KIAPI in Korea, using the Unreal Engine. By using the map in the open-source simulator, CARLA, we could collect both simulated and reality-based datasets of KIAPI in parallel. We integrated both datasets in several different ratios and evaluated their validity by training an end-to-end learning network with them.",KAIST
NVIDIA GTC 2020,Generating Diverse and Photorealistic Synthetic Data for Real-World Perception Tasks [S21321],"We'll cover these two related topics: First, leveraging generative adversarial networks for style transfer to diversify simulated images rendered in simple domains (that is, easier to render realistically, such as daytime) into photorealistic images in different weather and lighting conditions, using domain translation models (such as day-to-night, clear-to-rainy, clear-to-snowy) learned once from generic real-world datasets; and second, investigating the role of the discriminator’s receptive field in unsupervised sim-to-real image translation. We'll show that reducing the discriminator’s receptive field is directly proportional to improved structural coherence during translation in scenarios where the real and simulated images used for training have mismatched content — a situation often encountered in real-world deployment. Prior knowledge in computer vision and deep learning will help you get the most out of this session.",Ford Motor Company
NVIDIA GTC 2020,Geolocation of Traffic Lights and Signs Using Dashcam: Toward Low-Cost Map Maintenance [P21912],"HD maps for autonomous driving need to be maintained as close to the current real world as possible, but it's quite costly and time-consuming to employ many workers or special mobile-mapping systems. To solve that, we're developing technologies and systems that extract information required for map maintenance from low-cost dashcam videos by utilizing rapidly advancing computer vision techniques. This poster introduces our current basic pipeline to detect and geolocate traffic lights and signs captured in dashcam videos. We developed our own dataset by collecting videos from vehicles running on various Japanese roads. Experimental evaluation using the dataset demonstrates that our system can detect objects with more than 85% accuracy, and can estimate their locations within less than 10 meters under the Global Navigation Satellite System.",Ltd.
NVIDIA GTC 2020,GPU-Accelerated Data Pipeline and Machine Learning on DRIVE AGX using RAPIDS [S21665],"We'll present the extended capability of RAPIDS on the DRIVE AGX platform by demonstrating how it enhances the in-car user experience, with examples. ML algorithms are used extensively to address various challenges in autonomous cars. They include complex, multi-stage data science pipeline, sensor data processing, modeling, and analytics to accomplish new ML-based applications. Potential applications include recommender systems through driver or vehicle personalization, visual analytics of driving data, classification of driver condition, driving scenario, and more. In many cases, application workload runs on CPUs or ECUs, leading to performance bottlenecks as data size and their computes increase. Application workload can be parallelized, causing significant speedup, using GPUs. NVIDIA developed RAPIDS to accelerate entire end-to-end data science and analytics pipelines on GPUs. In this talk, we present extended capability of RAPIDS on NVIDA DRIVE AGX platform by demonstrating how RAPIDS works.",NVIDIA
NVIDIA GTC 2020,Improving Performance Limits of Obstacle-Avoidance Driving with Randomized Model Predictive Control Using GPU [P22242],"Our poster presents improvements in the performance of obstacle-avoidance driving enabled by a GPU-based controller. A 1:10-scale remote control car is driven in a 60-cm wide track with two parked cars acting as obstacles. The controller is model-predictive with nonlinear constraints. Since a sampling-based approach is used to solve this nonlinear optimization problem, computation speed becomes a constraint as sample size increases. We use parallel computing to solve the sampling-based optimizations. Experiments using our RC car showed that we could achieve >100% increase in driving speed and >40% improvement in cost-function values.",Nagoya Univeristy
NVIDIA GTC 2020,Object Recognition and Tracking Utilizing Millimeter-Wave Radar by Deep Neural Networks [s21188],"All-weather sensors are necessary for autonomous-driving Level 3 and higher. Millimeter-wave radar is the most robust sensor for adverse weather. However, the signal is noisy and fluctuated, and the resolution is low. Thus, recognition using the radar is difficult. Deep-learning algorithms are an effective solution. We'll show a method to classify and track objects in driving scenes with a high-resolution millimeter-wave radar applying long short-term memory. We designed and compared various types of input features and LSTM for our measured dataset and achieved high accuracy through cross validation. We'll also show a method to reconstruct shapes of parking spaces and cars with convolutional neural networks. Parking cars were scanned with side radar. The reflection signals were accumulated, and the shape was estimated by semantic segmentation framework, applying CNN for the ground-truth shape, annotated by a lidar.",Toyota Technological Institute
NVIDIA GTC 2020,Optimizing TensorRt Conversion for Real-Time Inference On Autonomous Vehicles [S22198],"TensorRt optimizes neural-network computation for deployment on GPU, but not all operations are supported. Reduced precision inference speeds up computation, but can cause regressions in accuracy. We'll introduce Zoox TensorRt conversion pipeline that addresses these problems. TensorRt compatibility checks are involved at the early stages of neural-network training to ensure that incompatible ops are discovered before wasting time and resources on full-scale training. Inference accuracy checks can be invoked at each layer to identify operations not friendly to reduced-precision computation. Detailed profiling reveals unnecessary computations that aren't optimized inside TensorRt, but can be optimized by simple code changes during graph construction. With this pipeline, we've successfully provided TensorRt conversion support to neural networks performing various perception tasks on the Zoox autonomous driving platform.",Zoox
NVIDIA GTC 2020,Pacefish: GPU-Accelerated CFD from Scratch to Market [S21926],"Gain insights into the challenge of developing GPU-accelerated computational fluid dynamics software, from concept to product. Pacefish transforms the power of up to 16 NVIDIA GPUs into a predicted physical flow behavior around complex geometries like cars, buildings, cities, or in rooms. This helps engineers design magnificent cars having better driving capabilities at lower fuel consumption and higher cruising range, for example. Remarkable GPU-to-CPU-node speedup on the order of 20-30x allows Pacefish to drop the costs per simulation by a factor of 10. We'll talk about milestones on the road from scratch to market that we started in 2007, and show some results emphasizing the current product state. You'll learn the lessons from our most important technological decisions from the initiator, founder, and main developer himself.",Numeric Systems GmbH
NVIDIA GTC 2020,Panoptic Segmentation DNN for Autonomous Vehicles [S21879],"We'll present our NVIDIA DriveAV's Panoptic Segmentation Deep Neural Network (DNN), which can be used for semantic and instance segmentation of complex scenes for self-driving car scenarios, such as complex urban areas, congested traffic, construction zones with unusual activities, and so on. With Panoptic Segmentation DNN, input images can be accurately parsed for both semantic segmentation (which pixels represent which object class), as well as instance content (which pixels represent which object instance). Planning and control modules can use panoptic segmentation results to better inform autonomous driving decisions. We'll cover our highly accurate GT dataset, DNN architecture, our multi-task training process, and our real-time inference (that includes post-processing steps) on vehicles' AGX compute. Our network achieves state-of-the-art accuracy and runs at 7ms end-to-end on NVIDIA AGX GPUs. We'll show videos of our experiments on a real vehicle in various challenging conditions.",NVIDIA
NVIDIA GTC 2020,Precise Ultra HD Map Data as Basis for NVIDIA DRIVE Sim Virtual Testing and Simulation [S22273],"Digital road data is the basis for virtual testing and simulation. The roads used for virtual testing and simulation have to be identical digital twins of the real-world roads. 3D mapping presents the technical solution for digitizing test tracks, race tracks, and public roads with high-end mobile surveying using high-resolution scanners and multiple cameras. The technology is used to produce precise high-definition reference maps in OpenDRIVE format, which are either used as basis for virtual simulation and testing or as reference map in the car for autonomous driving development. We'll show various project examples that were completed as the basis for NVIDIA DRIVE Sim through 2019.",3D Mapping Solutions GmbH
NVIDIA GTC 2020,PredictionNet: Predicting the Future in Multi-Agent Environments for Autonomous Vehicle Applications [S21899],"Predicting the future trajectories of road agents is an import part of the planning&control stack in autonomous vehicles. Deep-learning approaches can be superior to classical methods in this domain, because neural networks can learn to use context and environment as a prior to improve prediction. We'll present PredictionNet — a deep neural network (DNN) that can be used for predicting future behavior/trajectories of road agents in autonomous-vehicle applications. Our DNN takes a rasterized top-down view of the world provided by the perception system and computes future predictions from past observations. We'll present its architecture, training-data collection process, and our training procedures. We'll also show video demos of live predictions on our self-driving car.",NVIDIA
NVIDIA GTC 2020,PyTorch-TensorRT: Accelerating Inference in PyTorch with TensorRT [S21671],"TensorRT is a deep-learning inference optimizer and runtime to optimize networks for GPUs and the NVIDIA Deep Learning Accelerator (DLA). Typically, the procedure to optimize models with TensorRT is to first convert a trained model to an intermediary format, such as ONNX, and then parse the file with a TensorRT parser. This works well for networks using common architectures and common operators; however, with the rapid pace of model development, sometimes a DL framework like Tensorflow has ops that are not supported in TensorRT. One solution is to implement plugins for these ops. Another is to use a tool like TF-TRT, which will convert supportable subgraphs to TensorRT and use Tensorflow implementations for the rest. We'll demonstrate the same ability with PyTorch with our new tool PTH-TRT, as well leveraging the PyTorch API's great composability features to allow users to reuse their TensorRT-compatible networks in larger, more complex ones.",NVIDIA
NVIDIA GTC 2020,RTX-accelerated Stellar GPU GI – Behind the Scenes of the SIGGRAPH 2019 Demo of the 3DS Global Illumination Renderer [s21315],"We'll give an overview of the technology behind the 3DS global illumination renderer demo, the challenges building the demo, the insights gained from it, and the features developed since. Stellar GPU GI is the GPU backend of the global illumination renderer developed by Dassault Systèmes. First launched in the 3DEXPERIENCE platform in early 2019, it enables rendering experts and novices alike, from design, simulation, and marketing, to generate interactive and offline high quality renderings. Its features and performance, especially in combination with RTX GPUs and RTX servers, were demonstrated to the world at SIGGRAPH 2019.",Dassault Systemes
NVIDIA GTC 2020,Scalable Storage Environments Optimized for Autonomous Driving (Presented by DDN) [S22671],"Continued advances in the types and number of sensors used by autonomous vehicle companies is leading to developments. It is also leading to data management challenges on a massive scale. The autonomous mobility industry is an entirely new ecosystem combining sensors and other physical components, security, high performance computing technologies, consumer electronics, mapping and geolocation services and a variety of standard IT solutions. With vehicles generating up to 80TB per day, connected car initiatives can demand exabytes of data on a daily basis. We will explain how intelligent, optimized data environments for AI and HPC can streamline and tame the data onslaught. Parallel data paths, diverse data services, remote data caching and tight integration with advanced computing platforms, like NVIDIA DGX systems combine to enable these most scalable solutions.",DDN
NVIDIA GTC 2020,"Sensor Processing with the NVIDIA DriveWorks SDK: Abstraction, Algorithms, and Acceleration [S21714]","Autonomous vehicles (AV) rely on sensors to represent the world around them, so onboard processing must react to rapidly changing environments. The NVIDIA DriveWorks SDK enables developers to implement such AV solutions by providing an exhaustive library of software modules and tools that leverage the computing power of the NVIDIA DRIVE AGX platform. With DriveWorks, developers can focus on their applications instead of spending time on fundamental functionality and infrastructure. Our session will cover the DriveWorks Sensor Abstraction Layer, a unified interface for sensor life-cycle management, timestamp synchronization, and recording. We’ll then discuss the DriveWorks optimized low-level image and point cloud processing modules for processing incoming sensor data to enable advanced AV algorithms. DriveWorks also supports the DRIVE AGX hardware engines so that these modules can seamlessly run across the Xavier SoC, providing flexibility and performance.",NVIDIA
NVIDIA GTC 2020,Smart Cities in the Cloud: NVIDIA Metropolis on Red Hat OpenShift [S21994],"We'll talk about the collaboration between NVIDIA and Red Hat, and demo the integration between the NVIDIA's Metropolis application running on EGX and Red Hat OpenShift (Kubernetes) in the public cloud.",NVIDIA
NVIDIA GTC 2020,Super SloMo: High-Quality Estimation of Multiple Intermediate Frames for Video Interpolation [P22420],We present a computational approach to synthesize slow-motion videos from a plain one.,Amherst
NVIDIA GTC 2020,Temporal Information Prediction for Perception in Autonomous Vehicles [P22335],"Self-driving cars, which have gained significant interest in academia and industry, have become a reality for several automotive companies. Temporal information in an autonomous vehicle, including time-to-collision and object 2D/3D motion, are essential input information for different autonomous vehicle system components, such as automatic cruise control and automatic emergency braking. It allows the autonomous vehicle to accurately understand its surrounding obstacles and provide the critical information to the control system. It can also be used to predict movement of the other objects and estimate their intents. We propose using a recurrent neural network for temporal information prediction. This increased robustness to the temporal prediction for non-rigid objects, such as pedestrians; extended the detection range for faraway objects; reduced noise in prediction; and utilized context pixels of the obstacles.",NVIDIA
NVIDIA GTC 2020,Terrain Traversability Estimation using Normal Distribution Transform on GPU in Amazon Scout [S21179],"Scout is a autonomous robot from Amazon for delivering packages. Mapping its surroundings in real time to identify traversable space is critical for autonomous driving. We'll discuss how we implemented a technique called ""3D Normal Distribution Transform"" (3D NDT) on NVIDIA TX2 to calculate surface traversability and help Scout navigate the neighborhood autonomously and safely. First, we'll introduce what 3D NDT is and what it is used for. Then we'll discuss how we implemented NDT in Amazon Scout to calculate surface traversability to help the robot to avoid obstacles and navigate safely. We'll highlight how we implemented NDT on NVIDIA TX2 GPU, achieving more than 10x faster run time than before, and how that greatly improved Scout's navigation. We'll also go into the challenges and innovation involved.",Amazon
NVIDIA GTC 2020,Toward Large-Scale Steady-State Computational Fluid Dynamics Acceleration with Point Cloud Networks [P22049],"Computational fluid dynamics (CFD) simulation has accelerated product development in the automotive, aerospace, and biomedical industries. Instead of constructing expensive prototypes, a design’s performance can be characterized in simulation. Unfortunately, simulation itself has become a new bottleneck. CFD solvers can take hours, days, or even months to solve the partial differential equations that characterize a design. Deep neural networks trained on CFD solutions can provide approximate solutions in seconds. Point cloud networks can process the unstructured meshes that define most realistic CFD problems. We evaluate one such network (PointNet) on a 2D 131k point cloud dataset. The network makes accurate approximations to the true solution more than 230x faster than a competing GPU solver. With this approach, engineers can evaluate designs more quickly to build a superior product.",NVIDIA
NVIDIA GTC 2020,Ultra-Fast Radar Simulation for Radar System Design and Automotive Applications [s21966],"Ultra-fast, physics-based radar simulations are required to design and deploy hardware-in-the-loop systems for applications such as autonomous vehicles and driver assistance systems (ADAS/AV). Rapid radar image generation is also an enabling technology for AI algorithms, vastly expanding the data sets to train and test the algorithms. We're developing an ultra-fast, end-to-end, GPU-accelerated radar image simulation engine for automotive, AI, and other applications. The shooting and bouncing rays (SBR) technique generates physics-based range-Doppler images of dynamic driving scenarios in urban settings. GPU kernels using CUDA, OptiX, and cuFFT propagate radar energy from the radar, through the scene, and back to the receiver to generate range-Doppler images displaying distance and relative velocity of surrounding objects. ANSYS will discuss the new solver and show results of automotive scenarios in busy, complex environments. The solver shows promise toward our goal of ultra-fast radar simulation.",ANSYS
NVIDIA GTC 2020,"Virtual Validation of Automated Vehicle Safety: Challenges, Lessons Learned, and Opportunities [S21658]","We'll explain the need for virtual validation (VV) of automated vehicles (AV), identify the general strengths and weaknesses of VV, briefly review the present landscape of tools and solutions available to perform AV simulations, and list the major building blocks of a good virtual testing process (VTP). We'll also discuss the potential of NVIDIA DRIVE Constellation toward implementing a meaningful VTP for developers, regulators, and service providers. We'll outline some key challenges and opportunities to motivate further research. We'll also introduce two research projects at CETRAN (Nanyang Technological University, Singapore): first, modeling the sensing and perception (S&P) errors using a Perception Error Model (PEM) and studying their impact on decision-making (AV behavior), and second, developing methods to evaluate the holistic fidelity of simulation toolchain and judge its effectiveness in representing reality. We'll present some preliminary findings and a research roadmap.",Nanyang Technological University
Big Data 2019,A Practical-ish Introduction to Data Science,"Data Science has been described as the sexiest job of the 21st Century. But what is Data Science? And what has Machine Learning got to do with all of this?
In this talk, Mark will share insights and knowledge that he has gained from building up a Data Science department from scratch. 1. He’ll begin by defining what Data Science is, how it is related to Machine Learning and share some tips for introducing Data Science to your organisation.
2. Next up we’ll run through some commonly used Machine Learning algorithms used by Data Scientists, along with examples for use cases where these algorithms can be applied.
3. The final third of the talk will be a demonstration of how you can quickly get started with Data Science and Machine Learning using Python and the Open Source scikit-learn Library.",Bouvet Norge AS
Big Data 2019,You can AI like an Expert,"The talk will discuss Wolfram's progress towards an entirely automated workflow for machine learning that makes it possible for anyone who can code to make use of AI methods.

The talk will assume no knowledge of machine learning and will introduce key concepts before showing how to use the Wolfram Language to create advanced predictors from  Examples will be shown from computer vision such as content recognition, scene recogniton and segmenting images into components relevant for autonomous driving applications. The speaker will attempt to train a predictor using live camera images to be able to recognize the hand gestures of the game Rock, Paper, Scissors.scratch, in seconds. Use will be made of the Wolfram Language functions Predict and Classify which automate the process of data encoding, feature extraction, method selection, training, end decoding. The net result is a single function that produces a ready-to-use classifier or predictor, directly from raw data. Examples will be shown from computer vision such as content recognition, scene recogniton and segmenting images into components relevant for autonomous driving applications. The speaker will attempt to train a predictor using live camera images to be able to recognize the hand gestures of the game Rock, Paper, Scissors. The talk will explain how symbolic computation is the they key ingredient needed to make such automation possible. Symbolic computation provides a unified representation of structured data such as images, sounds, documents, databases, networks as well as the machine learning models themselves. The talk will go on to show that symbolic representation also helps in automating the transition from research experiments to the production deployment of AI services.",Wolfram Research Europe Ltd.
Big Data 2019,Deep Learning for Lazy People... Neural Architecture Search with Automated Machine Learning,"Deep Learning models are great, but choosing the right architecture is not easy. Many times, the easiest way of getting the best architecture is just by trying.
It would be nice to have a clairvoyant that is able to tell us the best architecture, right? We cannot have a clairvoyant but we have tools, like Automated Machine Learning, that are able to find the best architectures just with a few lines of code.This talk will cover the use of Automated Machine Learning (AML) for Deep Learning Architectures, but we will also see other uses of AML.",RavenPack
Big Data 2019,Predicting the Moment of Birth using Sensor Data in Dairy Cows,"Stillbirth, defined as calves that die during unobserved birth is often seen as an indicator of lowered animal welfare in dairy cows. Sensors have been proposed as a tool to support dairy farmers but accurate calving prediction models are often lacking. In this session, a machine learning data pipeline will be described using the spark ML framework. Heavy lifting and feature preparation for sensor data from 1331 cows on 8 herds from 21 days before until the day of calving was performed using sliding windows and time series analysis. In total a set of 100+ features was used in a random forest classification model trained and tested using different cross validation approaches. These different approaches were applied to avoid overfitting to specific cow and herd effects, assuring robust industry applicable models. The data pipeline created a ML model to predict the exact day of calving in dairy cows with an accuracy of 95%, offering a method to diminish unobserved calvings and stillbirth in dairy cows.",Utrecht University
Big Data 2019,Knowledge and AI Powering Microsoft & Office 365 Products,"Knowledge, Data and AI have been reshaping the way we live and work for the past few years, and the fact is that this is just the beginning.
Everyone has been using AI-and-knowledge-infused products and services every day, sometimes in ways that may not seem obvious. One of David’s goals in this session will be to explain how these non-obvious scenarios are being powered with Knowledge and AI (and how/why, through improvements in technology, these will just get better).
He wants the audience to understand how intelligent scenarios work by taking them through a journey of understanding Knowledge Graphs (including how knowledge is ingested and/or mined), showing real scenarios being powered by these, and explaining how this technology can/will be used in the enterprise world in products like Office 365 and Microsoft Search. He will explain how the technologies are improving and how this will cause the experiences to improve as well in the next few years.
As part of the session, David will define Artificial Intelligence and will show, with many examples, how this works. He will explain how AI will help people and enterprises and will explain why now is the perfect time for AI. He will show data and numbers which show how big data and AI are (and will become even more) crucial to organizations.
David will explain the difference between Knowledge & AI, and how both are required to power truly intelligent scenarios.
He will talk about key ingredients that will adequately position organizations to be able to power AI/Knowledge scenarios at scale, and he will present Microsoft’s AI Portfolio.
Since Davis currently works on Office 365 and Microsoft Search products, he will also show the latest in these areas.
Finally, for developers in the audience, David will go through Microsoft Cognitive Services that can allow them to infuse AI/Knowledge into their own apps/products.",Microsoft
Big Data 2019,Practical Data Science - How to Track Your Development Process with DVC,"Datacentric applications utilising machine learning models have evolved into common solutions. Many projects however still suffer from a lack of good patterns and practices, when developing such powerful technologies. Digging down into the nitty-gritty details, we explain how you can use DVC to version all parts of your projects: From the dataset, over gluecode up to the model itself. But wait, there’s more! We show you code that covers the full development cycle, including experiments and reproducability, as well as release and deployment of your model to machines in the wild.",codecentric AG
Big Data 2019,(Un)ethical Artificial Intelligence: How to Keep the AI Fair for Everyone,"This presentation highlights the current ethical problems that we face while building artificial intelligence solutions. The artificial intelligence systems based on machine learning algorithms are entering our products at an increasing rate. Unfortunately, keeping these systems fair is hard and a lot of hidden biases enter the models, even if the developers had the best intentions. We will investigate some of the failures of the AI systems and explore ways how to keep them fair for everyone.",Vinted
Big Data 2019,"Secure IoT Command, Control, and Exfil with Apache MiNiFi","Apache MiNiFi is a lightweight application which can be deployed on hardware orders of magnitude smaller and less powerful than the existing standard data collection platforms. Not only can this data be prioritized and have some initial analysis performed at the edge, it can be encrypted and secured immediately. Local governance and regulatory policies can be applied across geopolitical boundaries to conform with legal requirements. And all of this configuration can be done from central command & control using an existing Apache NiFi instance with the trusted and stable UI data flow managers already love. Recent events (Mirai, Dyn DDOS, etc.) have demonstrated the power of distributed botnets consisting of unsecured IoT devices and reinforced the need to securely command and control IoT devices while also ensuring data is only made accessible to authorized parties.",Cloudera
Big Data 2019,Big Data Legal Issues. GDPR and Contracts,"In a recent project, iunera sought to determine if it is possible to predict crypto currency exchange rates by utilizing social data from Twitter. Tim will talk about their experiences and describe how they leveraged online learning in conjunction with social data to determine if they are able to predict future currency exchange rates. He’ll point out the general architecture and describe the most interesting findings.
The audience can learn from Tim’s experiences and pitfalls. The new possibilities that open up with more and more Kappa architectures in enterprises leverage new possibilities for real online learning. Therefore, the aspiration is to bring this field in general forward, by sharing insights.",Legal IT Group
Big Data 2019,"Predicting Cryptocurrency Exchange Rates with Stream Processing, Social Data and Online Learning","In a recent project, iunera sought to determine if it is possible to predict crypto currency exchange rates by utilizing social data from Twitter. Tim will talk about their experiences and describe how they leveraged online learning in conjunction with social data to determine if they are able to predict future currency exchange rates.",iunera GmbH & Co. KG
Big Data 2019,Routing Billions of Events a Day: How we Do Routing in Schibsted,"In this talk Carlos will present how his team in Schibsted have set up a streaming data platform using cloud technologies, Kafka and Kafka streams. Schibsted is a leader in media and online classifieds in the Nordic market, delivering news to most of the population of Scandinavia. The streaming platform aims to simplify data management, providing a single point to manage data streams independently of data types. It includes a data quality solution, a data registration solution and a routing solution. Kafka is used as the backbone of the system and the routing solution is built on top of it. Kafka streams is used downstream to enrich streams. The data quality solution allows consumers and providers to register data quality checks and share metrics about the data that is being sent and received. The data registration solution allows providers to register the datasets owned by them, so consumers can find them. At the same time consumers can request access to the datasets and owners can grant access to those datasets, providing visibility and simplifying the sharing process. A key component of the streaming platform is the routing solution implemented using Kafka. This allows consumers of data to request data and adapt it to their own needs by applying filters and transformations, giving great flexibility.",Schibsted
Big Data 2019,How can Artificial Intelligence use Big Data for Translating Documents?,"In this session, John will walk us through the exciting world of machine translation. Specifically, he will show us how documents, known as corpora, filled with information from various sources can be used to provide artificial intelligence to a translation system. He will cover the evolution of statistical and neural-based translation systems and how they are currently used to achieve a superior translation quality compared to their predecessors. Big Data, Artificial Intelligence, and Machine Learning architectures will be the highlight of the presentation via a technique called transfer learning. This will be an awesome presentation for those wanting to see how it all fits together!",New York University
Big Data 2019,From Spark to Elasticsearch and Back - Learning Large Scale Models for Content Recommendation,"Serving tens of billions of personalized recommendations a day under a latency of 30 milliseconds is a challenge. In this talk I'll share our algorithmic architecture, including its Spark-based offline layer, and its Elasticsearch-based serving layer, that enable running complex models under difficult scale constrains and shorten the cycle between research and production.",Outbrain
Big Data 2019,Everything you Wanted to Know about Apache Kafka but you Were too Afraid to Ask!Â ,"Streaming platforms have emerged as a popular, new trend, but what exactly is a streaming platform? Part messaging system, part Hadoop made fast, part fast ETL and scalable data integration, with Apache Kafka at the core, streaming platforms offer an entirely new perspective on managing the flow of data. This talk will explain what a streaming platform such as Apache Kafka is and some of the use cases and design patterns around its use. Moreover, this talk will also present and answer a set of random — but recurring — questions from the community about Apache Kafka.",Confluent
Big Data 2019,Are You Sure about That?! Uncertainty Quantification in AI,"Streaming platforms have emerged as a popular, new trend, but what exactly is a streaming platform? Part messaging system, part Hadoop made fast, part fast ETL and scalable data integration, with Apache Kafka at the core, streaming platforms offer an entirely new perspective on managing the flow of data. This talk will explain what a streaming platform such as Apache Kafka is and some of the use cases and design patterns around its use. Moreover, this talk will also present and answer a set of random — but recurring — questions from the community about Apache Kafka.",inovex GmbH
Big Data 2019,Real Time Decision Making in Professionals Sports. From Description to Prediction and Prescription.,"Thanks to the amount of data that has been collected in recent decades in the world of football, we try to answer the right questions that have always existed around it, and from dimensions like never have been answered, such as for example How will a player perform next season in my team, What should I do to avoid conceding a goal? or What should I do so that players are not injured? For this, Artificial Intelligence is the discipline that best adapts to this scenario, being able to aproach the predictive and prescriptive dimensions, beyond the mere description offered by the data collection as currently occurs in the sector and allowing the development of several solutions in the main departments of professional football clubs and related entities.",Olocip
Big Data 2019,Big Data Information Architecture for AI,"IBM is famous for Watson, the artificial intelligence system which won the Jeopardy! general knowledge quiz show in 2011. Since then IBM have been feeding it big data and it is being used across industry to help humans work smarter. Here’s a presentation on some of the more interesting case studies from around the world such as Watson Willow at Woodside; the L’Oreal factory and Iplexia demo to show a factory line manager talking to Watson; Olli the self driving cognitive bus you can talk to; Ask Mercedes car manual, and the way we are putting Watson into IBM’s design management application Rational DOORS and IBM’s asset management application Maximo to improve the processes they support using AI.",IBM
Big Data 2019,Data Discovery with Amundsen,"Company data is increasingly widespread and it has always been difficult to understand who is using which tables or columns, how often and how the dataset was produced. There are commercial tools available that assist for some of these questions, but now there’s also an open-source tool called “Amundsen”, which is already in use by several larger companies. Amundsen helps you answer questions like how a dataset was produced, who else uses it, who’s the owner of a dataset and people using the tool can update the table or column descriptions. Amundsen isn’t around for a very long time, but data lineage and data discovery are already significant problems or at least challenges for companies; Amundsen is a strong answer to these problems, because it doesn’t just focus on the data itself, but also on the people and communities around the data that are using it.",Coolblue B.V.
Big Data 2019,The Creative Side of AI: Product Name Generation,"Product naming is not an easy task. It requires creativity and a goal-driven process. Can AI help? We propose here a solution to get a very large number in very short time of name candidates to inspire your marketing team from a many-to-many LSTM network. We will explore Recurrent Neural Networks in general and LSTM layers in particular and why they work so well for sequence generation. Kathrin will show how to prepare the data and build, train, and deploy a deep learning neural network in Keras and TensorFlow without writing a single line of code using the open source tool KNIME Analytics Platform.",KNIME
Big Data 2019,Computational Propaganda - How Algorithms Influence our Decisions,"“Manipulations in the era of algorithms”. Facebook having 2.2 Monthly Active Users, together with Google and Twitter create three interconnected digital republics where majority of human interactions take place and where increasing amounts of people spend great amount of time daily. Such constellation created fantastic environment for development and usage of targeted ad industry. The same algorithms however have recently gained momentum in being used by bad actors in creation of the so called computational propaganda – custom made messaging aimed at exploiting the micro-vulnerabilities of human mind in order to influence the public discourse and feed people’s decisions. The talk will discuss the casus of Cambridge Analytica, Brexit and the use of bots for influencing people’s decisions.”",Codewise
Big Data 2019,Deep Learning at Scale: Distributed Training and Hyperparameter Search for Image Recognition Problems,"Training complex image recognition model on a large dataset using one machine can be long and cumbersome. This talk focuses on methods and libraries, which allow us to train models on a dataset that does not fit into memory, or maybe even on the disk using multiple GPUs or even nodes. The ways of using multiple GPUs and nodes will be discussed and tradeoffs between different approaches will be compared. This talk includes a live demonstration of distributed training of image recognition model on large data set using Horovod and Petastorm on Databricks platform.",Databricks
Big Data 2019,Digital Business: Tomorrow is Already Here,"Digital business is about intelligently connecting people, things, and businesses. It’s an infinite world of new possibilities for companies to reimagine their business models, the way they work, and how they compete. New technologies like machine learning, the Internet of everything, blockchain, cloud, and the big data platform will transform value chains to enable completely new ways of doing business and our way of life. Hear how you can deliver a innovative customer experience at scale, with a fully-integrated front- and back-end operations based a solid digital core.",SAP
Big Data 2019,Turning a Wasting into a Learning Culture - Combining NLP and Neural Networks to truly Understand and Predict Customers' Behaviour,"Most of the customer feedback of companies around the globe is being wasted, as it is not used to learn, derive insights or to optimize products and processes. At the same time, the amount of customer survey and observation data within companies is growing at heavy speed. The presentation will introduce levers on how to cope with this phenomenon and illustrate, which role Data Science and Machine Learning should Play from an analytical and business perspective. Based on scaled and text feedback of customers, past-oriented data can be turned into predictive causal models that not only explain what truly drives customers loyalty, but also how customer behavior will change in the future based on optimization within the company and its customer service.",LINK Institute
Big Data 2019,Data Science at PMI - The Tools of The Trade,"Data Science is not a one man show. It is a team effort that requires every team member to master the tools of the trade. This is extremely important for effectively putting data science to work in a global organization. In this talk Michal would like to share with you the best practices to start, develop and ship data science products developed inside PMI the best practices and tools, currently in use by 40+ data scientists across four different location, where data science labs of PMI were established in 2017. He would like to share with you how the technologies (Docker, Artifactory, Jenkins) and methods (templates in Cookiecutter, CI/CD with GitFlow) well-known from software engineering are helping us in creating data science workflow that adapts to specific needs of every peculiar use case we need to deal with, provides transparency at all times, is reproducible not only at the data science but also data engineering and devops dimensions and allows at the same time frictionless development of data products and gives us the freedom to experiment.
Michal would also discuss the “transition challenges” and share some practical hints – for moving from pure exploration in Jupyter to building pip packages that will be put into production as well as for moving from data-to-code to code-to-data approaches in data science challenges. If you’re interested in how Python, Jupyter notebooks, Docker, AWS, Hadoop ecosystem, Spark, Artifactory, Jenkins, Atlassian suite, etc. are setup to support our collaborative work, devoted to building predictive models, this talk is for you.",Philip Morris International
Big Data 2019,Optimize your Data Pipeline without Rewriting it,"It is not fast enough! That is one of the more common responses to a data engineer when putting a data pipeline in production. It is easy to dig down into the code and try to optimize it. My experience as a data engineer shows me that it is often easier and more efficient, both in time spent and outcome, to focus on a more holistic view of the pipeline. In this talk, we will look at a structured process to optimize our batch pipelines. We will introduce steps that make our process data-driven instead of a gut feeling. With examples from real-world cases where delivery time was reduced in order by magnitude, we will look on actions where taken.
The intended audience is a beginner to intermediate data engineers. After the talk, you will have a better understanding of how to optimize your pipeline and be able to explain the steps taken for a stakeholder. You will know:
* what metrics to look at
* how to visualize the metrics
* how to detect bottlenecks and other time thieves from the metrics
* what actions to take.",Tink
Big Data 2019,Making Data Scientists Productive in Azure,"Doing data science today is far more difficult that it will be in the next 5-10 years. Sharing, collaborating on data science workflows in painful, pushing models into production is challenging. Let’s explore what Azure provides to ease Data Scientists’ pains. What tools and services can we choose based on a problem definition, skillset or infrastructure requirements?
In this talk, you learn more about Azure Databricks, MLFlow, Azure Machine Learning, Delta Lake, Data Science Virtual Machines and Cognitive Services, with all the perks and limitations.",Cognizant
Big Data 2019,Real-Time Data Streaming with Azure Stream Analytics,"It’s imperative in today’s world to be able to make split second decisions based on real-time data. Reports based on batch data are great for looking back at trends and potentially making long-term decision, but old data is in many cases already obsolete, and the opportunity to have an actionable impact on the success of a specific process may have been lost. What if we easily could set up a near real-time data pipeline, that could be used to provide complex analytics, and make intelligent actions based on the result? Allow me to introduce Azure Stream Analytics! In this talk, we will take a closer look at the Azure Stream Analytics ecosystem, and look at real world examples streaming twitter feeds as well as sensor data from Raspberry Pi’s, demonstrating how you can build your own burglar alarm.",Excella
Big Data 2019,Building a Scalable Data Science & Machine Learning cloud using Kubernetes,"Tagline: A real-life story of architecting & building a cloud-native data science platform using Kubernetes. A growing team of data scientists was looking for a simple, flexible, scalable and a secure way of migrating to the cloud as the on-prem data center started becoming a bottleneck. Kubernetes, which enables applications to run on a variety of private and public clouds, along with an ever-growing feature set, matched most of the team’s requirements. In this talk, Murali Paluru, who had the opportunity to work with the Data Scientists team, will share the gathered requirements, architecture and in-depth details of the Data Science platform built on top of Kubernetes. He will also demo a one-click solution that he has developed, to hide away the complexity of Kubernetes and enable the Data Scientists to focus on analyzing the data instead of managing the underlying infrastructure. Data Science is gaining much momentum and is being used by various organizations to get insights into the troves of data being accumulated. The attendees will learn how they can leverage Kubernetes within their teams. They can use the architecture shared in this presentation as-is or build on top of it and they can avoid the common mistakes and pitfalls that would be shared in this presentation.",Rancher Labs
Big Data 2019,Machine Learning Engineering,"When learning about machine learning methods, much effort is put into the fun part i.e. training and tweaking the models to improve upon your favorite performance metrics. But when the dust settles all these toys need to be working in your production environment and you want to have as little issues with them as possible. In this presentation, we will talk about things that are important if you want your ML system to be healthy and effective, like: testing, monitoring, delayed feedback loops, data quality, etc.", Adform
Big Data 2019,The Future of Traditional Shopping Driven by Customer Centric Approaches,"Artificial Intelligence gives retailers a great opportunity to evolve from point of sale to point of experience, convenience and omnichannel. In the coming years e.g., it will be easy to use bots for purchases perceived as boring and monotonous. As a consequence, offline retailers will have to be customer obsessed, keeping them engaged and loyal to their products if they don’t want to fall into an only-transactional buying category. Together with a major European retailer, we leveraged a 6-step Multi-Genre-Analytics approach with the objective of making the current range management customer centric. This required information to determine the wider impact of range changes, understanding e.g. the transferable demand of removed items and shopping missions. The desired outcome was not only to catch affinity effects but also to identify whether customers are loyal to a product or to a brand as removing a loyalty enabling product impacts negatively visits and sales. With the help of deep learning and video analytics, we aim now to be even more efficient optimising range and customer experience. The goal is to give the point of experience the equivalent capability of Web Analytics in the online world. How did this evolve as a business success story?",Teradata Corporation
Big Data 2019,A Game Theory Approach for Data Driven Business Decisions: Use Case in Portfolio Optimization,"Companies have a lot of useful information obtained from data that can be transformed into analytical models ready to use in business decision processes.
In this session, we will explore how this information, data and models, can be adapted and enriched in order to include in these company’s decision processes the existence of other players in the market, especially the competitors.
Each player in the market have their own strategies. Are we sure that when we are taking a decision, we are also taking into account all possible strategies of the competitors including retaliate actions? For example, if a company reduces prices, the first expected consequence can be the increase of market share, and then, the increase of the incomes. However, if competitors also reduce prices as a retaliate action the price reduction can be turned into an income reduction (same market share but lower prices). The consequences can be even worse if the initial price reduction starts a price war.
Is It possible to anticipate this outcome in order to avoid it? We propose to study the interaction between different competitors (players) in the market under the game theory scheme. Game theory can be used to answer very relevant business questions like “How can I optimize my portfolio?” or “Can my action start a price war?”. We have built a 3-step process:
First: We build a market simulator based on current customer behaviour, that allow us to simulate any possible portfolio obtaining gains and losses for each player for all possible scenarios.
Second: each player try to optimize the portfolio, modifying it. The portfolio for each player evolves following Monte Carlo simulations under some business conditions.
Third: the portfolio evolution finish when a given convergence criterion is reached, applying game theory equilibrium concepts.
In summary, this model provides Marketing Units with valuable information about possible evolutions of the company portfolio, or even the competitor’s one. Helps Marketing Units understanding the competitive dynamics of the market providing useful insights for the decision processes. So, business units can take initiative when needed, of planning retaliate actions if other players are going to take actions first.",Telefonica S.A.
Big Data 2019,Predicting Hotel Cancellations with Machine Learning,"Hotel cancellations can cause issues for many businesses in the industry. Not only do cancellations result in lost revenue, but this can also cause difficulty in coordinating bookings and adjusting revenue management practices. This session explores how machine learning techniques can be used to predict hotel cancellations. Firstly, data manipulation techniques with pandas are employed to effectively process over 20,000 customer entries. Feature selection tools such as the Extra Trees Classifier are then used to pinpoint the main drivers of hotel cancellations. The use of logistic regressions, support vector machines, and SARIMA are employed for prediction purposes, and extensive visualisations with pyplot are also generated to illustrate cancellation trends across different time periods.",MGCodesandStats
Big Data 2019,Stream Processing for Analysts with Flink and Nussknacker,"Analyzing and gaining insights from large amounts of data is one thing. Doing it in real time is a whole different business. There quite a few advanced stream processing engines, Apache Flink is one the most widely used. However, designing, testing and deploying streaming jobs usually demand development skills – it’s not so easy for analysts or business people. When Flink was introduced in one the largest Polish telcoms (which is used for real time marketing and fraud detection there), Maciek and his team decided to build open source project – Nussknacker – to make it easier for non-programmers to use all Flink power. Maciek will tell about it and show what they’ve built. The talk will be based on real-world examples – from simple filterings to aggregations and model serving.",TouK
Big Data 2019,Overview of Generative Adversarial Networks (GANs),"Until recently, generative modeling of any kind has had limited success. But now that Generative Adversarial Networks (GANs) have recently reached few tremendous milestones (and truly exponential growth in the interest in this technology), we are now closer to a general purpose framework for generating new data. Now GANs can achieve a variety of applications such as synthesizing full-HD synthetic faces, to semi-supervised learning as well as defending and mastering adversarial examples, we can discuss them in this talk. In this talk, we will start with the basics of generative models, but eventually, explore the state of the art in generating full HD images as presented in https://arxiv.org/abs/1710.10196 and dive into adversarial attacks and why this matters to all computer vision algorithms. GANs are a novel approach to generating new data or on a variety of adjacent problems that leverages the power of deep learning and two competing agents to achieve breath-taking results. Now GANs can achieve a variety of applications such as synthesizing full-HD synthetic faces, to semi-supervised learning as well as defending and mastering adversarial examples, we can discuss them in this talk.",Yepic
Big Data 2019,cGAN for Automatic Site Selection of Cultural Venues [P22404],"While analyzing structured data (even tremendous amounts of it) is a solved mystery nowadays, retrieving actionable insights from unstructured data (i.e. text) is the new challenge to be met. This talk even goes one step further and places this challenge in a streaming data setting. A reference architecture that works across industries will be shown to illustrate how to process text immediately after being written, how to analyze it, how to gather its meaning, and eventually visualize the results to provide actionable insights. This architecture will be composed of several open source projects. When combined, these tools are capable of accomplishing this ambitious task of analyzing streaming unstructured data. The talk will be completed by a live demo that showcases how real-life customer reviews can be processed in real-time to do sentiment analysis on unstructured data and display the results on a dashboard to provide actionable insights.",SHI GmbH
Big Data 2019,Big Data Serving: The Last Frontier. Processing and Inference at Scale in Real Time,"The big data world has mature technologies for offline analysis and learning from data, but have lacked options for making data-driven decisions in real time. This talk introduces vespa.ai a mature open source platform for storing, selecting processing and making model inferences over large data sets at end user request time.",VaHa
Big Data 2019,More Than a Query Language: SQL in the 21st Century,"“Great News–The Relational Model is Dead” was a prominent comment on the release of the new SQL standard in 1999. The message behind the provoking statement was that SQL has evolved beyond the relational model. As much as this move was discussed at that time, it took decades until database vendors caught up with this idiomatic change. Many developers haven’t heard of it until today. This talk provides the big picture on the evolution of SQL and introduces some selected modern SQL features by example. You will see that SQL has changed as much as our requirements have changed over the past decades.",winand.at
Big Data 2019,Embedded Analytics through Data API or What Big Companies are Hiding from You,"There are more than 20 years of experience of building amazing ETL and BI solutions and tools. These solutions proved being very useful and will serve us in the future.
However, such BI ecosystem was egoistically targeted for closed use cases with a focus on internal needs of companies using it, so only large companies allowed themselves offering great Analytical tools for their customers as self-service. Of course, there are many reasons for that – building such functionality takes enormous development effort, money and time. While the trend of making data more available for end users is there, big tech companies are still not giving access to main building blocks which would allow to easily build Data APIs and self-service embedded analytics to make life easier for many development teams. But the time has changed, and new evolving technologies are moving towards closing the gap and giving opportunities of creating infused Analytics to be used by wide range of companies and users. Let’s discuss coming new approaches, technics and components for better, faster and cheaper development of self-driven Analytics and Data oriented APIs.",Peekdata.io
Big Data 2019,Best Practices for Building AI Datasets,"Datasets are the most basic building blocks in AI systems, and the most innovative solutions often require manually collecting and labelling data. Yet most teams put their emphasis on magical models instead of solid, high quality datasets. In this talk, I will discuss many of the best practices for building AI datasets from scratch.",Electric Brain Software Corporation
Big Data 2019,Runtime Modifications of Distributed Big Data AI Models and Its Parameters,"There has been phenomenal growth of interest in distributed data analysis with large software and other commercial entities. They are trying to construct the solutions that facilitate flexible, scalable and heterogeneous IT-infrastructures dealing with Big Data analysis. Distributed data processing platforms such as Hadoop or Apache Spark became a de-facto standard in the world of Big Data processing, and in cloud economy. The processing pipelines consisting of AI models for such platforms are composed during design time and then submitted to the central (master) component who then distributes the code among several worker nodes. However, in many situations, the application is not static: the users want to add new processing steps, data scientists adjust parameters of their algorithm, testers find new bugs, requirements from user could change, etc. We propose an approach based on AI Planning to make AI models adaptive within distributed processing pipelines by updating them on-the-fly (runtime) in response to changes coming from AI experts, users, the environment, etc.",TNO
Big Data 2019,The Journey Through Real Estate Price Model Development,"Together with his team, Vygantas has developed a model that predicts the selling price of homes in Denmark. The development was an exciting journey with many challenges and different attempts to overcome them. Sometimes the build process is more interesting than the final model itself. Thus, Vygantas is very happy to offer you a walkthrough of this experience with all the pains and joys encountered along the way.",Danske Bank
Big Data 2019,Better Data Exploration and Visualisation with Elasticsearch and Kibana,"Data science has incredibly sophisticated analysis tools which are very effective, but rarely very convenient for their operators. In this live demo we will focus primarily on the prototyping and exploration phase of a data scientist’s workflow. We’ll push a slice of a bigger dataset held in pandas into Elasticsearch. We’ll then use Kibana to build on the fly visualisations and dashboards, allowing us to explore and discover the data’s features in a more intuitive, faster way than writing commands in a jupyter notebook. As Elasticsearch itself is a highly scalable datastore, we’ll discuss how you could take its many abilities beyond prototyping into production if it makes sense for a particular dataset. This demo only includes open source tools or tools free for all use, including commercial.",Elastic
Big Data 2019,Breakthroughs and the Future of (Deep) Reinforcement Learning,"Due to recent successes in winning computer games against humans, Reinforcement Learning (RL)
received a lot of attention in the media.
This presentation will elaborate the foundation of RL and demonstrate how it is implemented.
Also, it will demonstrate how we can track and understand a system’s learning progress.
Most recent trends of RL research and applications in logistics and pharmacy will be discussed.",Dr. B'hlmeier Consulting
Big Data 2020,Redis: a Multi-Model DB for IoT and Beyond,"An overview of Redis, an open-source multi-model NoSQL DB as a foundation for Big Data projects in the Internet of Things ecosystem and beyond. After a short introduction to Redis, its multi-model capabilities are discussed which make this popular open-source DB a top choice for unstructured data typically found in IoT environments, ranging from simple JSON data to complex graph and timeseries requirements. A few real-world use cases round off this presentation.",Redis Labs
Big Data 2020,Graph Processing for Open Metadata and Governance,Learn how ODPi Egeria uses its distributed virtual graph to connect metadata about an enterprise’s data and IT services from many different tools and then apply governance across this landscape. In this talk we will describe the principles behind the distributed virtual graph and how different technologies can connect in. We will also cover how the JanusGraph technology can be used to fill in the gaps between the tools to ensure the metadata is linked together.,ODPi Egeria & IBM
Big Data 2020,Data Governance From an Engineering Perspective,"For a long time, data governance seemed to me like an empty phrase meaning everything and nothing at the same time. Valdas was on the fence, not sure how to approach it from an engineering perspective.",IT Architect Specializing in Data Analytics and Cloud Computing
Big Data 2020,Introduction to FLaNK Stack,"Introducing the FLaNK stack which combines Apache Flink, Apache NiFi, Apache Kafka and Apache Kudu to build fast applications for IoT, AI, rapid ingest. FLaNK provides a quick set of tools to build applications at any scale for any streaming and IoT use cases.",Cloudera
Big Data 2020,Best Practices for Building Streaming Data Architectures,"The era of big data is over since everybody is a big data company these days. While this sounds super exciting in theory most companies seem to struggle in transforming this characteristic into something that really provides value. To address this problem a new architectural style have been used and it is called streaming data architecture. This architecture style allows companies to harness the power of data by ingesting, storing, and processing data as they happen and at scale to fuel modern analytics based applications.",Elastic
Big Data 2020,Kafka as a Platform: the Ecosystem from the Ground Up,"Kafka has become a key data infrastructure technology, and we all have at least a vague sense that it is a messaging system, but what else is it? How can an overgrown message bus be getting this much buzz? Well, because Kafka is merely the center of a rich streaming data platform that invites detailed exploration. In this talk, we’ll look at the entire streaming platform provided by Apache Kafka and the Confluent community components. Starting with a lonely key-value pair, we’ll build up topics, partitioning, replication, and low-level Producer and Consumer APIs. We’ll group consumers into elastically scalable, fault-tolerant application clusters, then layer on more sophisticated stream processing APIs like Kafka Streams and ksqlDB. We’ll help teams collaborate around data formats with schema management. We’ll integrate with legacy systems without writing custom code. By the time we’re done, the open-source project we thought was Big Data’s answer to message queues will have become an enterprise-grade streaming platform, all in 45 minutes.",Confluent
Big Data 2020,In the Shallow with AI,"As AI technologies continue to integrate and transform our society and organisations, questions around how these augment with human intelligence continues to be of much interest. Many have recognised the need for ‘humans-in-the-loop’ in AI, but most of these efforts have focused on eliciting human interventions at various points in the machine learning process rather than leveraging human ingenuity.In this session we’ll briefly look at:
* The role of objective and subjective data, and how these influence data-driven insights;
* ‘Trans-contextual’ information and the implications for AI; and
* How human ingenuity and adaptability can play an important role in mitigating the unintended and unwanted consequences of AI.",Phoensight
Big Data 2020,5 Pillars of User-Centric Analytics,"How one team created a user-centric analytics program that revolutionized data consumption across the company.
Some consider analytics the dark art of digital transformation. The truth is that, done right, analytics can illuminate the darkest corners of business.Analytics team have seen the power of analytics in action, one that brings the process of creating a truly user-centric analytics program into the light. Today, the team runs an analytics program that aligns with critical areas of decision-making within the company and empowers users with powerful decision-making capabilities every single day.
Join the session to learn more and how to apply the successful analytics strategy to your own organization(s).",ServiceNow
Big Data 2020,"Data Versioning, What Does it Mean?","The demand for better versioning of data is growing. There are a plethora of open source projects providing “data versioning”, “Git for data” and “manage data like code” capabilities (e.g Hudi, DoltHub,, Delta Lake, DVC, Pachyderm, and lakeFS). So how do you know you are choosing the right one? In this talk we will go over the difference between these solutions by clustering them according to 4 main use cases:
1. Collaboration over data: enabling teams to collaborate over data over time, while contributing to the data evolution.
2. Managing ML pipelines: allowing pipeline management of ML projects, from model creation to production.
3. The need for mutability: data formats that grant Insert, Update and delete over an immutable object storage.
4. The need for ACID guarantees over an object storage data lake: using branching logic to manage an object storage based data lake. By the end of the talk, you should have a good understanding of how these solutions compare and which you should choose for different types of use cases. ",Code4Thought
Big Data 2020,"Predicting Cryptocurrency Exchange Rates with Stream Processing, Social Data and Online Learning",Designing a system to cope with loads of billions of events is harder than it seems. In this talk the presenter will go through the most common use cases and pitfalls and provide tips and good practices about how to design systems to avoid them.,Schibsted
Big Data 2020,Azure Synapse Analytics Overview,"Azure Synapse Analytics is Azure SQL Data Warehouse evolved: a limitless analytics service that brings together enterprise data warehousing and Big Data analytics into a single service. It gives you the freedom to query data on your terms, using either serverless on-demand or provisioned resources, at scale. Azure Synapse brings these two worlds together with a unified experience to ingest, prepare, manage, and serve data for immediate business intelligence and machine learning needs. In this presentation, James will talk about the new products and features that make up Azure Synapse Analytics and how it fits in a modern data warehouse, as well as provide demonstrations.",Microsoft
Big Data 2020,Interactive BI Analytics with Presto,"Presto, an open source distributed SQL engine, is widely recognized for its low-latency queries, high concurrency, and native ability to query multiple data sources. Proven at scale in a variety of use cases at Airbnb, Facebook, LinkedIn, Lyft, Netflix, Twitter, and Uber, in the last few years Presto experienced an unprecedented growth in popularity in both on-premises and cloud deployments over Object Stores, HDFS, NoSQL and RDBMS data stores. From the start, Presto was targeting interactive BI analytics involving heterogeneous data sources. Recent improvements like aggregation pushdown, dynamic filtering, data masking and supporting new data sources including Druid, Delta Lake and Elasticsearch, pushed that use case to the next level. During the talk Lukasz and Karol will give a quick introduction to Presto in general, as well as some of its advanced features. They will show a demo, how it works in practice. In addition, they will demonstrate how to integrate different data sources in a data protected environment",Starburstdata
Big Data 2020,Designing and Building Data Science Solutions,The ability to apply data science and AI to business challenges can be impactful and exciting. Yet getting started can be daunting: how best to plan a project in order to ensure success can be unclear. Jon and Neri will share their learnings about sensible approaches to designing data science projects and present a framework that they have found to be useful in giving projects the best chance for success. This talk will largely mirror the material that appears in their book by the same title at https://datasciencedesign.com/.,Spot Intelligence
Big Data 2020,Adding AI Cloud Services to Your On-Prem Data Workflows for NLP & Content Enrichment,"Getting data out of data sources and into your favourite search engine or storage system is often one of the first challenges you face in any data project. There are some ETL tools out there that help you face that issue and consequently there are a lot of data pipelines that are run on-premise: Either based on open source software like Apache NiFi or commercial products. These tools usually offer a variety of transformations but are very limited when it comes to working with unstructured data and enriching that data. However, establishing a process for analysing and enriching unstructured data can be extremely tedious: You need the right persons (e.g. machine learning engineers, data scientists, natural language processing experts), a lot of annotated training and test data, computational power for training machine learning models etc. Cloud services to the rescue! In this presentation Daniel will show how on-premise data processing or indexing pipelines can be extended by cloud services to get more out of your unstructured data while bypassing all the above-mentioned challenges saving time and money.",SHI GmbH
Big Data 2020,From the Earth to the Moon: Lessons from the Space Race to Apply in Machine Learning Projects,"“The space race was a EEUU – Soviet Union competition to conquer the space. This competence helped to develop space technology in an incredible manner, developing other derivative technologies as a side effect.
This race was full of success in both sides, achieving goals that seemed impossible in record time.
From this space race we can learn some lessons that we can apply to our Machine Learning projects to have a bigger success rate in a limited amount of time.”",RavenPack
Big Data 2020,Analyzing Public Data About User Queries on Search Engine to Predict Trends and Evaluate Markets,"Keywords and queries from search engine public data continue to be a beautiful window into people's interests. If we want to use words as an interest analysis tool, however, we need to analyze large volumes of data and manual analysis can be expensive and inaccurate. We Will go through this topic starting from how retrieve data and how to analyze these data with some NLP libraries. ",ByTek
Big Data 2020,"The GDPR Challenges to Big Data, and How to Overcome Them","The GDPR poses real limits on how personal data can be used, but it does not mean the end of data markets, data analysis and data exchange, but the methods to fuel this will radically change. During this talk, Silvan Jongerius, expert in GDPR compliance for technology will explain the obvious and not so obvious challenges of GDPR in big data, and look at different approaches to overcome them. Privacy innovation can help big data businesses to reach their goals without breaking any rules.",TechGDPR
Big Data 2020,Real-Time Streaming with Python ML Inference,"The capabilities of machine learning are now pretty well understood and there are great tools to do data science and construct models that answer nontrivial questions about your data. These tools are often used from Python. The key new challenge is making the trained prediction model usable in real time, while the user is interacting with your software. Getting answers from an ML model takes a lot of CPU/GPU and must be done at serious scale. The ML tools are optimized mainly for batch-processing a lot of data at once, and often the implementations aren’t parallelized. In this talk Marko will show one approach which allows you to write a low-latency, auto-parallelized and distributed stream processing pipeline in Java that seamlessly integrates with a data scientist’s work taken in almost unchanged form from their Python development environment. The talk includes a live demo using the command line and going through some Python and Java code snippets.",Hazelcast
Big Data 2020,ML in Production - Serverless and Painless,"Productionising machine learning pipelines can be a daunting and difficult task for Data Scientists. Fortunately, many novel tools and technologies have become available in the past years to address this issue and make it easier than ever to deploy ML models into production, without the need to configure servers. In this session, Oliver will walk through some of the best serverless options on how to operationalize ML pipelines within the Tensorflow ecosystem and on Google Cloud Platform, based on actual case studies. One of these real-life case studies will dive into the journey of a global cosmetics brand to become packaging-free with the help of ML. The first step towards this goal allows customers to view product information simply by taking a picture. This completely eliminates the need for packaging and labels in stores. However, in order to do this effectively, an accurate image classification model, accessible on mobile phones, is needed. This session will cover the details of the end-to-end machine learning pipeline that was created to deliver and update performance ML models to mobile users.",Datatonic
Big Data 2020,Advanced Analytics in the Industry,"What is the scope in the data use industry? Different views on it are presented in Industry 4.0 Congress; from the description of the power of an artificial intelligence project presented by Gradiant, to real-time decision-making with the experience of Ibermática and its vision of improving OEE based on data. There will be presented also two selected cases from diverse sectors such as the automotive industry (DRAXTON-NAITEC case) or drug design (Phenobyte) that will highlight the importance of having powerful data engineering and enrich the listener’s experience.",GRADIANT
Big Data 2020,Ethical Side of Booming AI,"Nowaday everybody talks about AI and ML as a future for humany. But the thing is that AI, even in it current form will affect human being much deeper then we think. There is a huge ethical gap between what technology can deliver and what we really expect it do deliver. We have to turn our moral compass on. Let’s talk about human-faced side of AI.",Human Technologies
Big Data 2020,Supercharge your Data Analytics with BigQuery ML,"One of the hottest topics in database land these days is BigQuery ML. A new way to use machine learning on top of tabular data straight on your tables without leaving the query editor. With BigQuery ML, you can build machine learning models without leaving the database environment and training it on massive datasets.
In this demo session, we are going to demonstrate common marketing Machine Learning use cases how to build, train, eval and predict, your own scalable machine learning models using SQL language.
The audience will get first hand experience how to write CREATE MODEL sql syntax to build machine learning models such as:
– Multiclass logistic regression for classification
– K-means clustering
– Matrix factorization
– ARIMA time series predictions
– Import TensorFlow models for prediction in BigQuery Models are trained and accessed in BigQuery using SQL — a language data analysts know. This enables business decision making through predictive analytics across the organization without leaving the query editor.",REEA
Big Data 2020,"Real-Time Stream Processing for Insurance & Health Care With Kafka, Kafka Streams and Multi-Runtime Microservices","Insurance companies traditionally heavily used batch processing to manage their data. However, nowadays we expect to have the data ready in near real time. Moreover systems increased in complexity to satisfy new challenging requirements like usage of AI and ML components. As a service provider for insurance companies, pension & healthcare funds we rolled out a resilient stream processing platform running in kubernetes that we can scale out horizontally to integrate different microservices developed in different languages like java, scala or python. To communicate between microservices kafka is used, since it’s scalable and resilient. All microservices are usually stateless and are governed by scala based orchestrators developed with kafka-streams. For python microservices we adopted pykafka, while for scala microservices we heavily uses akka and akka-streams to be able to scale vertically inside a single microservice instance. The state is usually passed along each message down the data processing pipeline, but in case of big payloads (like documents to be classified by ML components) a redis cluster is used for in memory object storage or a mongo cluster if persisted storage is required. In house connectors based on akka are used to operate with traditional relational databases.",PREVINET
Big Data 2020,Introduction to Data Streaming,"While “software is eating the world”, those who are able to best manage the huge mass of data will emerge out on the top. The batch processing model has been faithfully serving us for decades. However, it might have reached the end of its usefulness for all but some very specific use-cases. As the pace of businesses increases, most of the time, decision makers prefer slightly wrong data sooner, than 100% accurate data later. Stream processing – or data streaming – exactly matches this usage: instead of managing the entire bulk of data, manage pieces of them as soon as they become available. In this talk, Nicolas will define the context in which the old batch processing model was born, the reasons that are behind the new stream processing one, how they compare, what are their pros and cons, and a list of existing technologies implementing the latter with their most prominent characteristics. He’ll conclude by describing in detail one possible use-case of data streaming that is not possible with batches: display in (near) real-time all trains in Switzerland and their position on a map. He’ll go through the all the requirements and the design. Finally, using an OpenData endpoint and the Hazelcast platform, he’ll try to impress attendees with a working demo implementation of it.",Hazelcast
Big Data 2020,Making Data Downtime a Pillar of Your Data Strategy,"Barr will introduce the concept of “data downtime” — periods of time when data is partial, erroneous, missing or otherwise inaccurate. Data downtime is highly costly for organizations, yet is often addressed ad hoc. She’ll discuss why data downtime matters to the data industry and tactics best-in-class organizations use to address it — including org structure, culture, and technology.",Monte Carlo
Big Data 2020,Trust and Quality in the Era of Software 2.0,"There is a widespread excitement about the potential of Machine Learning (ML) models, but with market pressures, haste to ship and deliver them; those models fail to deliver what they promise or even worse they can have a negative impact for businesses and individuals. It is our thesis that just as we define quality properties for governing a typical software system from the way it is implemented (e.g. maintainability) to the way it behaves (e.g. functional suitability), we need to do a similar thing for ML models. That is why apart from Accuracy, the so-called F.A.T. properties, (Fairness, Accountability, Transparency) should be essential quality properties for a ML model (or AI system) contributing towards its adoption and trusted governance. In his talk Yiannis Kanellopoulos will present an approach on how an ML model can be evaluated in terms of its Fairness, Accountability and Transparency. Using examples of case studies (from industrial and publicly available datasets) Yiannis will share insights and the benefits one can get by making a ML model accountable, transparent and trying to mitigate its biases.",Code4Thought
Big Data 2020,Towards Enterprise-Grade Data Discovery at ING with Apache Atlas and Amundsen,"From discussing what is AI to practical case studies of AI, Kane will discuss how companies in Hong Kong and world wide uses AI to create business values. Thereafter he will share his experience in working with large multi-international companies. He will discuss potential pitfalls and experiences on what to do and avoid. Finally, Kane will discuss practical tips on how to formulate AI road maps for companies to drive business value.",ThinkCol
Big Data 2020,Machine Learning Engineering,"A successful implementation of the data discovery solution is the key enabler for the true democratisation of data in an enterprise context. It is also a prerequisite for transforming an old-school big data platforms that were primarily designed for ETL processes into robust analytics environment that your data scientist would love.
In the presentation Verdan will share our experience from designing and implementing a data discovery product powered by open-source technologies such as Apache Atlas and Amundsen (initially created by Lyft, and then moved to Linux Foundation). He will discuss the overall architecture, integration with Big Data ecosystem components as well as what kinds of metadata we use to make our solution a powerful and trustworthy solution that facilitates data scientist and analysts work at ING.",ING Bank
Big Data 2020,The Importance of Good Data Quality and Understanding of Visitor Behavior,"In the session, Mats goes through how to measure a person’s journey from being a person in an interesting segment to becoming a customer. He also tells you how to get high data quality on your web visitors so that you can use it in machine learning in Facebook and google. He also addresses what knowledge can be good tools for understanding customer behaviors, such as behavior economics and reverse engineering.",paf
Big Data 2020,Exoplanet Detection Using Machine Learning,"Speaker will introduce a new machine learning based technique to detect exoplanets using the transit method. Machine learning and deep learning techniques have proven to be broadly applicable in various scientific research areas. He aims to exploit some of these methods to improve the conventional algorithm based approach used in astrophysics today to detect exoplanets. Abishek used popular time-series analysis library ‘TSFresh’ to extract features from lightcurves. For each lightcurve, we extracted 789 features. These features capture information about the characteristics of a lightcurve. He used these features later to train a tree-based classifier using a popular machine learning tool ‘lightgbm’. This was tested on simulated data which proved it to be more effective than conventional box least squares fitting (BLS). It produced comparable results to the existing state-of-art models while being much more computationally efficient and without needing more processed versions of lightcurves such as folded and secondary views. On Kepler data, the method is able to predict a planet with an AUC of 0.948 which means that, 94.8\% of the time a planet signal is ranked higher than a non-planet signal and recall of 0.96 meaning, 96\% of real planets are classified as planets. With Nasa’s Transiting Exoplanet Survey Satellite (TESS), a reliable classification system is much needed as we are receiving over a million lightcurves per month. However, classification of this data is harder as lightcurves are shorter. His method is able to classify lightcurves with an accuracy of 98\% and its able to identify planets with a recall of 0.82.",Hawk:AI
Big Data 2020,"Streaming Processing - an Overview of the Concepts, Architecture and Technology of Doing Data Science on Real-Time Data","Streaming Processing (or Fast Data processing) is becoming an increasingly popular subject in financial services, marketing, the internet of things, and healthcare. A typical stream processing solution follows a ‘pipes and filters’ pattern that consists of three main steps: detecting patterns on raw event data (Complex Event Processing), evaluating the outcomes with the aid of business rules and machine learning algorithms, and deciding on the next action. At the core of this architecture is the execution of predictive models that operate on enormous amounts of never-ending data streams. In this talk, Bas will present an architecture for streaming analytics solutions that covers many use cases that follow this pattern: actionable insights, fraud detection, log parsing, traffic analysis, factory data, the IoT, and others. He’ll go through a few architecture challenges that will arise when dealing with streaming data, such as latency issues, event time vs server time, and exactly-once processing. Finally, he’ll discuss some technology options as possible implementations of the architecture.",Aizonic
Big Data 2020,Covid-19: Big Data Analytics and Artificial Intelligence,"With the COVID-19 pandemic spreading across the globe, it’s becoming clear that few can avoid its reach, posing severe challenges to health services and having a range of related social and economic impacts. As governments try to protect lives whilst maintaining economic viability, they are seeking innovative and dynamic solutions to make the right decisions at the right time, particularly in developing countries, which are expected to be disproportionately impacted. This is where insights from mobile data can come into play. This session will focus on the response to COVID-19 data analytics.",Intellisystem Technologies
Big Data 2020,The Intuition Behind the Use of M.L. in Marketing Analytics,"Since 2013, important breakthroughs and advances in technology have made possible to run sophisticated predictive models capable of classifying images, text, sound and are now pervasive in many applications such as self-driving cars, chat bots, translation, among other fields. “Since 2013, important breakthroughs and advances in technology have made possible to run sophisticated predictive models capable of classifying images, text, sound and are now pervasive in many applications such as self-driving cars, chat bots, translation, among other fields. 
Marketing has not been an exception to use these new technologies collectively known as Machine Learning (ML); a sub-field of Artificial Intelligence (AI). This talk presents the key insights that make AI/ML useful for marketing and demystifies the core technology and illustrates case studies where my team applied the technology.
We use AI in creative ways to:- Improve the signal on A|B experiments and have better reads and insights- Advanced segmentation of customers by propensity to act, churn, open an email- Cross sell predictions- Models of resurrection and reactivation- Natural Language to provide insights on content- Loyalty programs. In this talk, Mario will discuss how predictive models are used across these areas
– How to think and interpret predictive models- What metrics we use to evaluate these models- The tools and technologies we use- Specific case studies in optimization, channel attribution",Credit Sesame
Big Data 2020,Stopping Public Transport Coronavirus Infections with Big Data,"The Coronavirus reduced the amount of public transport passengers to a minimum. Passengers fear to be infected by overfilled vehicles. We discuss how Big Data, analytics and forecasting help to limit infections. We give insights of our journey from idea development to the actual situation and share our learnings.",iunera GmbH & Co. KG
Big Data 2020,Orchestrating Data Workflows Using a Fully Serverless Architecture,"Fundbox is a growing fintech company that provides an automatic underwriting platform based on data and AI. While scheduling a limited number of data workflows is a generally manageable task, scaling to hundreds of data workflows with dependencies and diverse job types, requires a substantial customized engineering, complexity, and overall expensive resources. Serverless-based architectures offer an alternative to traditional resource management. Tomer Levi explains how the data engineering team at Fundbox uses AWS StepFunctions, Docker containers, and Spark to build a live serverless data orchestration platform, focusing on their decision to build a user-freindly yet powerful and scalable solution. Tomer will further describe AWS StepFunctions state machines, their limitations, and how to overcome them by building custom job scheduling and dependency features. Finally, the talk will illustrate how resource bottlenecks were overcome using Docker containers and AWS Fargate. Fundbox’s architecture is scalable and already serves dozens of engineers, BI developers and data scientists in the company.",Fundbox
Big Data 2020,Big Data Architecture in the Advertising Industry,"In this session, you will learn how Hybrid Theory’s team of data engineers and data scientists are ingesting billions of events per week to create multiple products using Kafka, Spark and ML. All these multiple components are part of the big data platform supporting the end-to-end process of data exploration, version control of ML models and orchestration of data pipelines via Apache Airflow.",Hybrid Theory
Big Data 2020,From Internet Access Devices Usage to Behavioural Model,"The way of using mouse, keyboard, tablet or phone can be a great source of the information about user behaviour. Data can be gathered from a spectrum of sensors built into these devices. Then this data can be transferred into features which characterize user, like manner of typing, speed of cursor or mouse movement curve. This presentation will focus on how to use those features to build behavioural model which will be unique and adjusted to each of thousends of customers. This problem has some important additional aspects. For one user several different models can be built, therefore some aggregation methods has to be delivered. Some of data has to be anonymized. Continuous authentication is excepted by business. These factors define important limitations for models productionization. They will also be discussed.",Digital Fingerprints
Big Data 2020,Product Management Principles to Drive Businesses in the AI Era,"A significant challenge for companies trying to adopt AI is the gap between the business domain and AI solution domain. On the one hand, the corporate strategists strive for sustainable profitability, but on the other hand, the AI technologists grind over problems like image recognition, prediction or recommendation engines. These are entirely different worlds. How come for some companies, the gap keeps widening, but for other companies, like Amazon and Google, the business excels behind high barriers for market entry? In this presentation, you’ll learn the answers. We will look into how AI product managers offer unprecedented value to navigate their organizations through AI solution adoption and development lifecycle with ease. Adnan will share lessons from decades of work at large corporations, and best practices acquired during consulting for companies in Silicon Valley.",AI Product Institute Silicon Valley
Digital Work 2020,Keynote Address: The CX-EX Connection (*All Times EDT),"In this keynote address, MIT research scientist Kristine Dery shares the findings from her research on the dynamic between technology and the way that people work, including the design and management of the workplace to understand how organizations use digital capabilities internally to create more effective ways of working, and the impact of new ways of engaging with talent in the digital era.",MIT Sloan School of Management
Digital Work 2020,Case Study: Building a Future Ready Workforce with VMware (*All Times EDT),"As organizations are moving from supporting remote work as an afterthought to becoming a true anywhere organization, employee experience is paramount to make that shift. Learn how Dell Technologies was able to adopt a digital workspace platform to deliver the right experiences to their distributed workforce to make employees successful across their lifecycle at an organization.",Dell Technologies Vmware
Digital Work 2020,Effectively Embedding User Researchers and Designers on Agile Teams (*All Times EDT),"The basic tenets of Agile are clear...that is, until teams attempt to seamlessly integrate design practices, processes, & approaches. Philosophical & logistical conflicts (e.g., the role that designers play, the timing of research & design activities) crop up almost routinely. I'll describe how user researchers, user experience designers, and visual designers can effectively embed on Agile teams and collaborate with developers to deliver optimal user experiences. I lay out an ideal project flow from building empathy for our users and their needs through delivery & refinement of solutions that optimally fulfill those needs. The session will include discussion of lessons learned as well as common challenges and how we can address them. 1. Understand the role that user research plays in defining and prioritizing a team's work.
2. Overcome key challenges that many teams face when attempting to integrate design and Agile.
3. Enable effective, successful team collaboration and deliver outcomes with strong user experiences.",IBM
Digital Work 2020,Harnessing AI in the Next-Gen Digital Workplace (*All Times EDT),"There is still a huge amount of confusion, hype and expectation surrounding Artificial Intelligence (AI) and Machine Learning (ML). Author and Consultant Katie King will share the evidence and insights drawn from her deeply researched book and her role on the UK All-Party Parliamentary Group (APPG) taskforce looking into adoption of AI in business. During this pragmatic, enlightening session, Katie will share real-world examples of how AI is impacting HR, focusing on culture, internal processes and operations, and how AI alters the nature of work, and how best to structure an organisation accordingly. With case studies drawn from multiple industry sectors and geographies, Katie will reveal how AI and wider collaboration tools can be deployed to improve employee productivity. She will also provide invaluable insights on both successes and failures, as well as practical implementation strategies. Katie will shed light on some of the major issues including the future of jobs, privacy, ethics, the skills gap and more. Gain actionable knowledge and insights on how AI is impacting HR, with techniques and tools to assist with structure, culture, team building and much more
Access Katie King's Scorecard for Success - a pragmatic, workable plan to consider the merits of AI and its real world applicability in a next-gen digital workplace
Become a disruptor and run a future-proofed productive digital workplace",AI in Business
Digital Work 2020,Moving to Microservices: Building an Integrated Digital Experience (*All Times EDT),"Follow the journey of Wells Fargo's move from platform dependency to microservices efficiencies from Digital Workplace VP, Christy Punch. Gain an inside look at the model built by this financial services giant to integrate multiple systems and applications into a seamless digital experience.",Wells Fargo
Digital Work 2020,Leaning Forward on the Employee Journey at Facebook (*All Times EDT),"Facebook recently committed to be the most forward-leaning company on remote work and to do this in a way that is measured, thoughtful, and responsible. Part of this shift will include redesigning and remodeling employee journeys, analytics and practices to cater to an increasingly remote workforce. Join Janis Avila, Director Employee Experience at Facebook and Sameer Chowdhri, Global Head HR Solutions at Workplace from Facebook as they deep dive into the thinking behind building and optimizing for employee experience in our new working normal.",Facebook
Digital Work 2020,Selecting the Right Tech for Your Intranet: A Practical Approach To Getting It Right (*All Times EDT),"Few tools impact every aspect of the employee experience like an intranet. When done right, it can transform an organization's culture and work style. When done wrong, it could take years -- and lots of resources -- to recover from such a high-visibility mishap. Session participants will hear from Ideal State, Cystic Fibrosis Foundation’s (CFF) former intranet project manager, and intranet software provider Beezy about the carefully crafted process CFF used to select and implement the right intranet technology for its 700+ employees. How to select the right intranet technology and provider?
Best practices for creating a great employee experience with your intranet
Get tips on how to manage cultural shifts when implementing a new intranet",Ideal State
Digital Work 2020,What's Hot: Trends in Productivity and Experience in the Intelligent Digital Workplace (*All Times EDT),"Today's workplace, even with the mid-pandemic impact we have been through, is still moving at an exceptional pace, and employees are often left to fend for themselves.  Today's digital workplace experience leaders need to keep ahead of the market to ensure their organizations can not only survive but also prepare for the future.  This session will cover important market trends that were identified in an SAP-sponsored market research project with IDC into the future of work and the intelligent digital workplace. What causes negative impacts on work productivity and how to overcome them
Market trends and drivers impacting work transformation initiatives
Key business challenges in work transformation",SAP
Digital Work 2020,[Roundtable Discussion] Developing a Future Ready Workforce (*All Times EDT),"Join an exclusive group of your peers for an open and interactive conversation around where you are in the learning curve with regard to COVID-19 and remote work, what has been the most challenging part of IT during this crisis, the relationship between HR and IT and what do you see as the biggest workplace challenges for 2021.",SMG / CMSWire
Digital Work 2020,Moving Faster:  Ramping Up Internal Communications for Speed and Growth at Hootsuite (*All Times EDT),"Hootsuite has enjoyed rapid growth since 2008, expanding across 15 offices in 11 countries.  This speed of growth led to new challenges around information sharing and retrieval, and internal communications struggled to keep pace. To succeed Hootsuite needed to centralize information sharing and announcements across its entire team. The organization also wanted to foster a culture of openness and accountability around accessing that information, making it easier and sustainable for every employee to find what they needed as the company grew. Grace Carter, Director of Internal Communications at Hootsuite, will share how the team overcame these obstacles to grow a connected and informed global team.",Hootsuite
Digital Work 2020,Always Day One: How the Tech Titans Plan to Stay on Top Forever (*All Times EDT),"Instead of slowing down and ossifying like many big companies, Amazon, Google, Facebook, and Microsoft are only getting stronger as they get larger. Their secret? An enthusiastic embrace of workplace technology that's allowed them to remain nimble and inventive despite their size and age. The tech giants use internal technology to minimize the time their employees spend supporting existing products - they give their employees the chance to dream up their next new business. They then put new ideas through well-honed processes meant to bring the best to life. By looking deep into the tech giants, as speaker Alex Kantrowitz did in his newly released book, Always Day One, we see the big picture strategy driving the world's most advanced digital workplaces. Think more strategically about the digital workplace. This talk will show the broader strategy that makes installing workplace technology so important in today's economy.
Learn about the technology keeping the tech giants on top. AI, automation, and collaboration technology are core to the tech giants' success. Get the inside scoop on what they use and how they use it.
Discover the secrets behind the tech giants inventiveness. The pathways these companies build to bring ideas to life are just as important as the technology they use to help generate them. Find out how some of their most important products came to life.",Always Day One
Digital Work 2020,An Insider Guide to Building Stellar Learning & Development (*All Times EDT),"It's not enough to simply have an L&D program in place. The best learning and development programs are invisible, adjusting seamlessly to the evolving wants of the learner, while meeting business needs in real time. Learning happens in an integrated fashion, as part of the way work gets done. With the growing skills gap at work, many organizations have turned to high profile upskilling and reskilling initiatives to close it. Done right, they benefit both organizations and employees. Companies develop a more productive, more skilled and adaptable workforce. Employees get the skills they need to succeed in new and higher level roles. It’s win-win. The theory and practice of learning in the flow of work
How adaptive learning is a game changer for skill development
What needs to happen to make reskilling and upskilling a reality",GE Healthcare
Digital Work 2020,From Nesting to Neighborhoods: Minimize Spatial Scarcity and Improve Collaboration (*All Times EDT),"Growing teams, dwindling workspace -- it gets more difficult each day to thrive and remain inspired in our surroundings. UXers' space needs are significant and nuanced; constantly fluid, with spatial landmarks that draw people together. How do we utilize integrated technology that serves to connect people and work that doesn't feel like a barrier? Learn how a Google team pioneered a UX Hub to align with how we build our products.",Google
Digital Work 2020,Flexible Working - A New Era of Productivity (*All Times EDT),"Although the idea of flexible working has been around for many years, the use of technology to enable this has plateaued in recent years. The COVID-19 crisis, coupled with the expectations of Millennials and Generation Z for flexible and remote work options, has provided a much needed opportunity to reassess exactly what is required today, with a clear view for the future. Building technology capabilities which enable flexibility in the workforce has now become a MUST for all organizations. Closing the gap between employee expectations, technology delivery and support models, is the challenge. In this session we will look at the key elements of delivering a Flexible Work level of service to gain a position of advantage. We will cover how end point analytics and optimal data utilization help achieve a productive employee experience. The benefits of flexible working based on technology that matches employee expectation
How to create enhanced productivity through employee engagement amidst a flexible working environment
The opportunities created with an IT / HR collaboration when planning for long-term remote work arrangements",Nexthink
Digital Work 2020,Leveraging a Digital Workspace App to Accelerate Employee Onboarding (*All Times EDT),"Onboarding is a critical milestone in an employee's journey at any organization. Learn how you can leverage a digital workspace app to engage employees after they sign their offer letter until they join the organization, no matter if they are working at home, in an office, or both. You will learn how personalized access to a digital workspace app can support employee onboarding, from providing the pre-hire employees the tools to make their first day productive to returning employees to the office safely. This session will provide an overview to digital workplace app functionalities and opportunities that will help you select features when shopping or building a DW app, with a focus on VMWare's Workspace ONE. The value of digital workspace app for employee engagement during onboarding
Requirements of digital workspace platforms for IT and HR to support employee onboarding
Advice on building a return to office planning guide",VMware
Digital Work 2020,Unleashing the Power of Technology to Deliver Great Employee Experiences (*All Times EDT),"The way we work will never be the same. The pandemic has created a new urgency for companies to digitally transform to succeed in the face of constant change. How and where work happens has been decoupled. Agile anywhere, anytime workplaces are on the rise. Managing the complex digital workflows of a distributed workplace will be critical to providing employees the services they need to be productive. In this session we will discuss the future of the workplace: How do we ensure the health and safety of our employees as they return to the workplace? ServiceNow leaders share how to unleash the power of technology to deliver great employee experiences as part of a digital transformation.",ServiceNow
Digital Work 2020,Activating Change Management Principles in your Learning and User Engagement Initiatives (*All Times EDT),"Adopting a new tool, process or procedure comes with a lot of angst for many users. Most of the time, all they want to know is - in the new tool, how can I replicate the work I've been doing for the last 20 years. To truly transform a workplace and adopt new ways of working in a digital workplace, we need to help users come along with us through our training and communications initiatives. The impact of the change cycle in user engagement
How to incorporate the company's strategic initiatives into your adoption planning
The critical role of implementing a communication plan during training to increase adoption velocity
How to design your training initiative to follow the project life cycle, and play a more effective role in user engagement",Gannett
Digital Work 2020,AI + Human: A Total Reimagination of Learning and Work (*All Times EDT),"After 50+ years of computing power growth supported by Moore's Law, and 60 years of AI development, we've entered an industry 4.0 era, with affordable cognitive computing power. The speed of innovation, competition and change is unprecedented, and advancing technologies mean both exponential competitor growth possibilities, as well as employee augmentation growth opportunities. With this the meaning and model of work and learning needs an overhaul. How do we level up work performance and creativity from individuals and teams in this new era?  The method requires total reimagination. What is the power of ""AI + Human/Team/Organization""?  How does it work? Understand the upcoming technology landscape and forces of change
Consider new visions for the collaborative intelligence of AI and humans at work
Learn how to prepare employees to work and learn with AI, through case studies",IEEE ICICLE
Digital Work 2020,The 2020 State of Digital Workplace Report (*All Times EDT),"Join Siobhan Fagan, Managing Editor and Sarah Kimmel, VP of Research, as they report key findings from the updated 2020 State of the Digital Workplace report, including: Impacts of the pandemic on the maturity of digital workplace practice
Shift in top priorities for digital workplace
Increasing effectiveness of digital workplace tools 
Surprise, surprise! Transitioning to digital workplace was easier than anyone thought.
Organizational plans for return to the office...or will they?",Reworked / CMSWire
Digital Work 2020,Keynote Address - What I Saw at the Workplace Revolution: Lessons from the Digital Frontlines (*All Times EDT),"Fast Company Editor-in-Chief Stephanie Mehta spends time with the entrepreneurs and inventors who are transforming business at dizzying speeds. She also knows, from her years interviewing leaders of the biggest corporations in the world as an editor at Fortune and The Wall Street Journal, that big organizations are working hard to inject their cultures and processes with entrepreneurial and innovative zeal. A natural storyteller, Mehta provides a lively, anecdote-filled roadmap that offers executives practical, actionable insights on how they can drive their digital company toward true transformation.",Fast Company
Digital Work 2020,Employee Experience Needs a New Approach (*All Times EDT),"Creating a seamless and integrated digital workplace experience requires putting employee needs front and center. But that's a challenge when you have siloed and disconnected applications that are creating fragmented experiences. Troy Campano, Head of Product at Workgrid, shares how you can align your technology around the employee and craft flexible experiences that scale and grow with your organization's needs.",Workgrid
Digital Work 2020,[Roundtable Discussion] Meeting the Employee Experience and Culture Expectations of Your Organization (*All Times EDT),"Don't miss this opportunity to join an exclusive group of your peers for an open and interactive conversation on the link between employee experience and building a connected culture, and share insights in practical approaches to building to meet and exceed expectations.",Reworked/CMSWire
Digital Work 2020,It's in The Vault: Rebuilding our Company-Wide Wiki (*All Times EDT),"As our company grew from 75 to 5,000+ employees in the span of five years, the imperative grew rapidly to comprehensively recreate our basic Wiki into a holistic intranet that would support the business's rapid growth. What made this project a success?  In this session we will discuss how we structured our project team.  We will present the problem our new intranet looked to solve, how we engendered new trust in the tool, the failures we overcame and the lessons learned, our information architecture development, content migration strategy and integrated change management practices.  We'll also include how we measured success, the metrics tracked and how this approach - while we built this internally - can be applied to any intranet-like project. Create a project team based on the trifecta of people, process, and technology
Integrate change management, communications planning, and employee participation in an intranet project
Learn from our mistakes!  Learn what worked during our project, what didn't and how you can apply our experience to your intranet project",Shopify
Digital Work 2020,Streamlining Information Flows in the Digital Workplace: The Role of Artificial Intelligence and Knowledge Engineering (*All Times EDT),"An efficient digital workplace is about getting the right information to the right people at the right time - the holy grail of personalization and the decades long efforts to optimize knowledge processes. With the advent of artificial intelligence, some believe that traditional information architecture approaches are not needed any longer. Nothing could be further from the truth. In fact AI requires IA (information architecture) now even more than ever.  Current incarnations of IA include knowledge architecture and knowledge engineering. vIn this session, Seth Earley, author of the AI Powered Enterprise, will review developments in artificial intelligence and knowledge and information architecture and show how each of these disciplines need to come together to speed up the information metabolism of the organization and streamline the user experience across every department and process. The foundation and underpinning of information structures are required for the modern digital workplace — especially as more work migrates to remote locations and distant collaboration. ",EIS
Digital Work 2020,Work is Not a Place -- Its an Activity! (*All Times EDT),"Key trends across the globe will be highlighted in this session by this international corporate leader. As more people collaborate across departments, functions, business units and time zones, how are digital workspaces achieving the merger of the accompanying internal and external communications and collaboration? How are successful businesses coping with five generations collaborating in the same workspace? What are the latest happenings for Firstline Workers, who make up 80% of the global workforce? What promising developments are currently underway as COVID-19 impacts the way the world is working - right now? Gain insight into workplace trends around the world, from 4-day work week trends, remote working and new ways of working
Learn about the benefits and challenges of the changing landscape of modern ways of working
Understand the critically relevant role of change management capabilities in today's global climate",Microsoft
Digital Work 2020,The Intranet Industry Changed. Were You Paying Attention? (*All Times EDT),"Newly distributed workplaces make intranets more crucial than ever. The industry has been shaken up with smarter deployment options, a second look at underlying technology, and new approaches to how organizations define what an intranet should and, more importantly, should not be. How organizations are rethinking their intranet deployment models to help today’s distributed workforce
What you need to consider the next time you overhaul your organization’s intranet
Case study examples of how leading companies are proving the value of intranet modernization",Simpplr
Digital Work 2020,4 Strategies for Creating a No Fail Employee Experience (*All Times EDT),"As the global pandemic is turning the business environment on its ear, organizations are figuring out that employees can't be productive and efficient if they do not feel supported or have the right resources. Attend this session and get a detailed plan for building a digital experience that supports employees long-term success, plus find out how The Riverstone Group made their employee experience vision a reality, by putting employees' needs first. Why a great employee experience is critical to business success
Keys to effectively deliver digital experiences that put the employee in the center
How to future-proof your employee experience through choosing the right technology and approach",RiverStone Workgrid
Digital Work 2020,How to Keep Employees Engaged: The Strategic Role of the Digital Workplace (*All Times EDT),"Research shows that informed and engaged employees are happier and more productive in the workplace, leading to better business results. The success of your digital workplace can directly affect the level of engagement amongst employees. Join Rob Ryan, Senior Director of Strategic Development and Business Value, to learn best practices for driving employee engagement in your digital workplace. Hear real examples and practical strategies around aligning your workforce, promoting a common corporate culture, streamlining the onboarding process -- and overall increasing employee engagement.",LumApps
Digital Work 2020,How to Build a Digital Workplace that Actually Works (*All Times EDT),"Companies must modernize how they work. Otherwise, they risk leaving customers unsatisfied and employees unengaged.  Modern work starts with a digital workplace, and a digital workplace requires information and insights. Too often, employees spend hours searching across silos and systems for information, as companies lack a business strategy and practical design for their digital workplaces. They build clouds, content platforms, apps, and analytics, and then hope to transform processes using AI augmentation. In reality, these technologies throw off inhuman amounts of data, which overwhelms the people doing the work. Sinequa’s enterprise search provides the missing building block. Join Sinequa and industry leaders to explore how to build digital workplace experiences that engage employees. Because employees gain a unified view of their information delivered in a familiar search-based experience they adopt digital workplaces more easily.",Sinequa
Digital Work 2020,Hybrid Work: the Requisite Merger of Intranet & Teams into a Modern Digital Workplace (*All Times EDT),"The COVID crisis has completely reshaped how employees engage with their company and their colleagues. Hybrid Work is becoming the new normal and requires a merger between your Intranet and Microsoft Teams for an effective and comprehensive Digital Workplace structure. In this session, we will review with Lorne Phelps from Powell Software and Erick Straghalis from StitchDX the different aspects that need to be considered in order to define a modern digital workplace that truly drives the Hybrid Work transformation.",StitchDX
Digital Work 2020,Defining the Modern Intranet - the Backbone of Employee Experience (*All Times EDT),"A great intranet is a unifying platform enmeshed with the organization's fabric and culture, tied tightly with the internal brand engagement. In this session we will explore the ways in which an intranet can best serve its constituents.  We will look at why an intranet should be approached as the core employee experience destination, and the importance of high alignment between your intranet and every employee engagement touch point. We will explore how a modern intranet contributes to greater employee retention, productivity and collaboration, and we will discover the ways in which an effective modern intranet is built, implemented and communicated to garner more productivity, shared knowledge, and engaged employees. The anatomy of an effective modern intranet
The value of aesthetics and design, and key intranet design components
Talking points that will help garner ""buy-in"" from leadership",Environmental Science Associates
Digital Work 2020,Transformation Tactics: Managing Big Change (*All Times EDT),"What does change management really feel like? A series of stories on thinking and working differently, with real-world examples and practical take-home ideas.  Join this breakout from Jim MacLennan, former SVP and CIO at IDEX Corporation, for a leader's perspective on the difference between leading v. managing big change, collaboration in practice and bringing it all together for powerful transformation.",Maker Turtle
Digital Work 2020,Keynote Address: Measuring Your Way to Digital Workplace ROI and Success (*All Times EDT),"Creating a compelling digital employee experience is now an imperative with the mass global shift to remote work. But modern digital workplaces also have to deliver significant benefits to the business so that investment and evolution can continue through fast-changing times. Learn how leading organizations are using tools and techniques like benchmarking, talent analytics, business intelligence, strategy execution platforms, and popular goal tracking frameworks like OKRs to ensure their digital workplace is moving the needle for the business. Come away with real-world examples and actionable steps to instrument your digital workplace to all new heights of performance using measurable outcomes and ROI.",Optimal Disruption
Oracle 2020,Oracle CEO Safra Catz at Oracle OpenWorld Europe,"Learn how cloud technologies begin to reshape the way we interact with the world around us. Today, emerging technology and automation permeate every aspect life. Learn how Oracle Cloud drives innovation and real change.",Oracle
Oracle 2020,Oracle Autonomous Cloud - World Bee Project CIC,"Bees are the most important species on earth. Yet bee numbers have declined dramatically over the last few years. Hear how Oracle and The World Bee Project are applying IoT, big data and AI to create more sustainable ecosystems for bees.",Oracle
Oracle 2020,A New Class of Cloud Infrastructure,"Learn how emerging technologies will help you eliminate the sources of data management complexity, lower costs, increase reliability, and speed -up development process.",Oracle Cloud Infrastructure
Oracle 2020,Big Shifts that will Determine Your Future Business of Technology Operating,"Explore nine shifts which determine your ability to deliver technology rapidly, successively, and at scale. Learn why organizations concluded their IT technology operating model needs to transform into a company wide Business of Technology capability.",Deloitte
Oracle 2020,Oracle Cloud Drives Innovation and Real Change,"Find out how cloud technologies begin to reshape how we think about and interacting with the world around us, and how emerging technologies and automations are impacting every aspect of our life.",Oracle Cloud
Oracle 2020,How to Turn Change into Opportunity,"Explore how Oracle Cloud applications can help you stay ahead of your competition. Also, learn how AI, blockchain, IoT, and conversational UI can help reshape your core business processes.",Oracle
Oracle 2020,How Oracle and Microsoft Hit Refresh,"Learn how our joint customers can benefit from Oracle and Microsoft distributed cloud deployment model through networking, unified identity, compatibility and support.",and Africa
Oracle 2020,Outpacing Change in a Disrupted Worldâ€”The New Core of Business,"Discover how new technologies such as AI, machine learning, conversational UI, and predictive analytics can help you reshape your finance department. Learn how abcam extends leadership in scientific research through business transformation.",Oracle
Oracle 2020,People:Power:Machine,"Learn how the development, adoption, and assurance of human-centric AI solutions positively contributes to innovations and economic developments to solve the world's most important problems.",PwC
Oracle 2020,Oracle Cloud: A Path and Platform,"Learn how Oracle Cloud drives innovation and real change for customers. Cloud technologies are beginning to reshape how we think about and interact with the world around us. The opportunities that the cloud presents are real and present today, and are providing the building blocks for companies to pioneer groundbreaking innovations and disrupt entire industries.",Asia Pacific and Japan
Oracle 2020,Autonomous Data Management,Discover what's new and what's coming next from the Database Development team. Find out how Autonomous Database is revolutionizing Data Management by utilising autonomous services on a modern cloud.,Oracl
Oracle 2020,How To Turn Change Into Opportunity With Oracle Cloud Apps,"Hear how AI, blockchain, IoT, and conversational UI can reshape your core business processes from HR to finance to supply chain to customer experience. Learn how you can win at business by anticipating your customers' future wants and needs.",Oracle Applications Product Development
Oracle 2020,Outpacing Change in a Disrupted Worldâ€”The New Core of Business,Hear from Oracle's ERP and EPM development leaders and trailblazing customers showcasing how technologies are reshaping and reinvigorating the finance and operations core of the business.,Oracle
Oracle 2020,Unlock Endless Possibilities with Oracle Cloud,Explore how machine learning and automation are powering the world's only autonomous cloud. Oracle Cloud Infrastructure has re-imagined cloud for the most important enterprise applications with consistent high performance and unmatched governance and security controls.,and Central and Eastern Europe
Oracle 2020,"Cloud Platform, Middleware Strategy and Roadmap",Learn about the strategy and vision for Oracle's comprehensive cloud platform services and on-premises software. Hear from customers on how they leverage Oracle Cloud for their digital transformation.,Oracle
Oracle 2020,A New Class of Cloud Infrastructure,Find out how Oracle's new class of cloud can help you scale your business and secure your data.,Oracle
Oracle 2020,Solution Keynote: Strategy and Roadmap for Cloud and On-Premise,Learn how Exadata architecture is being transformed to provide the world's most advanced cloud and in-memory functionality for both online transaction processing and analytics.,Oracle
Oracle 2020,Strategy and Roadmap for Cloud and On-Premise,Learn how Exadata architecture is being transformed to provide the world's most advanced cloud and in-memory functionality for both online transaction processing and analytics.,Oracle
